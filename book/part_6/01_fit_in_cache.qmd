# Avoid benchmarking with unrealistically small amounts of data

As we've seen in many of the previous chapters, a key to knowing whether you're making your code faster or slower is measurement.
However, if you use small amounts of data, the results of micro-benchmarks can be misleading.
One relevant reason: memory caches.

Because your CPU has memory caches that are faster than RAM, but smaller in size, if you can keep all your data in those caches you will get higher performance.
The smaller the data, the more likely you are to be able to use the smaller, faster caches.

But if your data gets larger, your process will end up using slower caches, or even RAM.
As a result, if the data you're benchmarking is much smaller than your real data, your benchmarks may be inaccurate.

Let's consider an example: we're going to pick 100,000 samples from an array, with repeats allowed.

```{python}
#| echo: false
%load_ext book_magics
```

```{python}
import numpy as np
from numba import njit

GENERATOR = np.random.default_rng(0)

@njit
def random_sample(random_gen, arr):
    result = np.empty((100_000,), dtype=arr.dtype)
    for i in range(100_000):
        chosen_index = random_gen.integers(0, len(arr))
        result[i] = arr[chosen_index]
    return result

# Pre-compile Numba code:
random_sample(GENERATOR, np.array([1, 2], dtype=np.uint8))

SMALL_DATA = np.arange(0, 1_000, dtype=np.uint8)
LARGE_DATA = np.arange(0, 100_000_000, dtype=np.uint8)
```

Since we're doing the same amount of computational work regardless of array size, one might expect the run time to be the same for both `SMALL_DATA` and `LARGE_DATA`.
In fact:

```{python}
#| echo: false
%%compare_timing --measure=instructions
random_sample(GENERATOR, SMALL_DATA)
random_sample(GENERATOR, LARGE_DATA)
```

Running the function on a large array takes 1.5Ã— as long, even though the number of CPU instructions is virtually identical.
What's going on?

In the case of `SMALL_DATA`, the data fits in the CPU's memory caches, so repeated access is fast.
`LARGE_DATA` however is too large to fit in the caches, so it needs to be read from RAM, which is slower.

We can see this numerically if we add a measure of memory cache misses to our benchmark (or more accurately, last-level cache misses):

```{python}
#| echo: false
%%compare_timing --measure=instructions,memory_cache_miss
random_sample(GENERATOR, SMALL_DATA)
random_sample(GENERATOR, LARGE_DATA)
```

If our real-world data is closer to `LARGE_DATA`, benchmarking using `SMALL_DATA` is going to be misleading.

## How large are the CPU's memory caches?

Given data size can impact performance results, it's useful to know how memory caches are organized, and their general sizes.
To simplify somewhat, here's how the CPU talks to RAM:

```{dot}
digraph G {
  rankdir = "LR"
  node [shape = "box"]
  l1i [label="L1 cache (instructions)"]
  l1d [label="L1 cache (data)"]
  l2 [label="L2 cache"]
  l3 [label="L3 cache"]
  CPU -> l1i
  CPU -> l1d
  l1i -> l2
  l1d -> l2
  l2 -> l3
  l3 -> RAM
}
```

To give a sense of scale, the 8 physical performance (higher-speed) CPU cores in my i7-12700K have:

1. 48KiB L1 data cache and 32KiB L1 instruction cache for each core.
2. 1280KiB L2 cache for each core.
3. 25MiB L3 cache shared by all the cores, including both the performance cores plus the 4 lower-speed efficiency cores.

RAM is the slowest, L3 is a bit faster, L2 more so, and L1 is the fastest.
Typically L1 is per CPU core, L2 may or may not be per core, and L3 is shared across cores.
There are actually two L1 caches, one for instructions, i.e. the code you're running, and one for the data your program is using.

::: {.callout-note}
The `lstopo` command included in the [`hwloc`](https://www.open-mpi.org/projects/hwloc/) package can draw nice diagrams of your CPU's memory cache configuration.
:::
