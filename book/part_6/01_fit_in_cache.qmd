# Avoid benchmarking with unrealistically small amounts of data

As we've seen in previous chapters, a key to knowing whether you're making your code faster or slower is measurement.
However, if you use small amounts of data, the results of micro-benchmarks can be misleading.
One relevant reason: memory caches.

Because your CPU has memory caches that are faster than RAM, but smaller in size, if you can keep all your data in those caches you will get higher performance.
The smaller the data, the more likely you are to be able to use the smaller, faster caches.

But if your data gets larger, your process will end up using slower caches, or even RAM.
As a result, if the data you're benchmarking is much smaller than your real data, your benchmarks may be inaccurate.

Let's consider an example: we're going to pick 100,000 samples from an array, with repeats allowed.

```{python}
#| echo: false
%load_ext book_magics
```

```{python}
import numpy as np
from numba import jit

GENERATOR = np.random.default_rng(0)

@jit
def random_sample(random_gen, arr):
    result = np.empty((100_000,), dtype=arr.dtype)
    for i in range(100_000):
        chosen_index = random_gen.integers(0, len(arr))
        result[i] = arr[chosen_index]
    return result

# Pre-compile Numba code:
random_sample(GENERATOR, np.array([1, 2], dtype=np.uint8))

SMALL_DATA = np.arange(0, 1_000, dtype=np.uint8)
LARGE_DATA = np.arange(0, 100_000_000, dtype=np.uint8)
```

Since we're doing the same amount of computational work regardless of array size, one might expect the run time to be the same for both `SMALL_DATA` and `LARGE_DATA`.
In fact:

```{python}
#| echo: false
%%compare_timing --measure=instructions
random_sample(GENERATOR, SMALL_DATA)
random_sample(GENERATOR, LARGE_DATA)
```

Running the function on a large array takes 1.5Ã— as long, even though the number of CPU instructions is virtually identical.
What's going on?

In the case of `SMALL_DATA`, the data fits in the CPU's memory caches, so repeated access is fast.
`LARGE_DATA` however is too large to fit in the caches, so it needs to be read from RAM, which is slower.
If our real-world data is closer to `LARGE_DATA`, benchmarking using `SMALL_DATA` is going to be misleading.
