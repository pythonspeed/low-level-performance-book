# Understand how abstractions lay out their memory

In the previous chapter we saw how the order of iteration over an N-dimensional array can have a performance impact.
This is an example of a broader point: you need to understand how data structures structure their memory, in order to get the most performance.
Put another way, while abstractions are critical to writing software, for the best performance you will need to peer behind the curtain and see how the code really works.

Partially this is because you want to make sure the ways you access data are efficient.
But it's also because you want to make sure the compiler has all the information it needs to generate efficient code.

In this chapter we'll look at a particular example, NumPy views, but similar issues can potentially apply to any data structure you are using.

## NumPy views point at the underlying array's data

When you take a slice of a NumPy array, you are not getting a copy of the data.
Rather, you get a view object, which points at the same underlying memory.


```{python}
#| echo: false
%load_ext book_magics
```

For example, we can create a two-dimensional array:

```{python}
import numpy as np

arr = np.arange(0, 6, dtype=np.uint16).reshape((2, 3))
print(arr)
```

And then slice it across two different dimensions:

```{python}
view1 = arr[0,:]
print("view1:", view1)
view2 = arr[:,0]
print("view2:", view2)
```

If we mutate the underlying array, this change is reflected in both views:

```{python}
arr[0, 0] = 100
print("view1:", view1)
print("view2:", view2)
```

## NumPy views can be contiguous or non-contiguous

The array we're looking at is two dimensional, but memory is best thought of as one dimensional.
With the default mapping from 2-D to 1-D, `view1` is using contiguous memory, but `view2` is not.
Both views look the same externally thanks to the abstraction provided by NumPy, but internally `view2` is skipping to different parts of the underlying array to get the data.

The implementation mechanism is called ["striding"](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.strides.html).
Each view has strides telling NumPy how many bytes to skip ahead to get to the next entry.
For contiguous arrays, the stride is the same size as the data type; in our case, that's 2 bytes to jump to the next 16-bit integer.
For non-contiguous arrays, the stride will be larger:

```{python}
print("view1 stride:", view1.strides)
print("view2 stride:", view2.strides)
```

## Tell the compiler when an array is contiguous

For the most part this is an implementation detail you don't have to think about... until performance becomes an issue.
In particular, the compiler can do a lot more optimization if it knows that data is laid out in a contiguous manner.

The way Numba deals with this is by automatically compiling different versions of the function for contiguous and non-contiguous arrays.
Since this happens by default, all our examples in the rest of the book benefited from this automatically.
But we can explicitly tell Numba to either accept all arrays, or only accept contiguous arrays or views:

```{python}
from numba import njit, int16

DATA = np.arange(0, 1_000_000, dtype=np.int16)

# The int16[:] syntax means the function must be able to accept both contiguous
# and non-contiguous 1-D arrays and views. As such, the compiler won't be able to
# optimize as much.
@njit([(int16[:],)])
def multiply_any(arr):
    result = np.empty(arr.shape, dtype=arr.dtype)
    for i in range(len(arr)):
        result[i] = arr[i] * 2
    return result

# This int16[::1] syntax means the function only accepts 1-D contiguous arrays,
# which allows the compiler to do more optimization. Notice the actual code is
# identical!
@njit([(int16[::1],)])
def multiply_contiguous(arr):
    result = np.empty(arr.shape, dtype=arr.dtype)
    for i in range(len(arr)):
        result[i] = arr[i] * 2
    return result

# Lacking type information, Numba compiles different version for contiguous and
# non-contiguous inputs.
@njit
def multiply_auto_specialized(arr):
    result = np.empty(arr.shape, dtype=arr.dtype)
    for i in range(len(arr)):
        result[i] = arr[i] * 2
    return result

assert np.array_equal(
    multiply_any(DATA),
    multiply_contiguous(DATA)
)
assert np.array_equal(
    multiply_any(DATA),
    multiply_auto_specialized(DATA)
)
```

The versions that can assume the array is contiguous are much faster, because the compiler can optimize better:

```{python}
#| echo: false
%%compare_timing
multiply_any(DATA)
multiply_contiguous(DATA)
multiply_auto_specialized(DATA)
```

While `multiply_contiguous()` is faster, it won't accept non-contiguous views, so it's less general purpose.
This is why by default Numba will compile different versions of the function, as is the case for `multiply_auto_specialized()`: one for contiguous views and arrays and one for non-contiguous views.

Different programming languages and libraries will have their own ways of dealing with optimizations of code that accepts contiguous arrays.
For example, here's an article I wrote that shows one way to [handle this in Cython](https://pythonspeed.com/articles/faster-cython-simd/) without code duplication.

More broadly, abstractions like NumPy views are useful, but they can also get in the way of performance.
So make sure to dig in to the details of your data is actually laid out, and see if there are ways to expose relevant information to the compiler.
