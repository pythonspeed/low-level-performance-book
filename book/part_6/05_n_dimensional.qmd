## Scan N-dimensional arrays in a linear fashion

As we saw in a previous chapter, scanning memory linearly is the fastest way to do so.
Whether or not you're scanning memory linearly becomes a little less obvious when you're dealing with N-dimensional arrays: the memory address space is linear, so there's different ways to map the different dimensions on.
Imagine the following array:

```
  X ‚Üí
Y 1, 2, 3
‚Üì 4, 5, 6
```

It could be laid out in memory like this:

```
1, 2, 3, 4, 5, 6
```

Or like this:

```
1, 4, 2, 5, 3, 6
```

However it's laid out, we want to iterate over the data in a way that matches that layout, because that will be faster.
In practice, NumPy arrays by default will be laid out so the last dimension is contiguous in memory, so you want to iterate over the dimensions in their listed order, from first to last.

## An example: summing a 2-D array

Let's see an example:

```{python}
#| echo: false
%load_ext book_magics
```

```{python}
import numpy as np
from numba import jit

two_d = np.ones((2_000, 2_000), dtype=np.int64)

@jit
def scan_y_first(arr):
    total = 0
    for y in range(arr.shape[0]):
        for x in range(arr.shape[1]):
            total += arr[y, x]
    return total

@jit
def scan_x_first(arr):
    total = 0
    for x in range(arr.shape[1]):
        for y in range(arr.shape[0]):
            total += arr[y, x]
    return total

assert scan_x_first(two_d) == scan_y_first(two_d)
```

Given NumPy's default memory layout, scanning the dimensions in order is faster:

```{python}
#| echo: false
%%compare_timing --measure=instructions
scan_y_first(two_d)
scan_x_first(two_d)
```

## Avoid memory reads that aren't linear

The number of CPU instructions is much lower in the linear scan.
Why is the number of CPU instructions much lower when we scan linearly?
Given an array that is known to be contiguous, and that is being scanned in a way that is known to match how it's laid out in memory, the compiler can use SIMD instructions.
The compiler isn't going to use SIMD instructions if the multiple data items being operated on aren't contiguous in memory.

So is this the reason why the linear scan is faster?
It is part of the reason, but not all: memory reads are also in play.
We can see this if we force Numba to compile a version of the code that can't take advantage of the knowledge than array is contiguous.
Keep in mind we're only doing this for educational purposes in the context of this chapter, since the resulting functions will be slower:

```{python}
from numba import int64

# üôÅ This syntax mean the function will accept a 2-D array, but it cannot
# assume which dimension is contiguous in memory. You probably don't want to do
# this if you can avoid it, since it'll give you slower code.
@jit([(int64[:,:],)])
def scan_y_first_nc(arr):
    total = 0
    for y in range(arr.shape[0]):
        for x in range(arr.shape[1]):
            total += arr[y, x]
    return total

@jit([(int64[:,:],)])
def scan_x_first_nc(arr):
    total = 0
    for x in range(arr.shape[1]):
        for y in range(arr.shape[0]):
            total += arr[y, x]
    return total

assert scan_x_first_nc(two_d) == scan_y_first_nc(two_d)
```

Unlike our previous version, the number of CPU instructions run by our new `scan_y_first_nc()` and `scan_x_first_nc()` are essentially identical.
Yet there is still a significant performance gap between the two:

```{python}
#| echo: false
%%compare_timing --measure=instructions
scan_y_first_nc(two_d)
scan_x_first_nc(two_d)
```
