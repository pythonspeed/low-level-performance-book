# Use linear memory reads to get faster memory access

Even if your data is too large to fit in to a memory cache, there are still ways to speed up access.
One way to do so is to access memory linearly, in the order it's stored in memory.
Compared to linear reads, reading memory in random order can be much slowerâ€”let's see why.

We'll set up our normal imports:

```{python}
# Import the dependencies we'll need:
import numpy as np
from numba import njit
```

```{python}
#| echo: false
%load_ext book_magics
```

We implement a function that can scan memory both linearly and pseudo-randomly, depending which multiplier we pass in.
We pass the multiplier in explicitly so the compiler doesn't optimize some of the work out of existence, as it would with a constant.

```{python}
DATA = np.ones((100_000_000,), dtype=np.uint8)

@njit
def scan_memory(arr, multiplier):
    index = 0
    size = len(arr)
    for _ in range(1_000_000):
        arr[index] += 1
        index = (multiplier * index + 1) % size
    return arr[0]

LINEAR = 1
RANDOM = 22695477

_ = scan_memory(np.ones((10,), dtype=np.uint8), LINEAR)
```

Depending on the value of the multiplier, the memory will be traversed in different ways:

1. If the multiplier is 1 (`LINEAR`), this is just a linear scan of memory.
2. If you pass in a large number like 22695477 (`RANDOM`), this is a pseudo-random traversal of the array, bouncing around in memory.

Here's the time it takes to reach each variation:

```{python}
#| echo: false
%%compare_timing --measure=instructions
scan_memory(DATA, LINEAR)
scan_memory(DATA, RANDOM)
```

In both cases we're running almost the same number of CPU instructions.
If the CPU were the only bottleneck, both variations should have taken the same amount of time because they're doing the same amount of work.

## Why linear reads are faster than random access reads

Why is a linear scan of memory faster than randomly jumping around?

### Pre-fetching

Modern CPUs will pre-fetch memory into the cache as an optimization technique, even before the running code asks for it.
When this works well, by the time you've gotten to the point of needing to read the next chunk of memory it will already be in the cache.

Linear memory scans are a common usage pattern in software, and they're also easy to detect.
So if you're doing a linear scan it's very likely the CPU will notice and prefetch the data you need into the cache.
Random scans are, of course, much harder to predict, and so prefetching is less likely to help.

If data is being prefetched, there will be fewer cache misses because the CPU will already have loaded the data into the memory caches.
And that means fewer loads from slower caches, or even slower RAM.

Let's rerun our comparison, showing the number of L1D cache misses:

```{python}
#| echo: false
%%compare_timing --measure=l1_memory_cache_miss
scan_memory(DATA, LINEAR)
scan_memory(DATA, RANDOM)
```

My computer's L1D cache is far smaller than the 1MB of data we read, yet we have almost no misses in the linear case.
That's likely because it's being filled by the CPU, automatically.

### Cache lines

Reading memory linearly can also benefit from the way data is loaded into the memory caches: in small chunks known as "cache lines", typically 64 bytes at a time.
Each of the values in the example above is an 8-bit integer, i.e. it takes 1 byte.
So if we're loading 64 bytes at a time, we'll also load the 2nd, 3rd, 4th up to the 63rd consecutive value into the L1 cache.

Comparing our two variations:

* **Linear scan:** Each load of an integer also loads the next 63 consecutive integers into a fast, local cache.
  This is helpful, since we're doing a linear scan, so we're going to be inspecting these integers next.
* **Random scan:** Each load of an integer also loads the next 63 consecutive integers, but they're of no immediate use to us since the next value we'll inspect will be elsewhere in the array.
  Quite possibly they'll be dropped from the cache before we actually need to see them.

As a result, in the linear scan case the data we need is much more likely to already be in the much-faster L1 cache.
