# Avoid benchmarking with unrealistically small amounts of data

As we've seen in many of the previous chapters, a key to knowing whether you're making your code faster or slower is measurement.
However, if you use small amounts of data, the results can be misleading.
In this chapter we'll focus on one particular reason: memory caches.

Because your CPU has memory caches that are faster than RAM, but smaller in size, if you can keep data in those caches you will get higher performance.
Once your data gets larger, your process will end up using slower caches, or even RAM.
As a result, if the data you're benchmarking is much smaller than the the real data, your benchmarks may be inaccurate.

Let's consider an example: we're going to pick 100,000 samples from an array, with repeats allowed:

```{python}
#| echo: false
%load_ext book_magics
```

```{python}
import numpy as np
from numba import njit

GENERATOR = np.random.default_rng(0)

@njit
def random_sample(random_gen, arr):
    result = np.empty((100_000,), dtype=arr.dtype)
    for i in range(100_000):
        chosen_index = random_gen.integers(0, len(arr))
        result[i] = arr[chosen_index]
    return result

# Pre-compile Numba code:
random_sample(GENERATOR, np.array([1, 2], dtype=np.uint8))

SMALL_DATA = np.arange(0, 1_000, dtype=np.uint8)
LARGE_DATA = np.arange(0, 100_000_000, dtype=np.uint8)
```

Since we're doing the same amount of computational work regardless of array size, one might expect the run time to be the same for both `SMALL_DATA` and `LARGE_DATA`.
In fact:

```{python}
#| echo: false
%%compare_timing --measure=instructions
random_sample(GENERATOR, SMALL_DATA)
random_sample(GENERATOR, LARGE_DATA)
```

Running the function on a large array takes 1.5× as long, even though the number of CPU instructions is virtually identical.
What's going on?

In the case of `SMALL_DATA`, the data fits in the CPU's memory caches, so repeated access is fast.
`LARGE_DATA` however is too large to fit in the caches, so it needs to be read from RAM, which is slower.

We can see this is if we add measure of memory cache misses to our benchmark (or more accurately, last-level cache misses):

```{python}
#| echo: false
%%compare_timing --measure=instructions,memory_cache_miss
random_sample(GENERATOR, SMALL_DATA)
random_sample(GENERATOR, LARGE_DATA)
```

If our real-world data is closer to `LARGE_DATA`, benchmarking using `SMALL_DATA` is going to be misleading.

## Visualizing the impact of caches on runtime

We can measure run time across a range of array sizes:

```{python}
#| output: false
import benchit

inputs = {
    size: [np.arange(0, size * 1024, dtype=np.uint8)]
    for size in [250, 500, 1_000, 2_500, 5_000, 10_000, 25_000, 50_000, 100_000]
}

def random_sample_(arr):
    return random_sample(GENERATOR, arr)

timings = benchit.timings(
    [random_sample_],
    inputs=inputs,
    multivar=True,
    input_name="Array memory size (KB)",
)
```

Here's what the resulting run time looks like; lower values on the Y axis are better:

```{python}
timings.plot(logx=True, logy=False, dpi=100, figsize=(8, 4))
```

## Using the mean in benchmarks can be misleading

The numbers above run the the benchmarks multiple times, and use the mean of the result.
When memory caches start affecting performance, as is the case here, using the mean may be misleading.
In particular, data that is sitting in the cache after the first run can impact the performance results of later runs.

We'll do the operation a few times in a row, measuring each _individual_ run, and then flush the cache and try again.

```{python}
from time import time

def timeit(f, *args):
    start = time()
    f(*args)
    print(f"{(time() - start) * 1_000_000} µs")

def flush_cache():
    print("Flushing memory caches...")
    # Create some garbage data large enough that it will hopefully clear out
    # any existing data from all CPU memory caches.
    data = np.arange(0, 10_000_000, dtype=np.uint8)
    del data
```

First, let's try with a small amount of data:

```{python}
flush_cache()
timeit(random_sample, GENERATOR, SMALL_DATA)
timeit(random_sample, GENERATOR, SMALL_DATA)
timeit(random_sample, GENERATOR, SMALL_DATA)
flush_cache()
timeit(random_sample, GENERATOR, SMALL_DATA)
```

Notice how the first run is much slower, because the data isn't in the CPU's memory cache.
The lower numbers later on may be representative _if_ in real-world usage the data is in the cache; if not, the first number is more accurate.

With a large amount of data the cache is never sufficient, so run times are more consistent:

```{python}
flush_cache()
timeit(random_sample, GENERATOR, LARGE_DATA)
timeit(random_sample, GENERATOR, LARGE_DATA)
timeit(random_sample, GENERATOR, LARGE_DATA)
flush_cache()
timeit(random_sample, GENERATOR, LARGE_DATA)
```

## More details about memory caches

To simplify somewhat, here's how the CPU talks to RAM:

```{dot}
digraph G {
  rankdir = "LR"
  node [shape = "box"]
  l1i [label="L1 cache (instructions)"]
  l1d [label="L1 cache (data)"]
  l2 [label="L2 cache"]
  l3 [label="L3 cache"]
  CPU -> l1i
  CPU -> l1d
  l1i -> l2
  l1d -> l2
  l2 -> l3 -> RAM
}
```

RAM is the slowest, L3 is a bit faster, L2 more so, and L1 is the fastest.
Typically L1 is per CPU core, L2 may or may not be per core, and L3 is shared across cores.
There are actually two L1 caches, one for instructions, i.e. the code you're running, and one for the data your program is using.

To give a sense of scale, the 8 physical performance (higher-speed) CPU cores in my i7-12700K have:

1. 48KiB L1 data cache and 32KiB L1 instruction cache for each core.
2. 1280KiB L2 cache for each core.
3. 25MiB L3 cache shared by all the cores, including both the performance cores plus the 4 lower-speed efficiency cores.

::: {.callout-note}
The `lstopo` command included in the [`hwloc`](https://www.open-mpi.org/projects/hwloc/) package can draw nice diagrams of your CPU's memory cache configuration.
:::
