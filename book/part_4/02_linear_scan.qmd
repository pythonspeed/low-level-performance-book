# Use linear memory reads to get faster memory access

Let's see the impact of the _order_ of memory access on performance.
We'll set up our normal imports:

```{python}
# Import the dependencies we'll need:
import numpy as np
from numba import njit
```

```{python}
#| echo: false
%load_ext book_magics
```

And implement the following function.
We pass the multiplier in explicitly so the compiler doesn't optimize things away as it would with a constant.

```{python}
@njit
def scan_memory(array_size, multiplier):
    data = np.ones((array_size,), dtype=np.uint8)

    index = 0
    for _ in range(10_000_000):
        data[index] += 1
        index = (multiplier * index + 1) % array_size
    return data[0]

LINEAR = 1
RANDOM = 22695477

scan_memory(1, LINEAR)
```

Depending on the value of the multiplier, the memory will be traversed in different ways:

1. If the multiplier is 1, this is just a linear scan of memory.
2. If you pass in a large number like 22695477, this is a pseudo-random traversal of the array, bouncing around in memory.

Here's the time it takes to reach each variation:

```{python}
#| echo: false
%%compare_timing --measure=instructions
scan_memory(100_000_000, LINEAR)
scan_memory(100_000_000, RANDOM)
```

Bouncing around memory is slower than a linear scan of memoryâ€”but why?
In both cases we're running the same number of CPU instructions.
If the CPU were the only bottleneck, both variations should have taken the same amount of time because they're doing the same amount of work.

## Why linear reads are faster than random access reads

Let's get back to our code example.
Why is a linear scan of memory faster than randomly jumping around?

### Cache lines

Memory is loaded into the memory caches we discussed in previous in small chunks known as "cache lines", typically 64 bytes at a time.
Each of the values in the example above is an 8-bit integer, i.e. it takes 1 byte.
So if we're loading 64 bytes at a time, we'll also load the 2nd, 3rd, 4th up to the 63rd consecutive value into the L1 cache.

Comparing our two variations:

* **Linear scan:** Each load of an integer also loads the next 63 consecutive integers into a fast, local cache.
  This is helpful, since we're doing a linear scan, so we're going to be inspecting these integers next.
* **Random scan:** Each load of an integer also loads the next 63 consecutive integers, but they're of no immediate use to us since the next value we'll inspect will be elsewhere in the array.
  Quite possibly they'll be dropped from the cache before we actually need to see them.

As a result, in the linear scan case the data we need is much more likely to already be in the much-faster L1 cache.

```{python}
#| echo: false
%%compare_timing --measure=ll_memory_cache_refs,l1_memory_cache_refs
scan_memory(100_000_000, LINEAR)
scan_memory(100_000_000, RANDOM)
```

### Pre-fetching

Modern CPUs will also pre-fetch memory into the cache as an optimization technique, even before the running code asks for it.
When this works well, by the time you've gotten to the point of needing to read the next chunk of memory it will already be in the cache.

Linear memory scans are a common usage pattern in software, and they're also easy to detect.
So if you're doing a linear scan it's very likely the CPU will notice and prefetch the data you need into the cache.
Random scans are, of course, much harder to predict, and so prefetching is less likely to help.

If data is being prefetched, there will be fewer cache misses because the CPU will already have loaded the data into the memory caches.
And that means fewer loads from slower caches, or even slower RAM.

Let's rerun our comparison, showing what percentage of cache accesses are misses:

```{python}
#| echo: false
%%compare_timing --measure=ll_memory_cache_miss,l1_memory_cache_miss
scan_memory(100_000_000, LINEAR)
scan_memory(100_000_000, RANDOM)
```

## TODO move to another chapter: N-dimensional scans

As we saw, scanning memory linearly is the fastest way to do so.
Whether or not you're scanning memory linearly becomes a little less obvious when you're dealing with N-dimensional arrays: the memory address space is linear, so there's different ways to map the different dimensions on.
For example, if you have a 2-dimensional array, you can iterate over the first dimension first or the second dimension first:

```{python}
two_d = np.ones((2_000, 2_000), dtype=np.int64)

@njit
def scan_y_first(arr):
    total = 0
    for y in range(arr.shape[0]):
        for x in range(arr.shape[1]):
            total += arr[y, x]
    return total

@njit
def scan_x_first(arr):
    total = 0
    for x in range(arr.shape[1]):
        for y in range(arr.shape[0]):
            total += arr[y, x]
    return total

assert scan_y_first(two_d) == scan_x_first(two_d)
```

Given NumPy's default memory layout, scanning the dimensions in order is faster:

```{python}
#| echo: false
%%compare_timing
scan_y_first(two_d)
scan_x_first(two_d)
```

### Be aware of the impact of NumPy views

TODO maybe?

### Keep your working set in memory

The other thing you can do keep your code running quickly, especially when linear scans aren't possible, is to keep your working set smaller.
The smaller your memory footprint, the more likely it is to be in one of the smaller, faster caches.

Previously we scanned an array using 100MB of memory, which was too big to fit in cache once enough random lookups happened.
With a smaller array of 1MB, however, the whole array can be kept in the L3 or probably even L2 cache.
And that means switching from a linear scan to random access isn't as expensive:

```{python}
#| echo: false
%%compare_timing
scan_memory(1_000_000, LINEAR)
scan_memory(1_000_000, RANDOM)
```

TODO swap
