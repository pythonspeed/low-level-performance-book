# Reduce the cost of bounds checking

In addition to explicit branches you might add to your code, there are other less obvious sources of branching that can be automatically added by the compiler.
In this chapter we'll cover bounds checking, and in the next chapter will cover division by zero.

## Bounds checking adds performance overhead

Let's say you have an NumPy array with 4 items, and you read the 5th item.
What happens?
There are two possible behaviors:

* **Error:** Some sort of error is raised, for example an `IndexError` Python exception.
* **Undefined behavior:** This can range from segfaults to corrupted data to completely arbitrary behavior.

You really want to get an error, not undefined behavior.
But that means that every time you read or write a value at a given index, you need to check if that index is within bounds.
And that is an extra branch being inserted into your code.
It's a predictable branch, since only buggy code will do out-of-bounds reads or writes, but it will slow down your code.

The behavior you get depends on the language you're using:

* C and C++ have no bounds checking; they will happily corrupt memory or crash your program in the name of performance.
    However, you can use [sanitizers and other tools](https://developers.redhat.com/blog/2021/05/05/memory-error-checking-in-c-and-c-comparing-sanitizers-and-valgrind) to catch problems when testing your code.
* Numba and Cython (the latter with some caveats, since it compiles to C or C++) allow you to turn bounds checking on or off.
* Rust always has bounds checking on[^rust].

[^rust]: Technically Rust also has additional `unsafe` APIs to do lookups without bounds checking. Using `unsafe` is tricky, and misuse invalidates all of Rust's safety guarantees, so I would suggest pretending these APIs don't exist.

In Numba, by default bounds checking is disabled, but we can manually enable or disable it to compare the performance impact.

```{python}
#| echo: false
%load_ext book_magics
```

```{python}
import numpy as np
from numba import njit

DATA = np.random.random((1_000_000))

@njit(boundscheck=False)
def moving_average(timeseries):
    result = np.empty(timeseries.shape, dtype=np.float64)
    for i in range(len(timeseries)):
        total = 0
        for j in range(max(i - 6, 0), i + 1):
            total += timeseries[j]
        result[i] = total / 7
    return result

# Same function, but bounds checks enabled:
@njit(boundscheck=True)
def moving_average_bc(timeseries):
    result = np.empty(timeseries.shape, dtype=np.float64)
    for i in range(len(timeseries)):
        total = 0
        for j in range(max(i - 6, 0), i + 1):
            total += timeseries[j]
        result[i] = total / 7
    return result

assert np.array_equal(
    moving_average(DATA),
    moving_average_bc(DATA)
)
```

```{python}
#| echo: false
%%compare_timing
moving_average(DATA)
moving_average_bc(DATA)
```

## Strategy #1: Leave bounds checking on, live with slower code

Quickly calculating the wrong answer is not very helpful.
As such, there's a strong argument to be made for always leaving bounds-checking enabled, even if this means a performance hit.

## Strategy #2: Help the compiler optimize bounds checking out of existence

In some cases you can leave bounds checking on, but give enough information to the compiler that it will remove the bounds checks as part of its optimization passes.
This gives you the best of both worlds: the safety guarantees of bounds checking, without the performance cost.
I first learned about this strategy from [this excellent article about applying it to Rust](https://shnatsel.medium.com/how-to-avoid-bounds-checks-in-rust-without-unsafe-f65e618b4c1e).

The general approach is to figure out idioms that are transparent enough to the compiler that it can prove to itself that the bounds will never be violated.
The specific approach is language-specific, or rather compiler-specific.

To reduce duplication of code, we'll write a little utility to generate two `@njit` versions of the same function, with and without bounds checking:

```{python}
# Return two @njitted functions, with and without bounds checking:
def with_and_without_boundscheck(f):
    with_bc = njit(boundscheck=True)(f)
    without_bc = njit(boundscheck=False)(f)
    return (with_bc, without_bc)
```

Next, we'll use the standard technique we've been using for iteration, a `for` loop over a `range(len(arr))`, and compare runtime with and without bounds checking:

```{python}
DATA = np.arange(0, 1_000_000, dtype=np.uint64)

def sum_for(arr):
    total = 0
    for i in range(len(arr)):
        total += arr[i]
    return total

sum_for_bc, sum_for_no_bc = with_and_without_boundscheck(sum_for)
assert sum_for_bc(DATA) == sum_for_no_bc(DATA)
```

```{python}
#| echo: false
%%compare_timing
sum_for_bc(DATA)
sum_for_no_bc(DATA)
```

Clearly this idiom is not enough to convince the compiler.
But what if we switched away from a `for` loop to a more explicit `while` loop?

```{python}
def sum_while(arr):
    total = 0
    i = 0
    while i < arr.shape[0]:
        total += arr[i]
        i += 1
    return total

sum_while_bc, sum_while_no_bc = with_and_without_boundscheck(sum_while)
assert sum_while_bc(DATA) == sum_while_no_bc(DATA)
```

```{python}
#| echo: false
%%compare_timing
sum_while_bc(DATA)
sum_while_no_bc(DATA)
```

Success! Both versions are almost the same speed, which means we can leave bounds checking on and still get the safety benefits without a large performance hit.

With a little experimentation, I was able to apply the technique to the moving average function above.
The new version has no performance impact from bounds checking, and even better, it's faster!

```{python}
def moving_average_2(timeseries):
    result = np.empty(timeseries.shape, dtype=np.float64)
    i = 0
    while i < len(timeseries):
        total = 0
        # Get a slice of timeseries, so we can apply the
        # while loop technique here as well.
        window = timeseries[max(i - 6, 0):i + 1]
        j = 0
        while j < len(window):
            total += window[j]
            j += 1
        result[i] = total / 7
        i += 1
    return result

moving_average_2_bc, moving_average_2_no_bc = with_and_without_boundscheck(
    moving_average_2
)
assert np.array_equal(
    moving_average(DATA),
    moving_average_2_bc(DATA)
)
assert np.array_equal(
    moving_average(DATA),
    moving_average_2_no_bc(DATA)
)
```

```{python}
#| echo: false
%%compare_timing
moving_average(DATA)
moving_average_2_bc(DATA)
moving_average_2_no_bc(DATA)
```

## Strategy #3: Enable bounds checking when testing, disable in production

If you don't want to enable bounds checking in production, you should at least enable it when running tests.
Combined with a thorough test suite, using both realistic data and property-based testing using [Hypothesis](https://hypothesis.readthedocs.io/), you can hopefully catch any bugs causing out-of-bounds reads or writes, and fix them.
If your code doesn't have bugs, disabling bounds checking in production isn't a problem.

That being said, decades of experience suggests that it is _very_ difficult to catch all such bugs once your software is complex enough.
