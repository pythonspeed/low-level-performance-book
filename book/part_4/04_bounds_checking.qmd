# Reduce the cost of bounds checking

In addition to explicit branches you add to your code, there are other less obvious sources of branches: those that are automatically added by the compiler.
In this chapter we'll cover bounds checking, and in the next chapter we'll cover division by zero.

## Bounds checking adds performance overhead

Let's say you have an NumPy array of length 4, and you read the 5th item.
What happens?
There are two possible behaviors:

* **Error:** Some sort of error is raised, for example an `IndexError` Python exception.
* **Undefined behavior:** This can range from segfaults to corrupted data to completely arbitrary behavior.

You really want to get an error, not undefined behavior.
But that means that every time you read or write a value at a given index, the code need to check if that index is within bounds.
And that is an extra branch being inserted into your code.
It's a predictable branch, since only buggy code will do out-of-bounds reads or writes, but it will slow down your code.

Bounds checking behavior depends on the language you're using:

* C and C++ have no built-in bounds checking; they will happily corrupt memory or crash your program in the name of performance.
    However, some data structures will provide them, and more broadly you can use [sanitizers and other tools](https://developers.redhat.com/blog/2021/05/05/memory-error-checking-in-c-and-c-comparing-sanitizers-and-valgrind) to catch problems when testing your code.
* Numba and Cython (the latter with some caveats, since it compiles to C or C++) allow you to turn bounds checking on or off.
* Rust always has bounds checking on[^rust].

[^rust]: Technically Rust also has additional `unsafe` APIs to do lookups without bounds checking. Using `unsafe` is tricky, and misuse invalidates all of Rust's safety guarantees, so I would suggest pretending these APIs don't exist.

In Numba, bounds checking is disabled by default, but we can manually enable or disable it to compare the performance impact.
As an example we'll use a moving average function again, but this time we'll just use zeros to fill in the missing first few values.

```{python}
#| echo: false
%load_ext book_magics
```

```{python}
import numpy as np
from numba import jit

DATA = np.random.random((1_000_000))

@jit(boundscheck=False)
def moving_average_no_bc(timeseries):
    result = np.empty(timeseries.shape, dtype=np.float64)
    for i in range(len(timeseries)):
        total = 0
        for j in range(max(i - 6, 0), i + 1):
            total += timeseries[j]
        result[i] = total / 7
    return result

# Same function, but bounds checks enabled:
@jit(boundscheck=True)
def moving_average_bc(timeseries):
    result = np.empty(timeseries.shape, dtype=np.float64)
    for i in range(len(timeseries)):
        total = 0
        for j in range(max(i - 6, 0), i + 1):
            total += timeseries[j]
        result[i] = total / 7
    return result

assert np.array_equal(
    moving_average_bc(DATA),
    moving_average_no_bc(DATA)
)
```

If we compare the two versions, the one with bounds checking is slower:

```{python}
#| echo: false
%%compare_timing --measure=instructions,branches
moving_average_bc(DATA)
moving_average_no_bc(DATA)
```

On the one hand, bounds checking is a great way to catch bugs, both in testing and production usage.
On the other hand, it slows down your code.
What can you do to balance safety and speed?

## Strategy #1: Enable bounds checking when testing, disable in production

If you don't want to enable bounds checking in production, you should at least enable it when running tests.
Combined with a thorough test suite, using both realistic data and property-based testing using [Hypothesis](https://hypothesis.readthedocs.io/), you can hopefully catch any bugs causing out-of-bounds reads or writes, and fix them.
If your code doesn't have bugs, disabling bounds checking in production isn't a problem.

That being said, decades of experience suggests that it is _very_ difficult to catch all such bugs once your software is complex enough.

## Strategy #2: Leave bounds checking on, live with slower code

Quickly calculating the wrong answer is not very helpful.
As such, there's a strong argument to be made for always leaving bounds-checking enabled, even if this means a performance hit.

## Strategy #3: Help the compiler optimize bounds checking out of existence

In some cases you can leave bounds checking on, but provide enough information to the compiler that it will remove the bounds checks as part of its optimization passes.
This gives you the best of both worlds: the safety guarantees of bounds checking, without the performance cost.
I first learned about this strategy from [this excellent article about applying it to Rust](https://shnatsel.medium.com/how-to-avoid-bounds-checks-in-rust-without-unsafe-f65e618b4c1e).

The general approach is to figure out idioms that are transparent enough to the compiler that it can prove to itself that the bounds will never be violated, or alternatively you can use asserts that give it more information.
The specific approach is language-specific, and often compiler-specific.

Let's start with a trivial example.
To reduce duplication of code, we'll write a little utility to generate two `@jit` versions of the same function, with and without bounds checking:

```{python}
# Return two @jitted functions, with and without bounds checking:
def with_and_without_boundscheck(f):
    with_bc = jit(boundscheck=True)(f)
    without_bc = jit(boundscheck=False)(f)
    return (with_bc, without_bc)
```

First, we'll use the standard technique we've been using for iteration, a `for` loop over a `range(len(arr))`, and compare runtime with and without bounds checking:

```{python}
DATA = np.arange(0, 1_000_000, dtype=np.uint64)

def sum1(arr):
    total = 0
    for i in range(len(arr)):
        total += arr[i]
    return total

sum_bc, sum_no_bc = with_and_without_boundscheck(sum1)
assert sum_bc(DATA) == sum_no_bc(DATA)
```

```{python}
#| echo: false
%%compare_timing --measure=instructions,branches
sum_bc(DATA)
sum_no_bc(DATA)
```

Clearly this idiom is not enough to convince the compiler to optimize bounds checks away.

Another thing we can try is `for value in arr:`, since this seems like it ought to avoid bounds checking altogether.

```{python}
DATA = np.arange(0, 1_000_000, dtype=np.uint64)

def sum2(arr):
    total = 0
    for value in arr:
        total += value
    return total

sum2_bc, sum2_no_bc = with_and_without_boundscheck(sum2)
assert sum2_bc(DATA) == sum2_no_bc(DATA)
```

And as expected, bounds checking doesn't _add_ much cost... but unfortunately our function is much slower even with bounds checking disabled.

```{python}
#| echo: false
%%compare_timing --measure=instructions,branches
sum2_bc(DATA)
sum2_no_bc(DATA)
```

So it seems like we want to keep the `arr[i]`, because some experimentation suggests it [makes it easier for auto-vectorization to happen](https://github.com/numba/numba/issues/9210), while still doing `for value in arr:`.

Here's what I came up with:

```{python}
def sum3(arr):
    total = 0
    for i, _ in enumerate(arr):
        total += arr[i]
    return total

sum3_bc, sum3_no_bc = with_and_without_boundscheck(sum3)
assert sum3_bc(DATA) == sum3_no_bc(DATA)
```

```{python}
#| echo: false
%%compare_timing --measure=instructions,branches
sum3_bc(DATA)
sum3_no_bc(DATA)
```

Success! Both versions are almost the same speed, which means we can leave bounds checking on and still get the safety benefits without a large performance hit.

With a little experimentation, I was able to apply the technique to the moving average function above.
The new version has a much more modest performance impact from bounds checking, and even better, it's faster!

```{python}
def moving_average_2(timeseries):
    result = np.empty(timeseries.shape, dtype=np.float64)
    for i, _ in enumerate(timeseries):
        total = 0
        # ðŸ˜Ž Get a slice of timeseries, so we can apply the
        # while loop technique here as well.
        window = timeseries[max(i - 6, 0):i + 1]
        # ðŸ˜Ž Loop in a way that allows Numba to optimize away legal bound
        # checks:
        for j, _ in enumerate(window):
            total += window[j]
        result[i] = total / 7
        i += 1
    return result

moving_average_2_bc, moving_average_2_no_bc = with_and_without_boundscheck(
    moving_average_2
)

assert np.array_equal(
    moving_average_no_bc(DATA),
    moving_average_2_bc(DATA)
)
assert np.array_equal(
    moving_average_no_bc(DATA),
    moving_average_2_no_bc(DATA)
)
```

```{python}
#| echo: false
%%compare_timing --measure=instructions,branches
moving_average_no_bc(DATA)
moving_average_2_bc(DATA)
moving_average_2_no_bc(DATA)
```

As you can see, you can write code that leaves bounds checking on, without paying a performance cost.
You do suffer from a bit of a readability cost, and this technique is very dependent on your compiler version.
A new, sufficiently different version of Numba might make this idiom or slower, or perhaps allow the same benefit with less funny-looking loops.

If you're writing Rust, which always has bounds checking on, using the [Rust-specific version of this technique](https://shnatsel.medium.com/how-to-avoid-bounds-checks-in-rust-without-unsafe-f65e618b4c1e) is a great way to speed up your code.
