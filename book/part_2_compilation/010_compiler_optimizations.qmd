# Rely on the compiler to optimize your code

A compiler translates your code—structured text that is intended to be read and written by humans—to machine code, the instructions the CPU understands.
But the compiler does more than that.
When your code gets compiled, the compiler also runs it through a series of optimization passes, with each pass transforming the code in the some way.
The hope is that the cumulative series of transformations will result in code that is more efficient by:

1. Removing redundant code execution, our focus in this chapter.
2. Utilizing the CPU as efficiently as possible.
   We won't consider these optimizations here, but we'll cover at least some of them in later chapters.

```{python}
#| echo: false
%load_ext book_magics
```

## Get a sense of what the compiler can do

In later chapters we'll consider some of the reasons the compiler might fail to optimize your code, and what you can do about it.
But before seeing the reasons compilations won't work, it's worth understanding some of what the compiler can do.

### Hoisting expressions out of inner loops

Consider these two functions, that do the same calculation in different ways; for now we're sticking to normal Python:

```{python}
def python_unhoisted(m, n):
    total = 0
    for i in range(m):
        total += m * ((n ** 2) * (1.5 / n) + n)
    return total

def python_manually_hoisted(m, n):
    total = 0
    constant = (n ** 2) * (1.5 / n) + n
    for i in range(m):
        total += m * constant
    return total

# The results are the same:
assert python_unhoisted(100, 17) == python_manually_hoisted(100, 17)
```

The expression `((n ** 2) * (1.5 / n) + n)` in the `python_unhoisted()` function is repeated in every iteration, but it always give the same result.
Unfortunately, this version of Python isn't smart enough to take advantage of that, so `python_unhoisted()` does a lot more work:

```{python}
#| echo: false
%%compare_timing

python_unhoisted(1_000, 3)
python_manually_hoisted(1_000, 3)
```

Now let's switch to compiled code:

```{python}
from numba import jit
import numpy as np

@jit
def compiler_will_hoist(m, n):
    total = 0
    for i in range(m):
        total += m * ((n ** 2) * (1.5 / n) + n)
    return total

@jit
def manually_hoisted(m, n):
    total = 0
    constant = (n ** 2) * (1.5 / n) + n
    for i in range(m):
        total += m * constant
    return total

# The results are the same:
assert compiler_will_hoist(100, 17) == manually_hoisted(100, 17)
```

The compiler is smart enough to notice that `((n ** 2) * (1.5 / n) + n)` is a repeated expression whose result doesn't change.
It is therefore able to move it ("hoist" it) out of the loop in `compiler_will_hoist()` and run it only once, as we do manually in `manually_hoisted()`.
Both functions therefore run at around the same speed:

```{python}
#| echo: false
%%compare_timing

compiler_will_hoist(1_000_000, 3)
manually_hoisted(1_000_000, 3)
```

While both versions have the same speed, the manually hoisted version is arguably clearer about what is going on in the calculation, and the compiler won't always be able to hoist expressions, so doing it manually might still be worth your time.

### Inlining

Calling a function takes some time, even in a low-level language.
In addition, it's more difficult for the compiler to optimize across function boundaries.
Thus, the compiler will sometimes take the contents of a function's code and put them inside the calling function, so that it can do more optimization passes.
This is known as "function inlining".

```{python}
@jit
def in_to_cm(cm):
    return cm * 2.54

@jit
def cm3_to_liters(cm3):
    return cm3 / 1000

# The calculation is split across multiple different functions. However, we can
# expect the compiler to automatically combine them into a single function, so
# there's no additional overhead from those function calls.
@jit
def volume_liters(width_in, height_in, depth_in):
    total = 0
    for i in range(1, 10_000):
        width_cm = in_to_cm(width_in + i)
        height_cm = in_to_cm(height_in + i)
        depth_cm = in_to_cm(depth_in + i)
        total += cm3_to_liters(width_cm * height_cm * depth_cm)
    return total

# All calculations are manually unified into a single function:
@jit
def volume_liters_manual_inlining(width_in, height_in, depth_in):
    total = 0
    for i in range(1, 10_000):
        width_cm = (width_in + i) * 2.54
        height_cm = (height_in + i) * 2.54
        depth_cm = (depth_in + i) * 2.54
        total += (width_cm * height_cm * depth_cm) / 1000
    return total

assert (
    volume_liters(17.0, 23.0, 52.0) ==
    volume_liters_manual_inlining(17.0, 23.0, 52.0)
)
```

By relying on inlining, we can choose to break up our code across multiple functions without having to worry about performance implications.

```{python}
#| echo: false
%%compare_timing

volume_liters(17.0, 23.0, 52.0)
volume_liters_manual_inlining(17.0, 23.0, 52.0)
```
How do we know whether the function will be inlined?
By default, compilers will use compiler and language-specific heuristics to decide whether or not to inline, often tied to function size.
Functions as trivial as `in_to_cm()` and `cm3_to_liters()` within the same file are likely to be inlined in pretty much any compiler.
To override the compiler's heuristics, most compiled languages allow you to add hints to functions, telling the compiler that it should (or shouldn't) inline that function.[^numba]

[^numba]: Numba has an inlining hint, but it's not relevant for performance optimization, as it's intended for a different part of the compilation process.

### Replacing generic implementations with faster, specialized implementations

For any given piece of code there are different ways the compiler could translate the code into CPU instructions.
If you specify the work to be done in a more constrained way, this gives the compiler opportunities to switch to a more specialized but faster implementation.
For example, let's say we want to divide numbers by 8; we can do it two ways:

```{python}
# This needs to use generic division code, because maybe you'll want to divide
# by 950124, or 237, or 3; the compiler can't know in advance.
@jit
def generic_division(divisor, size):
    arr = np.empty((size,), dtype=np.uint64)
    for i in range(len(arr)):
        arr[i] = i // divisor
    return arr

# This can generate division specifically specialized for the number 8; because
# it's a power of 2, that can be done very efficiently.
@jit
def specific_division_by_8(size):
    arr = np.empty((size,), dtype=np.uint64)
    for i in range(len(arr)):
        arr[i] = i // 8
    return arr

assert np.array_equal(
    specific_division_by_8(1000),
    generic_division(8, 1000)
)
```

If the compiler is asked to do a generic division, it needs to be able to handle any possible number you give it.
But if the compiler knows you're dividing by a specific number, it can choose to use faster operations than division to get the same result.
For powers of 2, at least, the compiler can use very efficient alternatives to a generic implementation of division:

```{python}
#| echo: false
%%compare_timing
generic_division(8, 100_000)
specific_division_by_8(100_000)
```

::: {.callout-note}
For Numba, you can see the underlying compiler's optimized output—specifically, the LLVM intermediate representation—by setting this option before you import `numba` for the first time:

```{python}
#| eval: false
os.environ["NUMBA_DUMP_OPTIMIZED"] = "1"
```

You can then search for `OPTIMIZED DUMP your_functions_name` in the output (replace `your_functions_name` with your function's name as relevant).
:::

## The compiler can't replace the Practice of Algorithmic Efficiency

Some of what the compiler is doing when it applies optimization passes can overlap, at least a little, with what you would do manually if you were applying the Practice of Algorithmic Efficiency.
But that doesn't mean a compiler can replace those optimizations:

* Compilers can't read your mind, nor can they observe how your code actually runs in practice[^pgo]; they can only read your code.
  Some optimizations rely on understanding intent, or on knowing what input data will look like.
* Compilers need to guarantee that optimizations don't break your code.
  Lacking understanding of your intent, the compiler will usually assume that _any_ change to code behavior is unacceptable.
  As a result, in cases where the optimization might change your code's behavior even in tiny ways, the compiler won't apply seemingly obvious optimizations.
* Compiler optimizations are pre-programmed heuristics, so there is simply a limit to how much they can do, compared to a human being actually thinking about a problem.

In a previous chapter I showed some algorithmic efficiency optimizations applied to the Sieve of Eratosthenes, with the examples implemented in Python.
To demonstrate that those optimizations still help even with a compiled language, I will switch to using Numba; I ported the original version and the final, optimized version.

```{python}
@jit
def find_primes_1(up_to_value):
    # Create an array of booleans, with default value being True:
    is_prime = np.ones((up_to_value,), dtype=np.bool_)
    is_prime[0] = False
    is_prime[1] = False

    for i in range(2, up_to_value):
        if not is_prime[i]:
            continue
        for j in range(i*2, up_to_value, i):
            is_prime[j] = False

    return is_prime

@jit
def find_primes_4(up_to_value):
    is_prime = np.ones((up_to_value,), dtype=np.bool_)
    is_prime[0] = False
    is_prime[1] = False

    for i in range(4, up_to_value, 2):
        is_prime[i] = False

    for i in range(3, up_to_value, 2):
        if not is_prime[i]:
            continue
        for j in range(i*i, up_to_value, 2 * i):
            is_prime[j] = False

    return is_prime

assert np.array_equal(find_primes_1(100), find_primes_4(100))
```

Comparing the speed of the two implementations, you can see the Practice of Algorithmic Efficiency is still speeding things up, even though I've switched to a compiled language:

```{python}
#| echo: false
%%compare_timing
find_primes_1(5_000)
find_primes_4(5_000)
```

[^pgo]: This is not strictly true. Techniques like Profiler-Guided Optimization can provide some runtime information to the compiler, but not in a way that invalidates the general claim. [`awesome-pgo`](https://github.com/zamazan4ik/awesome-pgo) is an extensive resource on the topic.
