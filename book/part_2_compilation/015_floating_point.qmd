# Manually optimize floating-point calculations

As discussed in more detail in the next section, the compiler won't apply many "obvious" optimization to floating-point calculations because the result will be different.
But it might only be _slightly_ different, or different in ways that you, the author of the code, know don't matter.
So there's nothing stopping you from applying similar optimizations manually, if you know it won't meaningfully change the result.

```{python}
#| echo: false
%load_ext book_magics
```

## Why compilers have a hard time optimizing floating point math

If you add two integers, the order doesn't matter; likewise for multiplication.
The compiler can take advantage of this flexibility to rearrange calculations to help with a variety of optimizations.
For example, it might hoist a calculation out of a loop so it's only done once.

Unfortunately, floating-point calculations may give different results if you run them in a different order:

```{python}
import numpy as np
from numba import jit

@jit
def add_in_order(start, middle, end):
    result = start
    for _ in range(100_000_000):
        result += middle
    result += end
    return result

LARGE = np.float32(100_000_000_000)
SMALL = np.float32(0.1)

assert (
    add_in_order(0, SMALL, LARGE) !=
    add_in_order(LARGE, SMALL, 0)
)
```

Why does the order make a difference?
Because when you start with a sufficiently large floating point number, adding a sufficiently small number is equivalent to adding zero!
So if you do that in a loop you are still left with the original number.


```{python}
assert LARGE + SMALL == LARGE
assert add_in_order(LARGE, SMALL, 0) == LARGE
```

But if you add lots of small numbers first, they eventually become a big enough number, such that adding it to the large number returns a different value:

```{python}
assert SMALL + SMALL > SMALL
assert add_in_order(0, SMALL, LARGE) > LARGE
```

**This is a problem for an optimizing compiler: it will only apply optimizations if it knows the result will be _identical_ to the original code.**
And clearly, re-ordering floating point calculations does not always give identical results.
As a result, the compiler can apply fewer optimizations to floating-point calculations than it would to equivalent integer calculations, resulting in slower code.

## Extract and hoist repetitive calculations out of a loop

Given the compiler isn't going to apply as many optimizations, it's up to you to do.
Imagine you have the dimensions of a series of boxes sold by a box company, stored in an array that has triples of width, height, and depth in inches.
You want to calculate the volume of these boxes in liters, so you first must convert the inches to centimeters by multiplying by `2.54`.

Here's what a first pass calculation might look like:

```{python}
import numpy as np
from numba import jit

generator = np.random.default_rng(0)
# Generate one million random container sizes, between 1 and 10 inches in each
# dimension:
SIZES_INCHES = generator.uniform(1, 10, (1_000_000, 3)).astype(np.float32)

@jit
def volume_liters(sizes):
    result = np.empty((sizes.shape[0],), dtype=np.float32)
    for i in range(sizes.shape[0]):
        width_in, height_in, depth_in = sizes[i]
        # Convert to centimeters:
        width_cm = width_in * 2.54
        height_cm = height_in * 2.54
        depth_cm = depth_in * 2.54
        # Convert to liters:
        result[i] = (width_cm * height_cm * depth_cm) / 1000.0
    return result

result = volume_liters(SIZES_INCHES)
```

With a little tweaking, you can move part of the calculation out of the inner loop, hopefully resulting in faster code:

```{python}
@jit
def volume_liters_2(sizes):
    result = np.empty((sizes.shape[0],), dtype=np.float32)
    # ðŸ˜Ž Hoist some of the calculations into a constant:
    in3_to_cm3 = 2.54 * 2.54 * 2.54
    for i in range(sizes.shape[0]):
        width_in, height_in, depth_in = sizes[i]
        result[i] = (width_in * height_in * depth_in * in3_to_cm3) / 1000.0
    return result

result_2 = volume_liters_2(SIZES_INCHES)

# This gives a different results...
assert not np.array_equal(result, result_2)
# ...but only very slightly different:
assert np.abs(result - result_2).max() < 0.001
```

And the new version is faster:

```{python}
#| echo: false
%%compare_timing --measure=instructions
volume_liters(SIZES_INCHES)
volume_liters_2(SIZES_INCHES)
```


## Use faster multiplication instead of division

In general, floating-point multiplication is faster than division.
So another optimization you can apply is replacing division by `X` with multiplication by `1 / X`.
Again, the compiler won't apply this optimization by default because it may change the results slightly.

```{python}
@jit
def volume_liters_3(sizes):
    result = np.empty((sizes.shape[0],), dtype=np.float32)
    # ðŸ˜Ž Hoist the division into a constant, so there is even less work in the
    # inner loop:
    in3_to_liter = (2.54 * 2.54 * 2.54) / 1000
    for i in range(sizes.shape[0]):
        width_in, height_in, depth_in = sizes[i]
        result[i] = width_in * height_in * depth_in * in3_to_liter
    return result

result_3 = volume_liters_3(SIZES_INCHES)

# This gives different results...
assert not np.array_equal(result, result_3)
# ...but again, only very slightly different:
assert np.abs(result - result_3).max() < 0.001
```

And the new version is indeed faster:

```{python}
#| echo: false
%%compare_timing --measure=instructions
volume_liters_2(SIZES_INCHES)
volume_liters_3(SIZES_INCHES)
```
