# Avoid aliasing to enable compiler optimizations

To understand the problem we'll cover in this chapter, we first need to cover a couple of definitions:

* A pointer is a programming language construct that points at a memory address.
  In the CPython interpreter you use to run Python, for example, every Python object you interact with is a pointer to a `PyObject`.
  In Numba, every NumPy array or view has a pointer internally.
* Aliasing means having multiple pointers referencing the same memory address at the same time.

When you slice a NumPy array, for example, by default the data doesn't get copied.
Instead, you get a view object that points at the same memory.
Now you have two pointers at the original memory: the original array and the view into it.

```{python}
#| echo: false
%load_ext book_magics
```

```{python}
import numpy as np
from numba import njit

arr = np.array([1, 2, 3], dtype=np.int64)
# This is a view to the original array:
arr_view = arr[:2]
print("Original view", arr_view)
# If we mutate the original array...
arr[0] = 100
# ...the view sees the same information:
print("Updated view", arr_view)
```

Here's the problem: the _possibility_ of aliasing can limit what the compiler is able to do to speed up your code.
That's because seemingly identically behaved functions can actually behave differently when inputs are aliased.

## An example of the problem

TODO note source of example, the systems programming book, in footnote or osmething

```{python}
@njit
def add_twice(first_arr, second_arr):
    for i in range(len(first_arr)):
        first_arr[i] += second_arr[i]
        first_arr[i] += second_arr[i]
    return first_arr

@njit
def add_doubled(first_arr, second_arr):
    for i in range(len(first_arr)):
        first_arr[i] += 2 * second_arr[i]
    return first_arr

def zeros():
    return np.zeros((1_000_000, ), dtype=np.uint64)

DATA2 = np.linspace(1_000_000, 0, 1_000_000, dtype=np.uint64)

# Both functions give the same result (in this case at least):
assert np.array_equal(
    add_twice(zeros(), DATA2),
    add_doubled(zeros(), DATA2)
)
```

The second implementation is faster, because it does less work and the compiler doesn't optimize that work away:

```{python}
#| echo: false
%%compare_timing

add_twice(zeros(), DATA2)
add_doubled(zeros(), DATA2)
```

On the face of it, the two implementations are semantically the same, and so it's surprising that the compiler won't transform the first implementation into the second, slightly faster form.

The likely issue is that the compiler can't know at compile time if the two arrays are actually the same array: it needs to take aliasing into account.
This matters because the two functions behave differently for inputs that involve aliasing, which means the compiler can't swap the two implementations out.
For example, if we pass the same array to both arguments of the functions, we get different results:

```{python}
arr = np.ones((3, ), dtype=np.uint64)
print("add_twice() gives:", add_twice(arr, arr))

arr = np.ones((3, ), dtype=np.uint64)
print("add_doubled() gives:", add_doubled(arr, arr))
```

## Solution #1: Manual optimization

Rather than relying on the compiler, you can implement the optimizations yourself.
In our example, you could write the verison of the code in `add_doubled()` instead of `add_twice()`, assuming that matches the semantics you want in the case when the data is aliased.

Unfortunately, there may be less obvious optimizations that you miss.
And some optimizations are much more difficult for you to implement yourself, in particular those that involve generating particular machine code.

## Solution #2: Ensure aliasing doesn't impact the result

Just because two pointers point at the same memory doesn't necessarily prevent the compiler from applying optimizations.
In particular, the issue in the examples above was due to _writing_ to the passed-in array.
If instead we write the data to a _new_ array, aliasing becomes irrelevant and the compiler should be able to apply more optimizations.

```{python}
@njit
def add_twice_new_array(first_arr, second_arr):
    result = np.empty(first_arr.shape, dtype=first_arr.dtype)
    for i in range(len(result)):
        result[i] = first_arr[i] + second_arr[i]
        result[i] += second_arr[i]
    return result

@njit
def add_doubled_new_array(first_arr, second_arr):
    result = np.empty(first_arr.shape, dtype=first_arr.dtype)
    for i in range(len(result)):
        result[i] = first_arr[i] + 2 * second_arr[i]
    return result

# Both functions give the same result:
ZEROS = zeros()
assert np.array_equal(
    add_twice_new_array(ZEROS, DATA2),
    add_doubled_new_array(ZEROS, DATA2)
)
```

In this case the compiler is able to optimize `add_twice_new_array()`:

```{python}
#| echo: false
%%compare_timing
add_twice_new_array(ZEROS, DATA2)
add_doubled_new_array(ZEROS, DATA2)
```

## Solution #3: Take advantage of language features

Your programming language may have a way to indicate that aliasing is not an issue.
For example:

* In C, you can use the `restrict` keyword on function arguments to manually indicate that you have enforced a uniqueness constraint.
* The most commonly-used C++ compilers support [a similar language extension](https://en.wikipedia.org/wiki/Restrict#Support_by_C++_compilers).
* In Rust, if you have a writable reference to data, you cannot simultaneously have any other references to it; this is enforced by the compiler.
  The Rust compiler is therefore able to optimize code without having to worry about aliasing.
  When dealing with external data like Python objects, Rust libraries will enforce this at runtime insofar as they are able to.
  See for example [the relevant documentation for the Rust NumPy integration](https://docs.rs/numpy/latest/numpy/borrow/index.html).

Numba is considering adding a solution similar to C's, but at the time of writing it has not yet been merged so I am unable to demonstrate it.
