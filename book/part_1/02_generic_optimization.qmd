# Get rid of repetitive and wasteful code

You've picked a good algorithm, your implementation language is now faster, but your code still isn't fast enough.
The next step is to make your implementation more efficient by removing repetitive, wasteful code.
Put another way, if your algorithm is $O(n·log(n))$, you can model the performance as being $k·n·log(n)$, and your goal is to try to shrink `k`.

```{python}
#| echo: false
%load_ext book_magics
```

```{python}
# Import the dependencies we'll need:
import numpy as np
from numba import njit
```

## Focus your efforts on inner loops

If the three most important factors in real estate are location, location, location, the three most important factors in the speed of your data processing code are loops, loops, loops.
Consider the problem domain this book addresses: processing relatively large amounts of data.
At the core of the low-level code to process this data you will inevitably either write or call functions with the following structure:

```{python}
#| eval: false
# This only gets called once:
do_some_setup()

for item in large_amount_of_data:
    # This gets called a lot:
    do_something(item)

# This only gets called once:
do_some_cleanup()
```

Given large amounts of data, and data structures like NumPy arrays or Arrow that are designed for fast access from low-level languages, the bulk of your processing will happen inside the loop.
You can speed up this sort of code by:

1. Looping over less data—perhaps there's data you don't need to process at all?
2. Reducing the work you repeatedly do inside the loop, the code you will be calling over and over again.

## Don't process data you don't need to

The fastest code is code you don't run at all.
If you can avoid processing data when it's unnecessary, that can provide significant speeds.

As an example, let's implement the Sieve of Eratosthenes.
We'll find prime numbers by crossing out multiples of 2 (4, 6, 8, ...), and then multiples of 3 (6, 9, 12, ...), multiples of 5 (10, 15, 20, ...), and so on.

```{python}
@njit
def find_primes(length):
    is_prime = np.ones((length,), dtype=np.bool_)
    is_prime[0] = False
    is_prime[1] = False
    for i in range(2, length):
        if not is_prime[i]:
            continue
        for j in range(i*2, length, i):
            is_prime[j] = False
    return is_prime

PRIMES = find_primes(1_000_000)
```

Can we make this code any faster?
One thing we can notice is that we're checking certain numbers twice.
For example, when we rule out multiples of 5, we're ruling out $5×2$ and $5×3$ and $5×4$ even though we've already ruled them out in previous rounds.
$2×5$ was ruled out as a multiple of 2.
So we can tweak our `j` loop to start at a later point (`i * i` instead of `i * 2`) to prevent doing duplicate work:

```{python}
@njit
def find_primes_2(length):
    is_prime = np.ones((length,), dtype=np.bool_)
    is_prime[0] = False
    is_prime[1] = False

    for i in range(2, length):
        if not is_prime[i]:
            continue
        for j in range(i*i, length, i):
            is_prime[j] = False
    return is_prime

assert np.array_equal(PRIMES, find_primes_2(1_000_000))
```

This small tweak makes our algorithm run noticeably faster:

```{python}
#| echo: false
%%compare_timing
find_primes(1_000_000)
find_primes_2(1_000_000)
```

## Don't do unnecessary work inside loops

In our loop above we do an `if is_prime[i]: continue` check on all even `i` numbers, even though we know that any even number larger than 2 isn't prime.
This is something we can avoid by special casing even numbers:

```{python}
@njit
def find_primes_3(length):
    is_prime = np.ones((length,), dtype=np.bool_)
    is_prime[0] = False
    is_prime[1] = False

    # Rule out all even numbers unconditionally:
    for i in range(4, length, 2):
        is_prime[i] = False

    # Now only check odd numbers:
    for i in range(3, length, 2):
        if not is_prime[i]:
            continue
        for j in range(i*i, length, i):
            is_prime[j] = False
    return is_prime

assert np.array_equal(PRIMES, find_primes_3(1_000_000))
```

This version is even faster:

```{python}
#| echo: false
%%compare_timing
find_primes(1_000_000)
find_primes_2(1_000_000)
find_primes_3(1_000_000)
```

## Avoid allocations in the hot loop

Another common example of work to avoid inside loops is temporary memory allocation.
In compiled languages creating an integer or float or other small type doesn't typically require memory allocation, and is quite cheap.
Larger values that need to persist beyond the lifetime of the function call need to be allocated from the operating system.
Allocating memory can happen explicitly—for example, in C you might call `malloc()`—or implicitly, depending on your language.

Allocating and freeing memory inside a loop can both make it harder for the compiler to optimize your code, and may be just wasted effort.
Instead, see if you can reuse allocations.

For our purposes, Numba allocates memory based on rules inherited from NumPy:

* Explicitly creating an array allocates memory, for example `np.ones((1_000,))`.
* Slicing an array does not allocate memory, it creates a view of the same underlying memory, for example `myarr[1:]`.
* Mathematical operations on arrays can create new arrays, thus allocating memory, for example `myarr + 1` creates a new array with item `i` equal to `myarr[i] + 1`.

To see the performance impact of allocating in a loop, consider the following example:

```{python}
@njit
def allocate_in_loop(arr):
    total = 0
    for i in range(len(arr) // 4):
        # This is just a view, so it doesn't allocate:
        slice_of_4 = arr[i * 4:(i + 1) * 4]
        # Doing power of 2 on the array view creates a temporary array! Oops.
        total += (slice_of_4 ** 2).mean()
    return total
```

Instead of allocating in every iteration, we can reuse a single array:

```{python}
@njit
def no_allocate_in_loop(arr):
    total = 0
    temp = np.zeros((4, ), dtype=arr.dtype)
    for i in range(len(arr) // 4):
        # This is just a view, so it doesn't allocate:
        slice_of_4 = arr[i * 4:(i + 1) * 4]
        # Replace the contents of temp with the values in slice_of_4:
        temp[:] = slice_of_4
        # These operations are in-place, so they don't allocate:
        temp *= slice_of_4
        total += temp.mean()
    return total

DATA = np.linspace(1_000_000, 0, 1_000_000, dtype=np.uint64)
assert allocate_in_loop(DATA) == no_allocate_in_loop(DATA)
```

The second implementation is faster, because it does no allocations in the hot inner loop, both reducing work and perhaps enabling additional compiler optimizations:

```{python}
#| echo: false
%%compare_timing

allocate_in_loop(DATA)
no_allocate_in_loop(DATA)
```
