# Get rid of repetitive and wasteful code

Once you've picked a good algorithm, you can make your implementation more efficient by removing repetitive, wasteful code.
Put more formally, if your algorithm is $O(n)$, you can model the performance with the formula $K¬∑n$, where $K$ is a constant tied to a specific implementation.
Your goal is to try to shrink the constant $K$ by making the code more efficient.
The code's scalability doen't change when you change $K$: doubling $n$ will still double the run time.
But for any given $n$, the code will run faster compared to the original version.

```{python}
#| echo: false
%load_ext book_magics
```

```{python}
# Import the dependencies we'll need:
import numpy as np
from numba import jit
```

## Focus your efforts on inner loops

As far as speed goes, the three most important locations in your data processing code are loops, loops, and loops.
A basic assumption in this book is that the code you're writing is used to process relatively large amounts of data.
That means you will almost always end up writing or calling functions with the following basic structure:

```{python}
#| eval: false
# This only gets called once:
do_some_setup()

for item in large_amount_of_data:
    # This gets called a lot:
    do_something(item)

# This only gets called once:
do_some_cleanup()
```

Given large amounts of data, and data structures like NumPy `ndarray`s that are designed for fast access from low-level languages, the bulk of your processing will happen inside the loop.
Code that runs a lot is sometimes called "hot" code.
You can speed up code running in a loop by:

1. Reducing the number of loop iterations. Perhaps there's data you don't need to process at all?
2. Speeding up the work you repeatedly do inside the loop, the code you will be calling over and over again.

If you have nested loops, the innermost loop will have the most iterations, and therefore is the most impactful place to fix inefficient code.

## Don't process data you don't need to

The fastest code is code you don't run at all.
So if you can avoid processing data altogether,  that can provide significant speed-ups.

As an example, let's implement the Sieve of Eratosthenes, a classic algorithm for finding prime numbers.
It works by iterating over all whole numbers in a range, crossing out multiples of 2 (4, 6, 8, ...), then multiples of 3 (6, 9, 12, ...), then multiples of 5 (10, 15, 20, ...), and so on.

```{python}
# üòé Remember that by using the @jit decorator, our Python code gets compiled
# to fast machine code.
@jit
def find_primes(length):
    is_prime = np.ones((length,), dtype=np.bool_)
    is_prime[0] = False
    is_prime[1] = False

    # For every number starting with 2:
    for i in range(2, length):
        # If we're already checked this number, we're done:
        if not is_prime[i]:
            continue
        # Otherwise, mark all multiples of i as not prime, since they're
        # multiples.
        for j in range(i*2, length, i):
            is_prime[j] = False

    return is_prime

PRIMES = find_primes(1_000_000)
```

Can we make this code any faster?
One thing to notice is that we're checking certain numbers twice.
When we rule out multiples of 5, we're ruling out `5 * 2 = 10` and `5 * 3 = 15` and `5 * 4 = 20` even though we've already ruled them out in previous rounds.
For example, `2 * 5 = 10` would have been previously ruled out as a multiple of 2.

That means we can tweak our `j` loop to start at a later point (`i * i` instead of `i * 2`) to prevent doing duplicate work:

```{python}
@jit
def find_primes_2(length):
    is_prime = np.ones((length,), dtype=np.bool_)
    is_prime[0] = False
    is_prime[1] = False

    for i in range(2, length):
        if not is_prime[i]:
            continue
        # üòé Anything smaller than i*i was already processed in an earlier
        # iteration, so no need to redo it.
        for j in range(i * i, length, i):
            is_prime[j] = False

    return is_prime

assert np.array_equal(PRIMES, find_primes_2(1_000_000))
```

This small tweak makes our algorithm run noticeably faster:

```{python}
#| echo: false
%%compare_timing
find_primes(1_000_000)
find_primes_2(1_000_000)
```

## Don't do unnecessary work inside loops

Continuing the same example, in our loop above we do an `if is_prime[i]: continue` check on all even values of `i`.
That includes even numbers like 8 or 212, even though we know that any even number larger than 2 isn't prime.
Instead of doing this check in every single iteration, we can split processing into two loops, one for odd numbers and one for even numbers.
The total number of iterations doesn't change, it's still `length`, but each iteration does less work:

```{python}
@jit
def find_primes_3(length):
    is_prime = np.ones((length,), dtype=np.bool_)
    is_prime[0] = False
    is_prime[1] = False

    # üòé Unconditionally rule out all even numbers larger than 2:
    for i in range(4, length, 2):
        is_prime[i] = False

    # üòé Now only check odd numbers:
    for i in range(3, length, 2):
        if not is_prime[i]:
            continue
        for j in range(i*i, length, i):
            is_prime[j] = False

    return is_prime

assert np.array_equal(PRIMES, find_primes_3(1_000_000))
```

This version is even faster:

```{python}
#| echo: false
%%compare_timing
find_primes_2(1_000_000)
find_primes_3(1_000_000)
```

Now that we're only iterating over odd numbers in the second loop, we apply another optimization in the inner-most `for j` loop.
Let's say we're ruling out multiples of 3.
We rule out the sequence `9`, `12`, `15`, `18`, and so on up to `length`.
Notice that every second number is even, because we're adding up pairs of odd numbers.
And those even numbers have already been marked as non-prime.

So we can optimize the code even further by only checking every second multiple:

```{python}
@jit
def find_primes_4(length):
    is_prime = np.ones((length,), dtype=np.bool_)
    is_prime[0] = False
    is_prime[1] = False

    for i in range(4, length, 2):
        is_prime[i] = False

    for i in range(3, length, 2):
        if not is_prime[i]:
            continue
        # üòé Only mark multiples that are odd (skipping ahead 2 * i in each
        # iteration), since even numbers have already been handled:
        for j in range(i*i, length, 2 * i):
            is_prime[j] = False

    return is_prime

assert np.array_equal(PRIMES, find_primes_4(1_000_000))
```

Once again, we've sped up the code:

```{python}
#| echo: false
%%compare_timing
find_primes_3(1_000_000)
find_primes_4(1_000_000)
```

## Avoid allocations in the hot loop

In compiled languages, creating an integer or float or other small type doesn't typically require memory allocation, and in our simplified mental model you can just assume it has no performance cost.
However, larger values that need to persist beyond the lifetime of the function call need to be allocated from the operating system (they're allocated on the "heap"), which is a more expensive operation.
Allocating memory can happen explicitly‚Äîfor example, in C you might call `malloc()`‚Äîor implicitly, depending on your language.

Allocating and freeing memory inside a loop can both make it harder for the compiler to optimize your code, and may also be wasted effort.
Instead, see if you can reuse allocations.

For our purposes, we'll focus on Numba, which allocates memory based on rules inherited from NumPy:

* Explicitly creating an array allocates memory, for example `np.ones((1_000,))`.
* Slicing an array does not allocate memory, it creates a view of the same underlying memory, for example `myarr[1:]`.
* Mathematical operations on arrays can create new arrays, thus allocating memory.
  For example, `myarr + 1` creates a new array where for every index `i`, the new array's value is `myarr[i] + 1`.

To see the performance impact of allocating in a loop, consider the following example:

```{python}
@jit
def allocate_in_loop(arr):
    total = 0
    for i in range(len(arr) // 4):
        # This is only a view, so it doesn't allocate:
        slice_of_4 = arr[i * 4:(i + 1) * 4]
        # üôÅ Doing power of 2 on the array view creates a temporary array! Oops.
        total += (slice_of_4 ** 2).mean()
    return total
```

Instead of allocating in every iteration, we can reuse a single array:

```{python}
@jit
def no_allocate_in_loop(arr):
    total = 0
    # üòé Create a single, temporary array we can reuse across loop iterations.
    temp = np.zeros((4, ), dtype=arr.dtype)
    for i in range(len(arr) // 4):
        slice_of_4 = arr[i * 4:(i + 1) * 4]
        # üòé Replace the contents of temp with the values in slice_of_4. This
        # is an in-place operation, so no allocation happens.
        temp[:] = slice_of_4
        # üòé Again, these operations are in-place, so they don't allocate:
        temp *= slice_of_4
        total += temp.mean()
    return total

DATA = np.linspace(1_000_000, 0, 1_000_000, dtype=np.uint64)
assert allocate_in_loop(DATA) == no_allocate_in_loop(DATA)
```

The second implementation is faster, because it does no allocations in the hot inner loop, both reducing work and perhaps enabling additional compiler optimizations:

```{python}
#| echo: false
%%compare_timing
allocate_in_loop(DATA)
no_allocate_in_loop(DATA)
```
