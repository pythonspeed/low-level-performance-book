# Microbenchmark your code to measure performance improvements

In the previous chapter we compared the timing of different implementations of the same algorithm.
Running a function once can give inconsistent timing results.

For example, we can use the `%time` magic in Jupyter or IPython to measure the time of running a function once:

```{python}
%time _ = sum(range(1_000))
%time _ = sum(range(1_000))
%time _ = sum(range(1_000))
```

Given an individual run can be inconsistent for a variety of reasons, it's better to run the code a large number of times, and report the average run time.
There are pre-existing utilities and libraries for doing this:

* Python has a [`timeit`](https://docs.python.org/3/library/timeit.html) module you can use for command-line or API-based microbenchmarking.
* In Jupyter or IPython you can use the `%timeit` magic.
* The [`pytest-benchmark`](https://pypi.org/project/pytest-benchmark/) package lets you write benchmark suites to be run by `pytest`.
* This book uses a microbenchmarking library I wrote myself, wrapping Python's `timeit` module.

Here's an example of the Jupyter `%timeit` magic:

```{python}
%timeit sum(range(1_000))
```

Later in the book we'll have a whole chapter on the pitfalls of microbenchmarking.
For now I'll just cover one particular problem you might hit you if you try running the code examples in this book yourself.

## Make sure you don't include Numba's compilation time in microbenchmarks

Most of the examples in this book use Numba.
The first time you run a function decorated with `@numba.jit` with a new set of types, the code will be compiled.
This is slow, and it's a one time cost, so you usually want to ignore this compilation time when benchmarking: after all, it doesn't reflect the speed of running the function the second, third, or millionth time.

Let's see this compilation time in action:

```{python}
from time import time
import numpy as np
from numba import jit

@jit
def arr_sum(arr):
    result = 0
    for i in range(len(arr)):
        result += arr[i]
    return result
```

The first time we call `arr_sum()` with an array of `np.int64`, the elapsed time will be much higher than subsequently, due to compilation:

```{python}
# One million int64s:
ARR_64 = np.ones((1_000_000,), dtype=np.int64)

print("The first time we run the function, it needs to be compiled:")
%time _ = arr_sum(ARR_64)

print("\nThe second and third run, compilation doesn't happen:")
%time _ = arr_sum(ARR_64)
%time _ = arr_sum(ARR_64)
```

If we call `arr_sum()` with an array of `np.uint32`, this will require Numba to compile a new version of the function, so once again the first run will be much slower:

```{python}
# One million uint32s:
ARR_32 = np.ones((1_000_000,), dtype=np.uint32)

print("A new type, so the function it needs to be recompiled:")
%time _ = arr_sum(ARR_32)

print("\nThe second and third run, compilation doesn't happen:")
%time _ = arr_sum(ARR_32)
%time _ = arr_sum(ARR_32)
```

**Make sure to run Numba functions at least once, with matching types, before doing any benchmarking.**
You'll notice every code example in the book does this.
This ensures the compilation time doesn't distort the benchmarks.
