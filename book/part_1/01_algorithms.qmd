# Use a scalable algorithm

Once you've decided on your software's architecture, a significant part of your code's speed depends on two sets of decisions embedded in your code:

1. Which algorithm you chose.
2. How you implemented the algorithm.
   For example, low-level languages like C usually run the same algorithm much faster than Python can.

In the rest of the book we'll focus on the second part, how exactly the algorithm is implemented.
But in this chapter we'll focus on the first decision.
The choice of algorithm is crucial because, as we'll see, if you pick the wrong algorithm the resulting performance problems will overwhelm any attempt to speed up the implementation.

```{python}
#| echo: false
%load_ext book_magics
```

## Avoid quadratic algorithms, accidental or otherwise

How long your program runs depends on the size of the input.
For example, processing a 1,000,000-base DNA sequence will almost certainly take longer than processing a 1,000-base sequence.
So when we discuss the choice of algorithm, we need to think in terms of scalability: how will speed change as the input size grows?
An algorithm that scales badly will be so slow that your implementation speed won't matter.
In particular, in this book I'm assuming you're dealing with sufficiently-large amounts of data that you're likely to hit algorithm-related scaling issues.

Let's consider an example: quadratic algorithms, which are very easy to write accidentally, to the point that there is [a blog devoted purely to showcasing examples](https://accidentallyquadratic.tumblr.com/).
A common source of accidentally quadratic algorithms involves a loop, and then call a function that doesn't look like a loop but actually causes another loop over the same data.

Before we can show the example, we need to briefly discuss the technology stack we'll be using for examples in this book, starting with the NumPy library.
NumPy's core data structure is the `ndarray`, which are N-dimensional arrays.
An `ndarray` is a bit like a standard Python list, except that `ndarray`s:

1. Are fixed in size: you can't append entries.
2. Only contain items of a single data type or "dtype", e.g. a 32-bit unsigned integer.
   This allows them to be accessed efficiently from compiled languages, unlike Python lists.
3. Can be multi-dimensional.

Let's implement an algorithm that operates on a pair on `ndarray`s.
Specifically, we'll implement a quadratic algorithm that filters an array down to only those items that are present in the second array.
We'll assume both arrays have approximately the same length.

```{python}
import numpy as np

def naive_filter_python(arr1, arr2):
    # Create a new array that is the same size and has the same
    # type as the first input array:
    result = np.empty(shape=arr1.shape, dtype=arr1.dtype)

    found_items = 0

    # This has len(arr1) steps. Because this for loop is implemented in
    # Python, it will be pretty slow, as it can't take advantage of NumPy's
    # faster APIs.
    for i in range(len(arr1)):
        item = arr1[i]
        # Checking if an item is contained in an array may require iterating
        # over the whole array, so this if statement takes len(arr2) steps:
        if item in arr2:
            result[found_items] = item
            found_items += 1

    # The result array may be too large, so shrink it down to the number of
    # items we found. A simple slice of a NumPy array doesn't copy the data, it
    # just creates a view pointing at the same underlying memory, so this is a
    # cheap operation:
    result = result[:found_items]
    return result
```

How long does this take to run for different input sizes?

```{python}
# Create an array with values from 0 to 999:
SMALL_DATA = np.arange(0, 1_000, dtype=np.uint64)
SMALL_DATA2 = np.arange(3, 1_003, dtype=np.uint64)
LARGER_DATA = np.arange(0, 10_000, dtype=np.uint64)
LARGER_DATA2 = np.arange(0, 10_003, dtype=np.uint64)
```

```{python}
#| echo: false
%%compare_timing --measure=instructions
naive_filter_python(SMALL_DATA, SMALL_DATA2)
naive_filter_python(LARGER_DATA, LARGER_DATA2)
```

When the data is 10× larger, the run time is rather more than 10× slower.
That's because the algorithm takes `len(arr1) × len(arr2)` steps.
If we assume both arrays have approximately the same length $N$, we can say the algorithm is $O(N^{2})$: it scales with the square of the length of the inputs.

## A faster language won't make an algorithm scale

Since this is a book about writing low-level compiled languages, let's see how fast our algorithm runs if we use a compiled language.
In particular, we will compile the code using [Numba](https://numba.pydata.org/).
Numba is a programming language that implements a subset of Python, but compiles to machine code, and has native support for NumPy arrays.
Keep in mind that this is not a book about Numba: the concepts the book covers also apply to C, C++, Rust, and other low-level languages.

Numba is useful for educational purposes because it is so easy to use: just decorate a function with `@numba.jit`.
The first time you call this function with a set of arguments with specific types, Numba will generate machine code customized for those particular types.
Subsequent runs will use the pre-compiled version and run extremely quickly.

```{python}
from numba import jit

# This is approximately the same code as naive_filter_python() above,
# but it will be compiled to machine code the first time it is called
# with particular types because of the @jit decorator.
@jit
def naive_filter_compiled(arr1, arr2):
    # When NumPy APIs are used, Numba uses an equivalent, Numba-specific
    # reimplemented version.
    result = np.empty(shape=arr1.shape, dtype=arr1.dtype)

    found_items = 0
    for i in range(len(arr1)):
        item = arr1[i]
        for j in range(len(arr2)):
            if arr2[j] == item:
                result[found_items] = item
                found_items += 1
                break

    result = result[:found_items]
    return result

assert np.array_equal(
    naive_filter_python(SMALL_DATA, SMALL_DATA2),
    naive_filter_compiled(SMALL_DATA, SMALL_DATA2)
)
```

How does this version scale?

```{python}
#| echo: false
%%compare_timing --measure=instructions
naive_filter_compiled(SMALL_DATA, SMALL_DATA2)
naive_filter_compiled(LARGER_DATA, LARGER_DATA2)
```

This is a lot faster than our original Python algorithm.
But because it uses the same $O(N^{2})$ it is still going to scale badly as the input size grows.

## Linear algorithms beat quadratic algorithms

Let's try a different approach: a more scalable algorithm.
Instead of a run time that scales $O(N^{2})$ with input size $N$, the following algorithm scales $O(N×k)$, where $k$ is a fixed constant.
We will implement this more scalable algorithm in Python, without bothering to use a compiled language:

```{python}
# This is still written in Python, a slow language!
def smarter_filter_python(arr1, arr2):
    result = np.empty(shape=arr1.shape, dtype=arr1.dtype)

    # This scales linearly with the size of arr2:
    arr2_as_set = set(arr2)

    found_items = 0

    for i in range(len(arr1)):
        item = arr1[i]
        # Checking if an item is contained in a Python set takes a fixed amount
        # of time, unlike checking membership in a list or array.
        if item in arr2_as_set:
            result[found_items] = item
            found_items += 1

    result = result[:found_items]
    return result

assert np.array_equal(
    naive_filter_python(SMALL_DATA, SMALL_DATA2),
    smarter_filter_python(SMALL_DATA, SMALL_DATA2),
)
```

Let's compare our implementation above that is implemented in a fast, compiled language (Numba) to our new implementation that is written in a slow language (Python).
We'll use the `benchit` library to generate a graph of run time over a series of inputs of increasing size:

```{python}
#| output: false
import benchit

inputs = {
    size: (np.arange(0, size, dtype=np.uint64),
           np.arange(3, size + 3, dtype=np.uint64))
    for size in range(100, 2000, 100)
}

timings = benchit.timings(
    [naive_filter_compiled, smarter_filter_python],
    inputs,
    multivar=True,
    input_name="Array length",
)
```

Here's what the resulting run time looks like; lower values on the Y axis are better:

```{python}
timings.plot(logx=False, logy=False, dpi=100, figsize=(8, 4), fontsize=12)
```

For small inputs, the Python version is slower than the compiled version.
But once inputs are large enough, the linear algorithm in Python is faster than the compiled (but quadratic) Numba version.
**This why it's so important to pick a scalable algorithm: once your inputs are large enough, a non-scalable algorithm will overwhelm any benefits you get from switching to a faster programming language.**

Once you've picked a scalable algorithm, what else can you do to speed up your code?
That's what we'll be covering in the rest of the book.
