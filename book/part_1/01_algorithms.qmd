# Use a scalable algorithm

How long it takes your program to run depends on two sets of decisions embedded in your code:

1. The choice of algorithm.
2. The implementation.
   For example, as we'll be repeating repeatedly, low-level languages like C usually run the same algorithm much faster than Python can.

In the rest of the book we'll focus on the implementation, but the choice of algorithm is just as important, as we'll see in this chapter.

```{python}
#| echo: false
%load_ext book_magics
```

## Avoid quadratic algorithms, accidental or otherwise

How long your program runs depends on the size of the input: processing a 1,000,000-base DNA sequence will probably take longer than processing a 1,000-base sequence.
And once you're dealing with large input sizes, an algorithm that scales badly will be so slow that your implementation speed won't matter.

In much of this book we'll be using the NumPy library for examples.
NumPy's core data structure, the `ndarray`, is a bit like a standard Python list, except that `ndarrays`s:

1. Are fixed in size.
2. Only contain items of a single data type or "dtype", e.g. a 32-bit unsigned integer.
3. Can be multi-dimensional.

Because of the second item an `ndarray` can be accessed efficiently from compiled languages, unlike Python lists.

Let's consider an example: quadratic algorithms, which are very easy to write accidentally, to the point that there is [a blog showcasing examples](https://accidentallyquadratic.tumblr.com/).
A typical mistake is to have a loop, and then call a function that doesn't look like a loop but actually causes another loop over the same data.

Here's an example, a naive implementation of a quadratic algorithm that filters an array down to only those items that are present in the second array.
We'll assume both arrays have approximately the same length.

```{python}
import numpy as np

def naive_filter_python(arr1, arr2):
    # Create a new array that is the same size and has the same
    # type as the first input array:
    result = np.empty(shape=arr1.shape, dtype=arr1.dtype)

    found_items = 0

    # This has len(arr1) steps:
    for i in range(len(arr1)):
        item = arr1[i]
        # Checking if an item is contained in an array may require iterating
        # over the whole array, so len(arr2) steps:
        if item in arr2:
            result[found_items] = item
            found_items += 1

    # A simple slice of a NumPy array doesn't copy the data, it just creates
    # a view pointing at the same underlying memory:
    result = result[:found_items]
    return result
```

How long does this take to run for different input sizes?

```{python}
# Create an array with values from 0 to 999:
SMALL_DATA = np.arange(0, 1_000, dtype=np.uint64)
SMALL_DATA2 = np.arange(3, 1_003, dtype=np.uint64)
LARGER_DATA = np.arange(0, 10_000, dtype=np.uint64)
LARGER_DATA2 = np.arange(0, 10_003, dtype=np.uint64)
```

```{python}
#| echo: false
%%compare_timing --measure=instructions
naive_filter_python(SMALL_DATA, SMALL_DATA2)
naive_filter_python(LARGER_DATA, LARGER_DATA2)
```

When the data is 10× larger, the run time is more than 100× slower!
That's because the algorithm takes `len(arr1) × len(arr2)` steps.
If we assume both arrays have approximately the same length `N`, we can say the algorithm is $O(N^{2})$: it scales with the square of the length of the inputs.

## A faster language won't make an algorithm scale

Since this is a book about using low-level compiled languages, let's see how fast that runs, by using [Numba](https://numba.pydata.org/).
Numba is a programming language that implements a subset of Python, but compiles to machine code, and has native support for NumPy arrays.
However, this is not a book about Numba, and the concepts we cover should apply to C, C++, Rust, and other low-level languages.

The reason Numba is helpful is because it's so easy to use: just decorate a function with `@numba.njit`.
The first time you call this function with a set of arguments with specific types, Numba will generate machine code customized for those particular types.
Subsequent runs will use the pre-compiled version and run extremely quickly.

```{python}
from numba import njit

# This is approximately the same code as naive_filter_python() above,
# but it will be compiled to machine code the first time it is called
# with particular types because of the @njit decorator.
@njit
def naive_filter_compiled(arr1, arr2):
    # When NumPy APIs are used, Numba uses an equivalent, Numba-specific
    # reimplemented version.
    result = np.empty(shape=arr1.shape, dtype=arr1.dtype)

    found_items = 0
    for i in range(len(arr1)):
        item = arr1[i]
        for j in range(len(arr2)):
            if arr2[j] == item:
                result[found_items] = item
                found_items += 1
                break

    result = result[:found_items]
    return result

assert np.array_equal(
    naive_filter_python(SMALL_DATA, SMALL_DATA2),
    naive_filter_compiled(SMALL_DATA, SMALL_DATA2)
)
```

How does this version scale?

```{python}
#| echo: false
%%compare_timing --measure=instructions
naive_filter_compiled(SMALL_DATA, SMALL_DATA2)
naive_filter_compiled(LARGER_DATA, LARGER_DATA2)
```

This is a lot faster, but it is still going to run vastly slower as the input size grows.

## Linear algorithms beat quadratic algorithms

Let's try a different approach: a faster algorithm.
Whereas checking membership in an array scales linearly with the size of the array, checking membership in a set takes a constant amount of time.

```{python}
# This is still written in Python, a slow language!
def smarter_filter_python(arr1, arr2):
    result = np.empty(shape=arr1.shape, dtype=arr1.dtype)

    # This scales linearly with the size of arr2:
    arr2_as_set = set(arr2)

    found_items = 0

    for i in range(len(arr1)):
        item = arr1[i]
        # Checking if an item is contained in a Python set takes a fixed amount
        # of time:
        if item in arr2_as_set:
            result[found_items] = item
            found_items += 1

    result = result[:found_items]
    return result

assert np.array_equal(
    naive_filter_python(SMALL_DATA, SMALL_DATA2),
    smarter_filter_python(SMALL_DATA, SMALL_DATA2),
)
```

Now, let's compare our quadratic compiled implementation to our new, Python-based but linear implementation.
We'll use the `benchit` library to generate a graph of run time over a series of inputs of increasing size:

```{python}
#| output: false
import benchit

inputs = {
    size: (np.arange(0, size, dtype=np.uint64),
           np.arange(3, size + 3, dtype=np.uint64))
    for size in [100, 1000, 10_000]
}

timings = benchit.timings(
    [naive_filter_compiled, smarter_filter_python],
    inputs,
    multivar=True,
    input_name="Array length",
)
```

Here's what the resulting run time looks like; lower values on the Y axis are better:

```{python}
timings.plot(logx=True, logy=True, dpi=100, figsize=(8, 4))
```

For small inputs, the Python version is slower than the compiled version.
But once inputs are large enough, the linear algorithm in Python is faster than the compiled (but quadratic) Numba version.

**That's why it's so important to pick a scalable algorithm: once your inputs are large enough, a non-scalable algorithm will overwhelm any benefits you get from switching to a faster programming language.**
In this book I'm assuming you're dealing with sufficiently-large amounts of data, where the choice of algorithm is critical.
But once you've picked a scalable algorithm, what else can you do to speed up your code?
That's what we'll be covering in the rest of the book.
