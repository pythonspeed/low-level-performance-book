# Trade precision and/or accuracy for speed

In our previous chapter we aimed to do the same calculation, with the same results, only faster.
Another potential optimization tied to the specifics of your particular data is relaxing the precision or even accuracy of your code.
In many cases, a performance optimization that changes results slightly may be acceptable and even desirable:

* The results you're calculating may be more "precise" than the error caused by your input data.
  If that extra precision is slowing your code down, you can reduce your precision and still got results that are faster and still just as accurate.
  A generic library has no way of knowing what level of precision is acceptable, so it can't make this sort of compromise.
* In other cases, less accurate results may also be acceptable.
  For example, the monetary cost of inaccuracy may be much lower than the monetary cost savings from a faster run time.

A generic library like NumPy can't make this decision for you, so once again this will require us to implement our own function.

## Example: Medians once again

Let's look at some example data from the realm of public transportation: bus "headways", the time between the arrival of the previous and the next bus.
Headways need to be as low as possible so that passengers can rely on the bus: if the bus arrives every 5 minutes that's a lot better than the bus arriving every 10 minutes.

Our goal is to compute the median headway experienced by passengers on my local transit authority's buses:

```{python}
#| echo: false
%load_ext book_magics
```

```{python}
# Import the dependencies we'll need:
import numpy as np
from numba import jit
import pandas as pd

BUS_HEADWAYS = pd.read_parquet(
    "../data/MBTA-bus-2022-05.parquet",
    columns=["headway"]
)["headway"].dropna().values

print("Size:", len(BUS_HEADWAYS))
print("dtype:", BUS_HEADWAYS.dtype)
```

NumPy has built-in support for calculating the median, so we could use that:

```{python}
print("Median (seconds):", np.median(BUS_HEADWAYS))
```

## Inspect the data to get a sense of how you can optimize your algorithm

We've already seen we can use a histogram to calculate a median, but will that work here?
Let's look at the data:

```{python}
print("Min value (seconds):", int(BUS_HEADWAYS.min()))
print("Max value (seconds):", int(BUS_HEADWAYS.max()))
print(
    "99.9th percentile (seconds):",
    int(np.quantile(BUS_HEADWAYS, 0.999))
)
```

If we have were to have a bucket for every potential value, as we did in the previous chapter, we'd need 75,000 buckets.
Creating a histogram and finding the median with that many buckets will likely be a lot slower, since we'll need to do a lot more addition to find the median.

But there's an additional problem: in the previous chapter, we knew values were limited to whatever fits in a `uint8`, i.e. 0 to 255.
So we knew we only needed 256 buckets.
In this case... it's a bit hard to tell. We'd probably want 100,000 buckets, just to be safe.

Let's try that, then:

```{python}
@jit
def histogram_median(bus_headways):
    histogram = np.zeros((100_000,), dtype=np.uint32)
    for headway in bus_headways:
        histogram[int(headway)] += 1

    # Now that we know how many samples there are for every quarter minute
    # index, add up the cumulative counts until we hit 50% of values:
    median_location = len(bus_headways) / 2
    cumulative_samples = 0
    median = 0
    for i in range(len(histogram)):
        cumulative_samples += histogram[i]
        if cumulative_samples >= median_location:
            median = i
            break

    return median
```

We can compare our algorithm to the built-in NumPy `np.median()`:

```{python}
print("Median:", np.median(BUS_HEADWAYS), "secs")
print("Histogram median:", histogram_median(BUS_HEADWAYS), "secs")
```

And we can compare the run time of the two:

```{python}
#| echo: false
%%compare_timing
np.median(BUS_HEADWAYS)
histogram_median(BUS_HEADWAYS)
```

## A lossy algorithm may suffice if it's losing unimportant information

But maybe we can get away with a small number of buckets anyway.
Given the above data range, and an understanding of the problem domain, we can build a histogram-based median algorithm using two reasonable assumptions:

**Super-long headways don't have to be very accurate** As we can see above, 99.9% of headways are less than 68 minutes (4067 seconds).
And from a passenger experience perspective, the impact of headways is much more meaningful for frequent buses.
For example, a change from a 5-minute to a 15-minute headway can have a major negative impact on the usefulness of a bus route.
On the other hand, a bus route that runs every 2 hours is almost as awful as a bus that runs every 3 hours.

**Reducing accuracy to 15 seconds is reasonable:** The data we have is at a 1-second resolution, but it's unlikely that a headway of 67 seconds really make a difference versus a headway of 66 seconds.
Plus, the arrival and departure of a bus at a stop takes a few seconds, and it's doubtful that the timing data is actually as accurate as it is precise.

Using these two constraints, we can write the following code:

```{python}
@jit
def lossy_histogram_median(bus_headways):
    # Histogram by number of quarter-minutes (15 seconds), with a maximum of
    # 500 × 15 = 7,500 seconds.
    by_quarter_minutes = np.zeros((500,), dtype=np.uint32)
    for headway in bus_headways:
        headway = int(headway)
        number_quarter_minutes = np.uint16(headway / 15)
        # If the data is too large, use the highest bucket. Since 7500 is
        # higher than the 99.9th percentile value, we're underestimating less
        # than 0.1% of the data.
        by_quarter_minutes[min(number_quarter_minutes, 499)] += 1

    # Now that we know how many samples there are for every quarter minute
    # index, add up the cumulative counts until we hit 50% of values:
    median_location = len(bus_headways) / 2
    cumulative_samples = 0
    median = 0
    for i in range(len(by_quarter_minutes)):
        cumulative_samples += by_quarter_minutes[i]
        if cumulative_samples >= median_location:
            median = i
            break

    # Convert back to seconds:
    return median * 15.0
```

We can compare our algorithm to the built-in NumPy `np.median()`, and see that the results are pretty close.
This is partially luck, but even in the worst case we won't be off by more than half a minute:

```{python}
print("Median:", np.median(BUS_HEADWAYS), "secs")
print("Approximate median:", lossy_histogram_median(BUS_HEADWAYS), "secs")
```

And if we compare the run time of the two, our custom algorithm is much faster:

```{python}
#| echo: false
%%compare_timing
np.median(BUS_HEADWAYS)
lossy_histogram_median(BUS_HEADWAYS)
```

Our new algorithm wouldn't work with generic inputs: it makes many assumptions about the distribution and range of the data.
And the result it gives are less accurate than a normal median calculation.
But for this _specific_ situation and data, it's good enough—and much faster than a general purpose algorithm.
