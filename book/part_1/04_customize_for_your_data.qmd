# Trade precision and/or accuracy for speed

In our previous chapter we aimed to do the same calculation, with the same results, only faster.
Another potential optimization tied to the specifics of your particular data is relaxing the precision or even accuracy of your code.
In many cases, a performance optimization that changes results slightly may be acceptable and even desirable:

* The results you're calculating may be more "precise" than the error caused by your input data.
  If that extra precision is slowing your code down, you can reduce your precision and still got results that are faster and still just as accurate.
  A generic library has no way of knowing what level of precision is acceptable, so it can't make this sort of compromise.
* In other cases, less accurate results may also be acceptable.
  For example, the monetary cost of inaccuracy may be much lower than the monetary cost savings from a faster run time.

A generic library like NumPy can't make this decision for you, so once again this will require us to implement our own function.

## Example: Back to median-filter local thresholding

Let's return to the example in the previous chapter, looking at an intermediate step for clarity.
We implemented a histogram-based median as a optimization of Numba's generic median implementation:

```{python}
#| echo: false
%load_ext book_magics
```

```{python}
from skimage.data import page
import numpy as np
from numba import jit

IMAGE = page()


@jit
def histogram_median(img):
    histogram = np.zeros((256,), dtype=np.uint32)

    for y in range(img.shape[0]):
        for x in range(img.shape[1]):
            value = img[y, x]
            histogram[value] += 1

    half_neighborhood_size = img.shape[0] * img.shape[1] // 2
    for l in range(256):
        half_neighborhood_size -= histogram[l]
        if half_neighborhood_size < 0:
            break
    return l


@jit
def median_local_threshold2(img, neighborhood_size, offset):
    assert neighborhood_size % 2 == 1
    radius = (neighborhood_size - 1) // 2
    result = np.empty(img.shape, dtype=np.uint8)

    for i in range(img.shape[0]):
        min_y = max(i - radius, 0)
        max_y = min(i + radius + 1, img.shape[0])
        for j in range(img.shape[1]):
            min_x = max(j - radius, 0)
            max_x = min(j + radius + 1, img.shape[1])

            neighborhood = img[min_y:max_y, min_x:max_x]
            median = histogram_median(neighborhood)

            if img[i, j] > median - offset:
                result[i, j] = 255
            else:
                result[i, j] = 0
    return result

NUMBA_RESULT = median_local_threshold2(IMAGE, 11, 13)
```

Here's the resulting image:

```{python}
#| echo: false
%display_image NUMBA_RESULT
```

## Reduce your algorithm's precision

This algorithm is a bit of a hack:

* It uses magic constants: the neighborhood size and the offset.
* While a median is a reasonable way to choose a threshold, there are others.
* The way we're judging the results is just by looking at the image and deciding if it's good enough.

So if we change the results slightly, as long as we still think the results look OK, that's perfectly fine.

So let's change the histogram algorithm to use 64 buckets instead of 256.
That should speed up the threshold calculation, and while as a _median_ calculation it will be less accurate, as a _threshold_ calculation it may still be sufficient.

```{python}
@jit
def approximate_histogram_median(img):
    # ðŸ˜Ž Use a smaller number of buckets:
    histogram = np.zeros((64,), dtype=np.uint32)

    for y in range(img.shape[0]):
        for x in range(img.shape[1]):
            value = img[y, x]
            # Convert the 0-255 value into one of 64 buckets:
            histogram[value // 4] += 1

    visited_pixels = img.shape[0] * img.shape[1] // 2
    for l in range(64):
        visited_pixels -= histogram[l]
        if visited_pixels < 0:
            break

    # Convert back to 0-255 range:
    return l * 4


@jit
def median_local_threshold2_approximate(img, neighborhood_size, offset):
    assert neighborhood_size % 2 == 1
    radius = (neighborhood_size - 1) // 2
    result = np.empty(img.shape, dtype=np.uint8)

    for i in range(img.shape[0]):
        min_y = max(i - radius, 0)
        max_y = min(i + radius + 1, img.shape[0])
        for j in range(img.shape[1]):
            min_x = max(j - radius, 0)
            max_x = min(j + radius + 1, img.shape[1])

            neighborhood = img[min_y:max_y, min_x:max_x]

            # ðŸ˜Ž Use a slightly less accurate median implementation:
            median = approximate_histogram_median(neighborhood)

            if img[i, j] > median - offset:
                result[i, j] = 255
            else:
                result[i, j] = 0
    return result

NUMBA_RESULT2 = median_local_threshold2_approximate(IMAGE, 11, 13)
```

Here's the resulting image:

```{python}
#| echo: false
%display_image NUMBA_RESULT2
```

It is very slightly different than the previous image, but not in a meaningful way.
And here's the performance of our new implementation:

```{python}
#| echo: false
%%compare_timing
median_local_threshold2(IMAGE, 11, 13)
median_local_threshold2_approximate(IMAGE, 11, 13)
```

## Fine-tune parameters to maximize performance

In addition to tweaking our algorithm, we can also tweak parameters.
For example, we can use a smaller neighborhood size:

```{python}
NUMBA_RESULT3 = median_local_threshold2_approximate(IMAGE, 9, 13)
```

Here's the resulting image:

```{python}
#| echo: false
%display_image NUMBA_RESULT3
```

This may be slightly worse for some letters, like the "e" in "values", but it's not that different.
And it gives us another small speed-up:

```{python}
#| echo: false
%%compare_timing
median_local_threshold2_approximate(IMAGE, 11, 13)
median_local_threshold2_approximate(IMAGE, 9, 13)
```

Are these optimizations worth it?
In this case I'd say yes.
Whether they'd be worth it for other images depends very much on the specific images and on the specific requirements for the algorithm.
