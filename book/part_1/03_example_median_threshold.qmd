# Fine-tune algorithms to your particular data

Libraries like NumPy and SciPy are general purpose: they need to run correctly for many different users, with a variety of different requirements.
When you are writing a custom application, for a specific kind of data, your problem is more constrained.
And that means opportunities to speed up your code, by customizing your calculations, algorithms, and data structures specifically for your particular situation and data.

## Example: Median-based local thresholding

Let's say we have an image, a photo of some text from a book.
We want to turn it into an image with only two colors, black and white, perhaps as a preliminary to running it though an OCR library that can turn an image into text.
We'll use an example image from [scikit-image](https://scikit-image.org/), an excellent image processing library:

```{python}
#| echo: false
%load_ext book_magics
```

```{python}
from skimage.data import page
import numpy as np
IMAGE = page()
assert IMAGE.dtype == np.uint8
```

Here's what it looks like (it's licensed under [this license](https://github.com/scikit-image/scikit-image/blob/main/LICENSE.txt)):

```{python}
#| echo: false
%display_image IMAGE
```

The task we're trying to do—turning darker areas into black, and lighter areas into white—is called thresholding.
Since the image is different in different regions, with some darker and some lighter, we'll get the best results if we use local thresholding, where the threshold is calculated from the pixel's neighborhood.

Simplifying somewhat, for each pixel in the image we will:

1. Calculate the median of the surrounding neighborhood.
2. Subtract a magic constant from the calculated median to calculate our local threshold.
3. If the pixel's value is bigger than the threshold, the result is white, otherwise it's black.

scikit-image includes an implementation of this algorithm.
Here's how we use it:

```{python}
from skimage.filters import threshold_local

def skimage_median_local_threshold(img, neighborhood_size, offset):
    # Create a threshold array, the median of each pixel's neighborhood:
    threshold = threshold_local(
        img, block_size=neighborhood_size, method="median", offset=offset
    )
    # Create array indicating where the pixel is bigger than threshold, with 0s
    # and 1s:
    result = (img > threshold).astype(np.uint8)
    # Convert to 0 and 255 (i.e. black and white):
    result *= 255
    return result

# The neighborhood size and offset value were determined "empirically", i.e.
# they're manually tuning the algorithm to work well with our specific example
# image.
SKIMAGE_RESULT = skimage_median_local_threshold(IMAGE, 11, 13)
```

And here's what the results look like:

```{python}
#| echo: false
%display_image SKIMAGE_RESULT
```

Let's see if we can make this faster!

## Use a compiled language for more flexibility

We'll start by switching to Numba.
Here's an initial implementation of the algorithm; it's not quite identical to the original, for example the way edge pixels are handled, but it's close enough for our purposes:

```{python}
from numba import jit

@jit
def median_local_threshold1(img, neighborhood_size, offset):
    # Neighborhood size must be an odd number:
    assert neighborhood_size % 2 == 1
    radius = (neighborhood_size - 1) // 2
    result = np.empty(img.shape, dtype=np.uint8)

    # For every pixel:
    for i in range(img.shape[0]):
        # Calculate the Y borders of the neighborhood:
        min_y = max(i - radius, 0)
        max_y = min(i + radius + 1, img.shape[0])
        for j in range(img.shape[1]):
            # Calculate the X borders of the neighborhood:
            min_x = max(j - radius, 0)
            max_x = min(j + radius + 1, img.shape[1])
            # Calculate the median:
            median = np.median(img[min_y:max_y, min_x:max_x])
            # Set the image to black or white, depending how it relates to the
            # threshold:
            if img[i, j] > median - offset:
                # White:
                result[i, j] = 255
            else:
                # Black:
                result[i, j] = 0
    return result

NUMBA_RESULT1 = median_local_threshold1(IMAGE, 11, 13)
```

Here's the resulting image; it looks similar enough that for our purposes:

```{python}
#| echo: false
%display_image NUMBA_RESULT1
```

Now we can compare the performance of the two implementations:

```{python}
#| echo: false
%%compare_timing
skimage_median_local_threshold(IMAGE, 11, 13)
median_local_threshold1(IMAGE, 11, 13)
```

Depending on your CPU, this might be slightly slower or slightly faster.
But that's OK, we're just getting started.

## Adjust your algorithm to take advantage of your data type

I'm going to guess that most of the time in this function is tied to calculating the median.
How do I know that?
For one thing, the median needs to look at all the neighbors of any given pixel; for a 11×11 neighborhood, that's 121 pixels to look at, which seems like it would be far more expensive than the thresholding step.
With a more complex function I would likely validate my assumption with a profiler, but one could certainly use a profiler in this case as well.

Now, the median implementation Numba provides is likely to be fairly generic, since it will need to work in a wide variety of circumstances.
We can hypothesize that it's not optimized for our particular case.
And even if it is, having our own implementation will allow for a second round of optimization, as we'll see in the next step.

We're going to implement a histogram-based median, based on the fact we're using 8-bit images that only have a limited range of potential values.
Here's the basic algorithm for a histogram-based median:

* Iterate over each pixel in the chosen neighborhood.
* Each pixel's value will increase a corresponding bucket in the histogram; since we know our image is 8-bit, we only need 256 buckets.
  For example, if a pixel has brightness value of 17, the count in bucket 17 will get incremented.
  Once we're done constructing the histogram, if for example the 17th bucket has a value of 32, that means 32 pixels had a brightness of 17.
* Then, we add up the size of each bucket in the histogram, from smallest to largest, until we hit 50% of the pixels we inspected, i.e. half the size of the neighborhood in pixels.
  That 50% point is the median, since 50% of the pixels are smaller in value and 50% of the pixels were larger in value.

To give a sense of what the histogram looks like, we can implement that subset of the algorithm, call it on the neighborhood of a particular pixel, and plot the result:

```{python}
import matplotlib.pyplot as plt

# This isn't actually used in the implementation below, we're just implementing
# this subset of the algorithm for educational purposes:
def plot_neighborhood_histogram(y, x):
    # Build a 256-bucket histogram, based on the values of the pixels in the
    # specified 11×11 neighborhood:
    histogram = np.zeros((256,), dtype=np.uint32)
    for i in range(y - 5, y + 5):
        for j in range(x - 5, x + 5):
            histogram[IMAGE[i, j]] += 1

    # Draw a barchart of the histogram:
    plt.bar(range(256), histogram)
    plt.ylabel("Number of pixels with that brightness")
    plt.xlabel("Buckets for each level of pixel brightness")
    plt.show()

plot_neighborhood_histogram(100, 100)
```

If we look at this histogram, we can estimate just by eye that the point where half the pixels are on one side and half on the other, which is to say the median, will be somewhere between 140 and 160.
And indeed:

```{python}
print("Median value of this neighborhood:", np.median(IMAGE[95:105, 95:105]))
```

In our full algorithm we're going to find the median value with code, rather than visually, but the same principle applies: we're finding the bucket where half the counted pixels are on each side.
Let's reimplement our local thresholding algorithm using a histogram-based median:

```{python}
@jit
def histogram_median(img):
    # A histogram with a bucket for each of the 8-bit values
    # possible in the image.
    histogram = np.zeros((256,), dtype=np.uint32)

    # Populate the histogram, counting how many of each value are in
    # the neighborhood we're inspecting:
    for y in range(img.shape[0]):
        for x in range(img.shape[1]):
            value = img[y, x]
            histogram[value] += 1

    # Use the histogram to find the median; keep adding buckets until
    # we've hit 50% of the pixels. The corresponding bucket is the
    # median.
    half_neighborhood_size = img.shape[0] * img.shape[1] // 2
    for l in range(256):
        half_neighborhood_size -= histogram[l]
        if half_neighborhood_size < 0:
            break
    return l


@jit
def median_local_threshold2(img, neighborhood_size, offset):
    assert neighborhood_size % 2 == 1
    radius = (neighborhood_size - 1) // 2
    result = np.empty(img.shape, dtype=np.uint8)

    for i in range(img.shape[0]):
        min_y = max(i - radius, 0)
        max_y = min(i + radius + 1, img.shape[0])
        for j in range(img.shape[1]):
            min_x = max(j - radius, 0)
            max_x = min(j + radius + 1, img.shape[1])

            # Get the neighborhood of the pixel:
            neighborhood = img[min_y:max_y, min_x:max_x]

            # Calculate its median:
            median = histogram_median(neighborhood)

            if img[i, j] > median - offset:
                result[i, j] = 255
            else:
                result[i, j] = 0
    return result

NUMBA_RESULT2 = median_local_threshold2(IMAGE, 11, 13)
```

Here's the resulting image:

```{python}
#| echo: false
%display_image NUMBA_RESULT2
```

And here's the performance of our new implementation:

```{python}
#| echo: false
%%compare_timing
median_local_threshold1(IMAGE, 11, 13)
median_local_threshold2(IMAGE, 11, 13)
```

That's faster!

## Validate performance against a wide range of data

Or is it?
We've only tested it against a single image, with a single neighborhood size of 11×11.
To actually know whether it's faster in general, we'll want to test it against a broad range of realistic data.

Since this is just an educational demonstration, I don't have more images available.
But we can compare different neighborhood sizes, and determine that our algorithm is faster in all cases.
The larger the neighborhood, the more of a speed benefit we get from the histogram-based median:

```{python}
#| echo: false
%%compare_timing
median_local_threshold1(IMAGE, 3, 13)
median_local_threshold1(IMAGE, 21, 13)
median_local_threshold2(IMAGE, 3, 13)
median_local_threshold2(IMAGE, 21, 13)
```

And we can also compare the edge cases of completely black and completely white images:

```{python}
BLACK_IMAGE = np.zeros((256, 256), dtype=np.uint8)
WHITE_IMAGE = np.ones((256, 256), dtype=np.uint8)
WHITE_IMAGE *= 255
```

These edge cases are informative insofar as our median algorithm cumulatively adds up the buckets in the histogram until it hits 50% of pixels in the neighborhood.
That mean very dark or very light medians will involve adding up fewer or more buckets, and therefore have different performance.
And in fact when run on a completely white image, our new algorithm is slightly slower:

```{python}
#| echo: false
%%compare_timing
median_local_threshold1(BLACK_IMAGE, 11, 13)
median_local_threshold1(WHITE_IMAGE, 11, 13)
median_local_threshold2(BLACK_IMAGE, 11, 13)
median_local_threshold2(WHITE_IMAGE, 11, 13)
```

Given we're not done optimizing, that's something we can live with.

## Take advantage of similar calculations you've already done

Our algorithm uses a rolling neighborhood or window over the image, calculating the median for a window around each pixel.
And the neighborhood for one pixel has a significant overlap for the neighborhood of the next pixel.
For example, let's say we're looking at a neighborhood size of 3.
We might calculate the median of this area:

```
......
.\\\..
.\\\..
.\\\..
......
......
```

And then when process the next pixel we'll calculate the median of this area:

```
......
..///.
..///.
..///.
......
......
```

If we superimpose them, we can see there's an overlap, the `X`:

```
......
.\XX/.
.\XX/.
.\XX/.
......
......
```

Given the histogram for the first pixel, if we remove the values marked with `\` and add the ones marked with `/`, we've calculated the exact histogram for the second pixel.
So for a 3×3 neighborhood, instead of processing 3 columns we process 2, a minor improvement.
For a 11×11 neighborhood, we will go from processing 11 columns to 2 columns, a much more significant improvement.

Here's what the code looks like:

```{python}
@jit
def median_local_threshold3(img, neighborhood_size, offset):
    assert neighborhood_size % 2 == 1
    radius = (neighborhood_size - 1) // 2
    result = np.empty(img.shape, dtype=np.uint8)
    histogram = np.empty((256,), dtype=np.uint32)

    for i in range(img.shape[0]):
        min_y = max(i - radius, 0)
        max_y = min(i + radius + 1, img.shape[0])

        # Populate histogram as if we started one pixel to the left:
        histogram[:] = 0
        for neighbor_y in range(min_y, max_y):
            for neighbor_x in range(0, radius):
                value = img[neighbor_y, neighbor_x]
                histogram[value] += 1

        for j in range(img.shape[1]):
            min_x = max(j - radius, 0)
            max_x = min(j + radius + 1, img.shape[1])

            # 😎 Instead of recalculating histogram from scratch, re-use the
            # previous pixel's histogram.

            # Substract left-most column we don't want anymore:
            if min_x > 0:
                for y in range(min_y, max_y):
                    histogram[img[y, min_x - 1]] -= 1

            # Add new right-most column:
            if max_x < img.shape[1]:
                for y in range(min_y, max_y):
                    histogram[img[y, max_x - 1]] += 1

            # Find the the median from the updated histogram:
            half_neighborhood_size = ((max_y - min_y) * (max_x - min_x)) // 2
            for l in range(256):
                half_neighborhood_size -= histogram[l]
                if half_neighborhood_size < 0:
                    break
            median = l

            if img[i, j] > median - offset:
                result[i, j] = 255
            else:
                result[i, j] = 0
    return result

NUMBA_RESULT3 = median_local_threshold3(IMAGE, 11, 13)
```

Notice that we've lost some of the abstraction we had in `median_local_threshold2()`.
Rather than a separate, reusable histogram-based median function, the median calculation is now completely tied to the higher-level algorithm.
Improving performance sometimes involves removing abstractions, so that we can tune implementation details specifically to our use case.

Here's the resulting image:

```{python}
#| echo: false
%display_image NUMBA_RESULT3
```

And here's the performance of our latest code:

```{python}
#| echo: false
%%compare_timing
median_local_threshold2(IMAGE, 11, 13)
median_local_threshold3(IMAGE, 11, 13)
```

## Use adapative heuristics to take advantage of patterns in your data

Notice that a median's definition is symmetrical:

1. The first value that is smaller than the highest 50% values.
2. Or, the first value that is larger than the lowest 50% values.
   We used this definition in our code above, adding up buckets from the smallest to the largest.

Depending on the distribution of values, one approach to adding up buckets to find the median may be faster than the other.
For example, given a 0-255 range, if the median is going to be 10 we want to start from the smallest bucket to minimize additions: we'll only have to check 10 buckets, vs 245 if we started from the other side.
But if the median is going to be 200, we want to start from the largest bucket.

So which side we should start from?
One reasonable heuristic is to look at the previous median we calculated, which most of the time will be quite similar to the new median.
If the previous median was small, start from the smallest buckets; if it was large, start from the largest buckets.

```{python}
@jit
def median_local_threshold4(img, neighborhood_size, offset):
    assert neighborhood_size % 2 == 1
    radius = (neighborhood_size - 1) // 2
    result = np.empty(img.shape, dtype=np.uint8)
    histogram = np.empty((256,), dtype=np.uint32)
    # 😎 We're going to preserve previous median values across pixels:
    median = 0

    for i in range(img.shape[0]):
        min_y = max(i - radius, 0)
        max_y = min(i + radius + 1, img.shape[0])

        histogram[:] = 0
        for neighbor_y in range(min_y, max_y):
            for neighbor_x in range(0, radius):
                value = img[neighbor_y, neighbor_x]
                histogram[value] += 1

        for j in range(img.shape[1]):
            min_x = max(j - radius, 0)
            max_x = min(j + radius + 1, img.shape[1])

            if min_x > 0:
                for y in range(min_y, max_y):
                    histogram[img[y, min_x - 1]] -= 1

            if max_x < img.shape[1]:
                for y in range(min_y, max_y):
                    histogram[img[y, max_x - 1]] += 1

            half_neighborhood_size = ((max_y - min_y) * (max_x - min_x)) // 2
            # 😎 Find the the median from the updated histogram, choosing the
            # starting side based on the previous median; we can go from small
            # bucket to big bucket, or in reverse:
            the_range = range(256) if median < 127 else range(255, -1, -1)
            for l in the_range:
                half_neighborhood_size -= histogram[l]
                if half_neighborhood_size < 0:
                    median = l
                    break

            if img[i, j] > median - offset:
                result[i, j] = 255
            else:
                result[i, j] = 0
    return result

NUMBA_RESULT4 = median_local_threshold4(IMAGE, 11, 13)
```

```{python}
#| echo: false
%display_image NUMBA_RESULT4
```

The end result is 25% faster.
Since the heuristic is tied to the image contents, the performance impact will depend on the image.

```{python}
#| echo: false
%%compare_timing
median_local_threshold3(IMAGE, 11, 13)
median_local_threshold4(IMAGE, 11, 13)
```

## Trade precision and/or accuracy for speed

Another potential optimization tied to the specifics of your particular data is relaxing the precision or even accuracy of your code.
In many cases, a performance optimization that changes results slightly may be acceptable and even desirable:

* The results you're calculating may be more "precise" than the error caused by your input data.
  If that extra precision is slowing your code down, you can reduce your precision and still got results that are faster and still just as accurate.
  A generic library has no way of knowing what level of precision is acceptable, so it can't make this sort of compromise.
* In other cases, less accurate results may also be acceptable.
  For example, the monetary cost of inaccuracy may be much lower than the monetary cost savings from a faster run time.

So far we've been trying to calculate the median (mostly) accurately.
Next, we'll change the histogram algorithm to use 32 buckets instead of 256.
While it will be less accurate as a _median_ calculation, as a _threshold_ calculation it may still be sufficient.

```{python}
@jit
def median_local_threshold5(img, neighborhood_size, offset):
    assert neighborhood_size % 2 == 1
    radius = (neighborhood_size - 1) // 2
    result = np.empty(img.shape, dtype=np.uint8)
    # 😎 Smaller number of buckets:
    histogram = np.empty((32,), dtype=np.uint32)
    median = 0

    for i in range(img.shape[0]):
        min_y = max(i - radius, 0)
        max_y = min(i + radius + 1, img.shape[0])

        histogram[:] = 0
        for neighbor_y in range(min_y, max_y):
            for neighbor_x in range(0, radius):
                value = img[neighbor_y, neighbor_x]
                # Convert 0-255 into 0-31 to match the buckets:
                histogram[value // 8] += 1

        for j in range(img.shape[1]):
            min_x = max(j - radius, 0)
            max_x = min(j + radius + 1, img.shape[1])

            if min_x > 0:
                for y in range(min_y, max_y):
                    histogram[img[y, min_x - 1] // 8] -= 1

            if max_x < img.shape[1]:
                for y in range(min_y, max_y):
                    histogram[img[y, max_x - 1] // 8] += 1

            half_neighborhood_size = ((max_y - min_y) * (max_x - min_x)) // 2
            the_range = range(32) if median < 127 else range(31, -1, -1)
            for l in the_range:
                half_neighborhood_size -= histogram[l]
                if half_neighborhood_size < 0:
                    # Convert the median from 0-31 buckets to values of range
                    # 0-255:
                    median = l * 8
                    break

            if img[i, j] > median - offset:
                result[i, j] = 255
            else:
                result[i, j] = 0
    return result

NUMBA_RESULT5 = median_local_threshold5(IMAGE, 11, 13)
```

Here's the new image; it doesn't look much different, and may well be acceptable:

```{python}
#| echo: false
%display_image NUMBA_RESULT5
```

At the same time, the code runs twice as fast:

```{python}
#| echo: false
%%compare_timing
median_local_threshold4(IMAGE, 11, 13)
median_local_threshold5(IMAGE, 11, 13)
```

## The big picture

Here's a performance comparison of all the versions of the code:

```{python}
#| echo: false
%%compare_timing

skimage_median_local_threshold(IMAGE, 11, 13)
median_local_threshold1(IMAGE, 11, 13)
median_local_threshold2(IMAGE, 11, 13)
median_local_threshold3(IMAGE, 11, 13)
median_local_threshold4(IMAGE, 11, 13)
median_local_threshold5(IMAGE, 11, 13)
```

Let's go over the steps we went through:

1. **Switch to a compiled language:** this gives us more control.
2. **Reimplement the algorithm taking advantage of constrained requirements:** our median only needed to handle `uint8`, so a histogram was a reasonable solution.
3. **Reuse previous calculations to prevent repetition:** our histogram for the neighborhood of a pixel is quite similar to that of the previous pixel.
   This means we can reuse some of the calculations.
4. **Adaptively tweak the algorithm at runtime:** as we run on an actual image, we use what we've learned up to this point to hopefully run faster later on.
   The decision from which side of the histogram to start is arbirary in general.
   But in this _specific_ algorithm, the overlapping pixel neighborhoods mean we can make a reasonable guess.
5. **Reduce the precision of the result:** this requires understanding the kind of data you're getting, and the kind of output you find acceptable.

By tweaking your custom code for your particular use case and your particular data, you can often run much faster than generic libraries.
