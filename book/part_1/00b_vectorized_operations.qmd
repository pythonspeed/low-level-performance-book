# Consider using batch ("vectorized") operations

In many cases, you can speed up computational code with existing libraries.
Libraries like NumPy, SciPy, Pandas and others can process data very quickly by doing batch processing in functions implemented in a low-level language like C or Fortran.
These are operations that operate on whole arrays at once.
In the Python world this is known as "vectorization" (we'll discuss a different meaning for "vectorization" in a later chapter, when we cover SIMD).
The benefit of these libraries that you don't have to implement everything yourself, and can rely on a library of already-fast operations.

## Introducing NumPy

Before we can move on to the example, it's worth briefly discussing the technology stack we'll be using for examples in this book, starting with the NumPy library.
For more details on the technologies used, [see the appendix](../appendices/01_prerequisites.qmd).

NumPy's core data structure is the `ndarray`, which are N-dimensional arrays.
An `ndarray` is a bit like a standard Python list, except that an `ndarray` is:

1. Fixed in size: you can't append entries.
2. Only contains items of a single data type or "dtype", e.g. 32-bit unsigned integers.
   This allows `ndarray`s to be accessed efficiently from compiled languages, unlike Python lists.
3. Potentially N-dimensional, e.g. it might describe a 3D volume.

You can access NumPy arrays in three ways:

1. Batch ("vectorized") APIs used from Python.
2. Normal `for` loops in Python.
3. A compiled language's `for` loops.

Let's see how fast and efficient each of these approaches is.

## Example: Rescaling the intensity of images

Imagine we have an 8-bit greyscale image.
Oftentimes, low values and high values aren't used much, resulting in an image whose contrast is too low:

```{python}
# This is scikit-image, an image processing framework built on top of NumPy
# arrays:
from skimage import io

# This loads the image into a NumPy array:
IMAGE = io.imread("../images/lowcontrast.jpg")
```

The original image looks like this:

```{python}
#| echo: false
%load_ext book_magics
%display_image IMAGE
```

We want to stretch the values between 30 and 190 to be between 0 and 255, thus increasing the contrast.

## NumPy vectorized operations operate on full arrays

By operating on a whole array, and bypassing the slowness of Python, batch or "vectorized" operations in NumPy and similar libraries can run quickly.
Here's a naive implementation of contrast rescaling using NumPy batch operations:

```{python}
import numpy as np

def numpy_rescale_naive(img, min_value, max_value):
    # Limit the image to values between min_value and max_value:
    clipped = img.clip(min_value, max_value)
    # Shift the image so min_value becomes zero:
    shifted = clipped - min_value
    # Scale the image so it becomes a floating array with values between 0.0
    # and 1.0:
    scaled = shifted / (max_value - min_value)
    # Scale the image so it's between 0.0 and 255.0:
    scaled = scaled * 255
    # Round to whole numbers and convert back to uint8:
    return np.round(scaled).astype(np.uint8)

ORIGINAL_RESCALED = numpy_rescale_naive(IMAGE, 30, 190)
```

The result looks like this:

```{python}
#| echo: false
%display_image ORIGINAL_RESCALED
```

In this case the image just looks better, but in many image processing pipelines adjusting contrast is a required early step.

One problem with the above implementation is that it uses huge amounts of memory.
Because libraries like NumPy operate on whole arrays, temporary values also have to be whole arrays, which can lead to high memory usage.
Here's how much memory the original image uses:

```{python}
print("Memory used by by image, in bytes:", IMAGE.nbytes)
```

And here's peak memory usage from our implementation:

```{python}
#| echo: false
%%compare_timing --measure=peak_memory
numpy_rescale_naive(IMAGE, 30, 190)
```

That's almost 20Ã— the memory used by the initial image!
If we start processing large images, and/or many images in parallel, this overhead will become an expensive bottleneck.

## Avoid temporary arrays to reduce memory usage

We can reduce memory usage by reducing how many temporary arrays we create, trying to do operations in-place, reusing existing arrays instead of creating new ones for every single operation:

```{python}
def numpy_rescale_optimized(img, min_value, max_value):
    shifted = img.clip(min_value, max_value)
    # ðŸ˜Ž Shift in place:
    shifted -= min_value
    # ðŸ˜¢ Still need a temporary floating point array:
    scaled = shifted / (max_value - min_value)
    # ðŸ˜Ž But at least we can do this in place:
    scaled *= 255
    # ðŸ˜Ž Round in place:
    np.round(scaled, out=scaled)
    # ðŸ˜Ž Re-use an array we already created:
    shifted[:] = scaled
    return shifted

assert np.array_equal(
    numpy_rescale_naive(IMAGE, 30, 190),
    numpy_rescale_optimized(IMAGE, 30, 190)
)
```

::: {.callout-important}
### How to read code examples in this book

In many of the book's chapters I will end up rewriting a function step-by-step in order to speed it up.
That means some of the code will change, and some will stay the same.
To make it easier to notice what has changed:

* Scan the code for comments, which I have added anywhere I change the code or where you need to notice an important point.
* Code without comments should be the same as previous versions of a function.
* Comments starting with a ðŸ˜Ž indicate a performance speedup.
:::

Here's the performance and memory usage of the new API:

```{python}
#| echo: false
%%compare_timing --measure=peak_memory
numpy_rescale_naive(IMAGE, 30, 190)
numpy_rescale_optimized(IMAGE, 30, 190)
```

This is a bit better, but we're still using way more memory than we ought to.

## Use `for` loops to reduce memory usage

Thing is, we don't need a whole temporary array.
Each pixel in the image is calculated independently, so if we did the calculation one pixel at a time, we'd use minimal amounts of memory.
Let's try that out, using a Python `for` loop instead of batch/vectorized operations that operate on whole arrays:

```{python}
def python_rescale_intensity(img, min_value, max_value):
    shifted_max = max_value - min_value
    # Create an empty array for the result:
    result = np.empty(img.shape, dtype=np.uint8)
    # Iterate over every pixel in the image:
    for y in range(img.shape[0]):
        for x in range(img.shape[1]):
            old_value = img[y, x]
            if old_value < min_value:
                shifted = 0
            else:
                shifted = old_value - min_value
            shifted = min(shifted, shifted_max)
            scaled = (shifted / shifted_max) * 255
            # Write the scaled value to the result array:
            result[y, x] = np.uint8(np.round(scaled))
    return result

assert np.array_equal(
    ORIGINAL_RESCALED,
    python_rescale_intensity(IMAGE, 30, 190)
)
```

Here's how it performs:

```{python}
#| echo: false
%%compare_timing --measure=peak_memory
numpy_rescale_optimized(IMAGE, 30, 190)
python_rescale_intensity(IMAGE, 30, 190)
```

Good news, this uses much less memory.
Bad news, it's _extremely_ slow.

In general, you should avoid using pure Python `for` loops and similar constructs like list comprehensions on large NumPy arrays.
Python just isn't fast enough.

## Use a compiled language to get fast `for` loops

Here's our choice so far:

1. Use vectorized operations, and suffer from high memory usage.
2. Use a Python `for` loop, with less memory usage but abysmal speed.

By switching to a compiled language, we can get the benefits of both: fast execution and reduced memory usage.

In particular, we will compile the code using [Numba](https://numba.pydata.org/).
Numba is a programming language that implements a subset of Python, compiles to machine code, and has native support for NumPy arrays.
Keep in mind that this is not a book about Numba: the concepts the book covers also apply to C, C++, Cython, Rust, and other low-level languages.

Numba is easy to install and simple to use: you can install the `numba` package with `pip`/`conda`/your package manager of choice.
To make a function faster, you decorate it with `@numba.jit`.
The first time you call this function with a set of arguments with specific types, Numba will generate machine code customized for those particular types.
Subsequent runs will use the pre-compiled version and hopefully run much more quickly.

```{python}
from numba import jit

# ðŸ˜Ž By decorating the function with @jit, it will get compiled to machine code
# the first time it is called. Subsequent calls should be much faster:
@jit
def compiled_rescale_intensity(img, min_value, max_value):
    shifted_max = max_value - min_value
    # Create an empty array for the result:
    result = np.empty(img.shape, dtype=np.uint8)
    # Iterate over every pixel in the image:
    for y in range(img.shape[0]):
        for x in range(img.shape[1]):
            old_value = img[y, x]
            if old_value < min_value:
                shifted = 0
            else:
                shifted = old_value - min_value
            shifted = min(shifted, shifted_max)
            scaled = (shifted / shifted_max) * 255
            # Write the scaled value to the result array:
            result[y, x] = np.uint8(np.round(scaled))
    return result

assert np.array_equal(
    ORIGINAL_RESCALED,
    compiled_rescale_intensity(IMAGE, 30, 190)
)
```

And now we can compare our three different versions:

```{python}
#| echo: false
%%compare_timing --measure=peak_memory
numpy_rescale_optimized(IMAGE, 30, 190)
python_rescale_intensity(IMAGE, 30, 190)
compiled_rescale_intensity(IMAGE, 30, 190)
```

The compiled function runs quicklyâ€”in fact it's faster than the NumPy versionâ€”and it also uses far less memory.

## Identify situations when vectorized operations are not enough

When should you abandon batch APIs like those provided by NumPy, and switch to a compiled language?
There are three common cases where this is helpful; we've already seen the first in our examples above.

### Case 1: The vectorized implementation uses too much memory

Vectorized APIs operate on complete arrays.
That means when you create temporary intermediate values, they will also have to be arrays.
The result is that you often have gigantic temporary arrays using massive amounts of memory.

You can reduce this massive memory usage overhead by doing the operation in chunks or batches, so the temporary arrays are smaller.
Or, you can switch to a lower-level language and then use a `for` loop to iterate over individual values is fast enough.
This means temporary values can be tiny, instead of gigantic, saving you lots of memory.

### Case 2: The vectorized implementation is too slow

Sometimes the vectorized implementation is just too slow.
If you want an optimized version, you're going to have to implement it with lower-level code, where you have more control over how the code runs.

### Case 3: The algorithm can't be expressed with vectorized operations

Sometimes you _can't_ implement the algorithm with existing vectorized APIs.
When this happens, the only solution is to implement a new vectorized API with low-level code.
This is why libraries like SciPy and scikit-image have so much low-level compiled code: the building blocks in NumPy aren't always sufficient.
