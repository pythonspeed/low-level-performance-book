# Know when to switch from existing libraries to compiled code

In many cases, you can speed up computational code with existing libraries.
Libraries like NumPy, SciPy, Pandas and others can process data very quickly by doing batch processing in functions implemented in a low-level language like C or Fortran.
These are operations that operate on whole arrays at once.
In the Python world this is known as "vectorization" (we'll discuss a different meaning for "vectorization" in a later chapter, when we cover SIMD).
The benefit of these libraries that you don't have to implement everything yourself, and can rely on a library of already-fast operations.

In this chapter we'll see how they're used, and then see the benefits of switching to compiled code you write yourself.

## Introducing NumPy

Before we can move on to the example, it's worth briefly discussing the technology stack we'll be using for examples in this book, starting with the NumPy library.
For more details on the technologies used, [see the appendix](../appendices/01_prerequisites.qmd).

The basic principles you'll learn aren't NumPy-specific, it's just a useful example of a library that lets you efficiently share data between Python and low-level compiled languages.
The same principles will apply to other data processing libraries, for example Pandas and Polars, even if the underlying datastructures are somewhat different.

NumPy's core data structure is the `ndarray`, an N-dimensional array.
An `ndarray` is a bit like a standard Python list, but with some key differences:

|                                      | Python `list`      | NumPy `ndarray`               |
|--------------------------------------|--------------------|-------------------------------|
| Can change size?                     | Yes, `l.append(1)` | No, fixed size                |
| Can contain multiple types?          | Yes, `[12, "abc"]` | No, fixed data type ("dtype") |
| How many dimensions?                 | Just 1             | Can be 1D, 2D, 3D, etc.       |
| Fast access from compiled languages? | No ðŸ˜¢              | Yes ðŸ˜Ž                        |

You can access NumPy arrays in three ways, as we'll see in the following example:

1. Batch ("vectorized") APIs used from Python.
2. Normal `for` loops in Python.
3. A compiled language's `for` loops.

Let's see how fast and efficient each of these approaches is.

## Example: Rescaling the intensity of images

Imagine we have an 8-bit greyscale image.
Oftentimes, low values and high values aren't used much, resulting in an image whose contrast is too low:

```{python}
# This is scikit-image, an image processing framework built on top of NumPy
# arrays:
from skimage import io

# This loads the image into a NumPy array:
IMAGE = io.imread("../images/lowcontrast.jpg")
```

The original image looks like this:

```{python}
#| echo: false
%load_ext book_magics
%display_image IMAGE
```

We want to stretch the values between 15 and 190 to be between 0 and 255, thus increasing the contrast.

## NumPy vectorized operations operate on full arrays

By operating on a whole array, and bypassing the slowness of Python, batch or "vectorized" operations in NumPy and similar libraries can run quickly.
These operations are written in a fast, compiled programming language, taking advantage of their ability to efficiently access NumPy arrays.
Other data processing Python libraries, for example Pandas and Polars, also operate in the same way: typically you're running operations written in a compiled language that operate on the whole datastructures (DataFrames or their columns for these two libraries).

In NumPy's case, the syntax for these operations looks like you're operating on a single value, but it's actually operating on a whole array:

```{python}
import numpy as np

# A one-dimensional array of length 3, containing 64-bit signed integers:
arr = np.array([1, 2, 3], dtype=np.int64)

# Add 100 to every item in the array:
new_arr = arr + 100

# The result is a new array, with the same dtype:
assert isinstance(new_arr, np.ndarray)
assert new_arr.dtype == np.int64
print(new_arr)
```

We can also do in-place operations, mutating an existing array:

```{python}
new_arr -= 50
print(new_arr)
```

The `IMAGE` we loaded earlier is also an `ndarray`:

```{python}
# It's an array:
assert isinstance(IMAGE, np.ndarray)
# Its type is uint8:
assert IMAGE.dtype == np.uint8
# It is two-dimensional, with a height of 450 and a width of 600:
assert IMAGE.shape == (450, 600)
# We can access individual pixels if we want, e.g. y = 17, x = 23:
print(IMAGE[17, 23])
```

We can use NumPy batch operations to implement contrast rescaling:

```{python}
def numpy_rescale_naive(img, min_value, max_value):
    # Limit the image to values between min_value and max_value, creating a new
    # uint8 array:
    clipped = img.clip(min_value, max_value)
    assert clipped.dtype == np.uint8

    # Shift the image so min_value becomes zero, creating a new uint8 array:
    shifted = clipped - min_value
    assert shifted.dtype == np.uint8

    # Scale the image, creating a new floating point array with values between
    # 0.0 and 255.0:
    scaled = shifted * (255 / (max_value - min_value))
    assert scaled.dtype == np.float64

    # Round to whole numbers and convert back to uint8:
    return np.round(scaled).astype(np.uint8)

ORIGINAL_RESCALED = numpy_rescale_naive(IMAGE, 15, 190)
```

The result looks like this:

```{python}
#| echo: false
%display_image ORIGINAL_RESCALED
```

In this case the image just looks better; notice the three figures in the background "pop" more in the second photo.
But in many automated image processing pipelines adjusting contrast is a required early step.

One problem with the above implementation is that it uses disproportionate amounts of memory.
Because libraries like NumPy operate on whole arrays, temporary values also have to be whole arrays, which can lead to high memory usage.
Here's how much memory the original image uses:

```{python}
print("Memory used by image, in bytes:", IMAGE.nbytes)
```

And here's peak memory usage from our implementation:

```{python}
#| echo: false
%%compare_timing --measure=peak_memory
numpy_rescale_naive(IMAGE, 15, 190)
```

That's 19Ã— the memory used by the initial image!
If we start processing large images, and/or many images in parallel, this overhead will become an expensive bottleneck.

## Avoid temporary arrays to reduce memory usage

We can reduce memory usage by reducing how many temporary arrays we create.
In particular, by using in-place mutating operations, we can reuse existing arrays instead of creating new ones for every single operation:

```{python}
def numpy_rescale_optimized(img, min_value, max_value):
    temp_arr = img.clip(min_value, max_value)
    assert temp_arr.dtype == np.uint8

    # ðŸ˜Ž Shift in place:
    temp_arr -= min_value

    # ðŸ˜¢ Because we're getting a float64 array here, we can't reuse the
    # `temp_arr` array:
    scaled = temp_arr * (255 / (max_value - min_value))
    assert scaled.dtype == np.float64

    # ðŸ˜Ž Round in place:
    np.round(scaled, out=scaled)

    # ðŸ˜Ž While we now have whole number results, we want the returned array to
    # be uint8, and `scaled` is float64. So we copy the data into `temp_arr`,
    # which has the right type:
    temp_arr[:] = scaled
    return temp_arr

assert np.array_equal(
    numpy_rescale_naive(IMAGE, 15, 190),
    numpy_rescale_optimized(IMAGE, 15, 190)
)
```

::: {.callout-important}
### How to read code examples in this book

In many of the book's chapters I will end up rewriting a function step-by-step in order to speed it up.
That means some of the code will change, and some will stay the same.
To make it easier to notice what has changed:

* Scan the code for comments, which I have added anywhere I change the code or where you need to notice an important point.
* Code without comments should be the same as previous versions of a function.
* Comments starting with a ðŸ˜Ž indicate a performance speedup.
:::

Here's the performance and memory usage of the new API:

```{python}
#| echo: false
%%compare_timing --measure=peak_memory
numpy_rescale_naive(IMAGE, 15, 190)
numpy_rescale_optimized(IMAGE, 15, 190)
```

This uses less memory, and is faster too, but we're still using way more memory than we ought to.

## Use `for` loops to reduce memory usage

Thing is, we don't need a whole temporary array.
Each pixel in the image is calculated independently, so if we did the calculation one pixel at a time, we'd use minimal amounts of memory.
Let's try that out, using a Python `for` loop instead of batch/vectorized operations that operate on whole arrays:

```{python}
def python_rescale_intensity(img, min_value, max_value):
    shifted_max = max_value - min_value
    # Create an empty array for the result:
    result = np.empty(img.shape, dtype=np.uint8)

    # Iterate over every pixel in the image:
    height, width = img.shape
    for y in range(height):
        for x in range(width):
            # old_value is between 0 and 255 (inclusive) since input is a uint8
            # array:
            old_value = img[y, x]
            # Clip:
            if old_value < min_value:
                # Make sure we don't create negative values:
                clipped = min_value
            elif old_value > max_value:
                clipped = max_value
            else:
                clipped = old_value
            shifted = clipped - min_value
            scaled = shifted * (255 / shifted_max)

            # Write the scaled value to the result array:
            result[y, x] = np.uint8(np.round(scaled))
    return result

assert np.array_equal(
    ORIGINAL_RESCALED,
    python_rescale_intensity(IMAGE, 15, 190)
)
```

Here's how it performs:

```{python}
#| echo: false
%%compare_timing --measure=peak_memory
numpy_rescale_optimized(IMAGE, 15, 190)
python_rescale_intensity(IMAGE, 15, 190)
```

On the one hand, this uses much less memory.
On the other hand, it's _extremely_ slow.

In general, you should avoid using pure Python `for` loops and similar constructs like list comprehensions on large NumPy arrays.
Python just isn't fast enough.

## Use a compiled language to get fast `for` loops

Here's our choices so far:

1. Use vectorized operations, and suffer from high memory usage.
2. Use a Python `for` loop, with less memory usage but abysmal speed.

By switching to a compiled language, we can get the benefits of both: fast execution and reduced memory usage.

In particular, we will compile the code using [Numba](https://numba.pydata.org/).
Numba is a programming language that implements a subset of Python, compiles to machine code, and has native support for NumPy arrays.
Keep in mind that this is not a book about Numba: the concepts the book covers also apply to C, C++, Cython, Rust, and other low-level languages.

Numba is easy to install and simple to use:

* You can install the `pip install numba`, or the equivalent with `conda` or your package manager of choice.
* To make a function faster, you decorate it with `@numba.jit`.
  The first time you call this function with a set of arguments with specific types, Numba will generate machine code customized for those particular types.
  Subsequent runs will use the pre-compiled version and typically run much more quickly.

Here's our example, using Numba:

```{python}
from numba import jit

# ðŸ˜Ž By decorating the function with @jit, it will get compiled to machine code
# the first time it is called. Subsequent calls should be much faster. The
# function's code is identical to python_rescale_intensity().
@jit
def compiled_rescale_intensity(img, min_value, max_value):
    shifted_max = max_value - min_value
    result = np.empty(img.shape, dtype=np.uint8)

    height, width = img.shape
    for y in range(height):
        for x in range(width):
            # old_value is between 0 and 255 (inclusive) since input is a uint8
            # array:
            old_value = img[y, x]
            # Clip:
            if old_value < min_value:
                # Make sure we don't create negative values:
                clipped = min_value
            elif old_value > max_value:
                clipped = max_value
            else:
                clipped = old_value
            shifted = clipped - min_value
            scaled = shifted * (255 / shifted_max)

            result[y, x] = np.uint8(np.round(scaled))

    return result

# We make sure to call the function at least once before benchmarking it, so
# that the compilation time isn't included in the benchmark. Plus, it's useful
# to ensure the calculation result is still the same.
assert np.array_equal(
    ORIGINAL_RESCALED,
    compiled_rescale_intensity(IMAGE, 15, 190)
)
```

And now we can compare our three different versions:

```{python}
#| echo: false
%%compare_timing --measure=peak_memory
numpy_rescale_optimized(IMAGE, 15, 190)
python_rescale_intensity(IMAGE, 15, 190)
compiled_rescale_intensity(IMAGE, 15, 190)
```

The compiled function runs quicklyâ€”in fact it's faster than the NumPy versionâ€”and it also uses far less memory.

## Identify situations where vectorized operations are not enough

When should you abandon batch APIs like those provided by NumPy, and switch to a compiled language?
There are three common cases where this is helpful:

### Case 1: The vectorized implementation uses too much memory

We've already seen this one in the example above.
Since vectorized APIs operate on complete arrays, temporary intermediate values will also have to be arrays.
This wastes limited memory.

You can reduce this overhead by doing the operation in chunks or batches, so the temporary arrays are smaller.
Or, you can switch to a compiled language and then use a `for` loop to iterate over individual values.

### Case 2: The vectorized implementation is too slow

Sometimes the vectorized implementation is just too slow.
If you want an optimized version, you're going to have to implement it with lower-level code, where you have more control over how the code runs.
In a later chapter we'll optimize the Numba function above so it runs even faster.

### Case 3: The algorithm can't be expressed with vectorized operations

Sometimes you _can't_ implement the algorithm with existing vectorized APIs.
When this happens, the only solution is to switch to compiled code.
This is why libraries like SciPy and scikit-image that build on NumPy have so much compiled code of their own: the building blocks in NumPy aren't always sufficient.
