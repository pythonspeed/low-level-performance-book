# Reduce how many CPU instructions your code uses

This book is written with a series of assumptions to limit its scope:

* The goal is to optimize data processing: you're running the same code repeatedly on relatively large amounts of data.
* You care about throughput, not responsiveness.
* As discussed in the previous chapter, you've already picked a scalable algorithm.
* Your performance bottleneck involves computation, i.e. CPU or RAM, not disk or network access.
  See a later chapter for some discussion of other bottlenecks.
* The focus is on optimizing runtime on a single CPU core.
  Even if you're using parallel or distributed algorithms, eventually you will be running some code on a single CPU core.

Given the CPU is how our code runs, it's useful to understand how that CPU works.
Of course, modern CPUs have billions of transistors.
No human being, not even the chips' designers, can truly comprehend exactly how they work.
But to write fast software you don't need perfect understanding, you just need a sufficiently accurate mental model.

As you read through the book you will build a more sophisticated, performance-oriented mental model of how the CPU works.
Our starting point is a much simpler model:

1. A CPU core sequentially executes a series of instructions, one at a time.
   An instruction might add two numbers, or read a value from RAM.
2. Each instruction takes approximately the same amount of time to execute.

In short, the number of CPU instructions your program runs can be used as a proxy for your code's performance.
And even though this is inaccurate, it's still useful.
Certainly if you cut the number of instructions your program runs by 99%, there's a very good chance your code will run faster.

We can reduce how many CPU instructions your code runs by:

1. Figuring out a more efficient way to get the same results.
2. Accepting somewhat different results in exchange for running fewer instructions.
