# Use a scalable algorithm

As explained in the introduction, this book focuses on situations where you've already identified which parts of the code are slow, broadly speaking, and where you've determined the performance bottleneck is computation.
Given those assumptions, there are two high-level reasons the code might be slow:

1. The choice of algorithm.
2. How you implemented the algorithm.
   For example, low-level languages like C usually run the same algorithm much faster than Python can.

In the rest of the book we'll focus on the second part, how exactly the algorithm is implemented.
But in this chapter we'll focus on the first decision, the choice of algorithm.

Because an algorithm can have multiple implementations, we can't really measure its speed.
Instead, we can consider how it scales with its input size.
For example, if we switch from processing a 1,000-base DNA sequence to a 1,000,000-base sequence, the algorithm will almost certainly take longer to run.
The question is, how much longer?

Depending on the algorithm's scalability, it might take 3× as long to run, or 1,000× as long, or even 1,000,000× as long.
That last case is a warning: if your algorithm scales badly enough, larger inputs will may make your code so slow that you will not vastly better implementation speedups to make a meaningful difference in run time.

```{python}
#| echo: false
%load_ext book_magics
```

## An example: quadratic algorithms scale badly

Let's consider a specific example: we'll compare a quadratic algorithm and a linear algorithm, both of which solve the same problem.
Quadratic algorithms are a good example because they're easy to write accidentally, to the point that there is [a blog devoted purely to showcasing examples](https://accidentallyquadratic.tumblr.com/).

Before we can show the example, we need to briefly discuss the technology stack we'll be using for examples in this book, starting with the NumPy library.
For more details on the technologies used, see the appendix.
NumPy's core data structure is the `ndarray`, which are N-dimensional arrays.
An `ndarray` is a bit like a standard Python list, except that an `ndarray` is:

1. Fixed in size: you can't append entries.
2. Only contains items of a single data type or "dtype", e.g. 32-bit unsigned integers.
   This allows `ndarray`s to be accessed efficiently from compiled languages, unlike Python lists.
3. Potentially N-dimensional, e.g. it might describe a 3D volume.

Let's implement an algorithm that operates on a pair on `ndarray`s.
Specifically, we'll implement a function that filters one array down to only those items that are present in a second array.
We'll assume both arrays have approximately the same length.

```{python}
import numpy as np

def quadratic_filter_python(arr1, arr2):
    # Create a new array that is the same size and has the same
    # type as the first input array:
    result = np.empty(shape=arr1.shape, dtype=arr1.dtype)

    found_items = 0

    # This has len(arr1) steps. Because this for loop is implemented in
    # Python, it will be pretty slow, as it can't take advantage of NumPy's
    # faster APIs.
    for i in range(len(arr1)):
        item = arr1[i]
        # 🙁 Checking if an item is contained in an array may require iterating
        # over the whole array, so this if statement takes len(arr2) steps. We
        # could also say "if item in arr2:", which would be faster, but that
        # would still internally operate the same way, comparing the items one
        # by one, so for scalability purposes it won't make a difference.
        for j in range(len(arr2)):
            if item == arr2[j]:
                result[found_items] = item
                found_items += 1
                break

    # The result array may be too large, so shrink it down to the number of
    # items we found. A simple slice of a NumPy array doesn't copy the data, it
    # creates a view pointing at the same underlying memory, so this is a cheap
    # operation:
    result = result[:found_items]
    return result
```

::: {.callout-note}
You may be wondering why I'm doing `for i in range(len(arr1))` and not `for item in arr1:`.
The reason involves maximizing Numba performance to work around current limitations in the Numba compiler; I talk about this in a later chapter.

In other cases, the code examples choose to use easier-to-read APIs rather than more idiomatic NumPy APIs, in order to minimize how much knowledge you need to know to understand the code.
:::

How long does this take to run for different input sizes?

```{python}
# Create an array with values from 0 to 999:
SMALL_DATA = np.arange(0, 100, dtype=np.uint64)
SMALL_DATA2 = np.arange(3, 103, dtype=np.uint64)
LARGER_DATA = np.arange(0, 3_000, dtype=np.uint64)
LARGER_DATA2 = np.arange(0, 3_003, dtype=np.uint64)
```

```{python}
#| echo: false
%%compare_timing
quadratic_filter_python(SMALL_DATA, SMALL_DATA2)
quadratic_filter_python(LARGER_DATA, LARGER_DATA2)
```

When the data is 30× larger, the run time is rather more than 30× slower!
Why is that?

## Understand Big-O notation

`quadratic_filter_python()` might run slower or faster on different computers, and below we'll reimplement the same algorithm in a compiled language.
But in this chapter we're focusing on how an algorithm's performance scales with the size of the input.
And across any different hardware environments and implementations, the scalability won't change, because the scalability is tied to the algorithm, not the implementation.

Big-O notation is a way to express that scalability mathematically.
Specifically, $O()$ implies an upper bound; the function will scale this badly, and no worse.

Our algorithm does `len(arr1) · len(arr2)` comparisons.
For every item in `arr1`, it checks if the item is in `arr2` by comparing it with all the items in `arr2` one by one; in the worst case this involves traversing every item in `arr2`.
We'll define:

* $n =$ `len(arr1)`
* $m =$ `len(arr2)`

And now we can say the algorithm is $O(n·m)$.

$O(n·m)$ means the run time of a specific implementation will at worst be proportional to $n·m$.
If we double $n$ (i.e. `len(arr1)`), the run time will at worst double; if we double $m$ (i.e. `len(arr2)`), the run time will at worst double:

$$O(n·m) \implies ImplRuntime(n, m) \propto n·m$$

In the real world, with a specific implementation, things are more complicated, but this simplified model does give us a good sense of an algorithm's scalability.

::: {.callout-tip}
### Further reading

To learn more about Big-O notation, you can read:

* [Big-O: How Code Slows as Data Grows](https://nedbatchelder.com/text/bigo.html), by Ned Batchelder.
  There's also a video talk version linked from the article.
* [_Grokking Algorithms, 2nd Edition_](https://www.manning.com/books/grokking-algorithms-second-edition) by Aditya Y Bhargava.
:::

## A faster language won't make an algorithm scale

So far we've been using Python, which is pretty slow; next we'll try an implementation of the same $O(n·m)$ algorithm in a compiled language.
In particular, we will compile the code using [Numba](https://numba.pydata.org/).
Numba is a programming language that implements a subset of Python, compiles to machine code, and has native support for NumPy arrays.
Keep in mind that this is not a book about Numba: the concepts the book covers also apply to C, C++, Cython, Rust, and other low-level languages.

Numba is easy to install and simple to use: you can install the `numba` package with `pip`/`conda`/your package manager of choice.
To make a function faster, you decorate it with `@numba.jit`.
The first time you call this function with a set of arguments with specific types, Numba will generate machine code customized for those particular types.
Subsequent runs will use the pre-compiled version and hopefully run much more quickly.

```{python}
from numba import jit

# 😎 This is approximately the same code as quadratic_filter_python() above. It
# uses Numba because of the `@jit` decorator. The first time it is called with
# particular types of inputs (e.g. two int64 arrays) it will be compiled to
# machine code.
@jit
def quadratic_filter_compiled(arr1, arr2):
    # When NumPy APIs are used, Numba uses an equivalent, Numba-specific
    # reimplemented version.
    result = np.empty(shape=arr1.shape, dtype=arr1.dtype)

    found_items = 0
    for i in range(len(arr1)):
        item = arr1[i]
        for j in range(len(arr2)):
            if arr2[j] == item:
                result[found_items] = item
                found_items += 1
                break

    result = result[:found_items]
    return result

assert np.array_equal(
    quadratic_filter_python(SMALL_DATA, SMALL_DATA2),
    quadratic_filter_compiled(SMALL_DATA, SMALL_DATA2)
)
```

How does this version scale?

```{python}
#| echo: false
%%compare_timing
quadratic_filter_python(SMALL_DATA, SMALL_DATA2)
quadratic_filter_python(LARGER_DATA, LARGER_DATA2)
quadratic_filter_compiled(SMALL_DATA, SMALL_DATA2)
quadratic_filter_compiled(LARGER_DATA, LARGER_DATA2)
```

This is a lot faster than our original Python algorithm.
But because it uses the same $O(n·m)$ algorithm it is still going to scale badly as the input size grows.

## Linear algorithms beat quadratic algorithms

Let's try a different approach: a more scalable algorithm.
Instead of a run time that scales with $O(n·m)$ given inputs of size $n$ and $m$, the following algorithm scales with $O(n+m)$, i.e. we're switching from quadratic scaling to linear scaling.
As you might imagine, as $n$ and $m$ get bigger, $n·m$ gets larger much faster than $n+m$!

We will implement this more scalable algorithm in Python, without bothering to use a compiled language:

```{python}
# This is still written in Python, a slow language!
def linear_filter_python(arr1, arr2):
    result = np.empty(shape=arr1.shape, dtype=arr1.dtype)

    # This scales linearly with the size of arr2, which we're calling `m`:
    arr2_as_set = set(arr2)

    found_items = 0

    # This scales linearly with the size of arr1, which we're calling `n`:
    for i in range(len(arr1)):
        item = arr1[i]
        # 😎 Checking if an item is contained in a Python set takes a fixed
        # amount of time, unlike checking membership in a list or array.
        if item in arr2_as_set:
            result[found_items] = item
            found_items += 1

    result = result[:found_items]
    return result

assert np.array_equal(
    quadratic_filter_python(SMALL_DATA, SMALL_DATA2),
    linear_filter_python(SMALL_DATA, SMALL_DATA2),
)
```

Let's compare `quadratic_filter_compiled()`—implemented in a fast language (Numba)—to `linear_filter_python()`, which is written in a slow language (Python).
We'll use the `benchit` library to generate a graph of run time over a series of inputs of increasing size:

```{python}
#| output: false
import benchit

inputs = {
    size: (np.arange(0, size, dtype=np.uint64),
           np.arange(3, size + 3, dtype=np.uint64))
    for size in range(100, 1500, 200)
}

timings = benchit.timings(
    [quadratic_filter_compiled, linear_filter_python],
    inputs,
    multivar=True,
    input_name="Array length",
)
```

Here's what the resulting run time looks like; lower values on the Y axis are better:

```{python}
timings.plot(logx=False, logy=False, dpi=100, figsize=(8, 4), fontsize=12)
```

For small inputs, `linear_filter_python()` is slower than `quadratic_filter_compiled()`, with the choice of programming language being the bottleneck.
Python is slower than Numba, and that's what matters.
But once inputs are large enough, the `linear_filter_python()` function becomes faster than `quadratic_filter_compiled()`, despite being implemented in slower Python.

## Use a scalable algorithm, if you can

As we saw in the above examples, once input lengths gets large enough, a slow implementation of a scalable algorithm can beat a fast implementation of a non-scalable algorithm.
Of course, a fast implementation of scalable algorithm is even better.
And in some situations there is no scalable algorithm, and the only thing you can do is focus on speed of implementation:

```{python}
#| echo: false
%%maybe_table
|                     | Non-scalable algorithm | Scalable algorithm    |
|--------------------:|:----------------------:|:---------------------:|
| Slow implementation | Worst                  | Definitely improvable |
| Fast implementation | Might be improvable    | Best                  |
```

Once you've picked an algorithm, then, the next thing to do is to speed up your implementation.
That's what we'll be covering in the rest of the book.
