# Use a scalable algorithm

Once you've decided on your software's architecture, a significant part of your code's speed depends on two sets of decisions embedded in your code:

1. Which algorithm you chose.
2. How you implemented the algorithm.
   For example, low-level languages like C usually run the same algorithm much faster than Python can.

In the rest of the book we'll focus on the second part, how exactly the algorithm is implemented.
But in this chapter we'll focus on the first decision.
The choice of algorithm is crucial because, as we'll see, if you pick the wrong algorithm the resulting performance problems will overwhelm any attempt to speed up the implementation.

```{python}
#| echo: false
%load_ext book_magics
```

## Avoid quadratic algorithms, accidental or otherwise

How long your program runs depends on the size of the input.
For example, processing a 1,000,000-base DNA sequence will almost certainly take longer than processing a 1,000-base sequence.
So when we discuss the choice of algorithm, we need to think in terms of scalability: how will speed change as the input size grows?
In this book I'm assuming you're dealing with sufficiently large amounts of data, such that scalability is relevant.

An algorithm that scales badly will be so slow that your implementation speed won't matter.
Let's consider a specific example of bad scaling: quadratic algorithms, which are very easy to write accidentally, to the point that there is [a blog devoted purely to showcasing examples](https://accidentallyquadratic.tumblr.com/).

Before we can show the example, we need to briefly discuss the technology stack we'll be using for examples in this book, starting with the NumPy library.
For more details on the technologies used, see the appendix.
NumPy's core data structure is the `ndarray`, which are N-dimensional arrays.
An `ndarray` is a bit like a standard Python list, except that an `ndarray` is:

1. Fixed in size: you can't append entries.
2. Only contains items of a single data type or "dtype", e.g. 32-bit unsigned integers.
   This allows `ndarray`s to be accessed efficiently from compiled languages, unlike Python lists.
3. Potentially N-dimensional, e.g. it might describe a 3D volume.

Let's implement an algorithm that operates on a pair on `ndarray`s.
Specifically, we'll implement a quadratic algorithm that filters one array down to only those items that are present in a second array.
We'll assume both arrays have approximately the same length.

```{python}
import numpy as np

def naive_filter_python(arr1, arr2):
    # Create a new array that is the same size and has the same
    # type as the first input array:
    result = np.empty(shape=arr1.shape, dtype=arr1.dtype)

    found_items = 0

    # This has len(arr1) steps. Because this for loop is implemented in
    # Python, it will be pretty slow, as it can't take advantage of NumPy's
    # faster APIs.
    for i in range(len(arr1)):
        item = arr1[i]
        # üôÅ Checking if an item is contained in an array may require iterating
        # over the whole array, so this if statement takes len(arr2) steps:
        if item in arr2:
            result[found_items] = item
            found_items += 1

    # The result array may be too large, so shrink it down to the number of
    # items we found. A simple slice of a NumPy array doesn't copy the data, it
    # just creates a view pointing at the same underlying memory, so this is a
    # cheap operation:
    result = result[:found_items]
    return result
```

::: {.class-note}
You may be wondering why I'm doing `for i in range(len(arr1))` and not `for item in arr1:`.
The reason involves maximizing Numba performance to work around current limitations in the Numba compiler; I talk about this in a later chapter.

In other cases, the code examples choose to use easier-to-read APIs rather than more idiomatic NumPy APIs, in order to minimize how much knowledge you need to know to understand the code.
:::

How long does this take to run for different input sizes?

```{python}
# Create an array with values from 0 to 999:
SMALL_DATA = np.arange(0, 1_000, dtype=np.uint64)
SMALL_DATA2 = np.arange(3, 1_003, dtype=np.uint64)
LARGER_DATA = np.arange(0, 30_000, dtype=np.uint64)
LARGER_DATA2 = np.arange(0, 30_003, dtype=np.uint64)
```

```{python}
#| echo: false
%%compare_timing
naive_filter_python(SMALL_DATA, SMALL_DATA2)
naive_filter_python(LARGER_DATA, LARGER_DATA2)
```

When the data is 30√ó larger, the run time is rather more than 30√ó slower!
Why is that?

## Understand Big-O notation

In this chapter we're focusing on how an algorithm's performance scales with the size of the input.
In `naive_filter_python()`, at its core the algorithm does `len(arr1) ¬∑ len(arr2)` steps.
For every item in `arr1`, it checks if the item is in `arr2`, which in the worst case involves traversing every item in `arr2`.
To simplify our discussion, let's assume that both arrays have approximately the same length $n$.
Then the algorithm will, worst case, take $n^2$ steps; more succinctly, we can say the algorithm is $O(n^2).

Big-O syntax which indicates an upper bound on how the algorithm run time scales with the input size.
In this case, we're saying that the runtime of `naive_filter_python()` is at worst proportional to ${n^{2}}$.
Maybe we'll get lucky and all the entries from `arr1` will be at the start of `arr2` and we'll do better than this, but we can't guarantee that.

Let's switch to a faster implementation of the same $O(n^2)$ algorithm and see if it will help us.

## A faster language won't make an algorithm scale

Since this is a book about writing low-level compiled languages, we're going to see how fast our algorithm runs if we use a compiled language.
In particular, we will compile the code using [Numba](https://numba.pydata.org/).
Numba is a programming language that implements a subset of Python, but compiles to machine code, and has native support for NumPy arrays.
Keep in mind that this is not a book about Numba: the concepts the book covers also apply to C, C++, Rust, and other low-level languages.

Numba is easy to install and simple to use: you can install the `numba` package with `pip`/`conda`/your package manager of choice.
To make a function faster, you decorate it with `@numba.jit`.
The first time you call this function with a set of arguments with specific types, Numba will generate machine code customized for those particular types.
Subsequent runs will use the pre-compiled version and hopefully run much more quickly.

```{python}
from numba import jit

# üòé This is approximately the same code as naive_filter_python()
# above, but it will be compiled to machine code the first time it is called
# with particular types because of the @jit decorator.
@jit
def naive_filter_compiled(arr1, arr2):
    # When NumPy APIs are used, Numba uses an equivalent, Numba-specific
    # reimplemented version.
    result = np.empty(shape=arr1.shape, dtype=arr1.dtype)

    found_items = 0
    for i in range(len(arr1)):
        item = arr1[i]
        for j in range(len(arr2)):
            if arr2[j] == item:
                result[found_items] = item
                found_items += 1
                break

    result = result[:found_items]
    return result

assert np.array_equal(
    naive_filter_python(SMALL_DATA, SMALL_DATA2),
    naive_filter_compiled(SMALL_DATA, SMALL_DATA2)
)
```

How does this version scale?

```{python}
#| echo: false
%%compare_timing
naive_filter_compiled(SMALL_DATA, SMALL_DATA2)
naive_filter_compiled(LARGER_DATA, LARGER_DATA2)
```

This is a lot faster than our original Python algorithm.
But because it uses the same $O(N^{2})$ it is still going to scale badly as the input size grows.

## Linear algorithms beat quadratic algorithms

Let's try a different approach: a more scalable algorithm.
Instead of a run time that scales with $O(n^{2})$ given input size $n$, the following algorithm scales with $O(n)$, i.e. we're switching from quadratic scaling to linear scaling.
We will implement this more scalable algorithm in Python, without bothering to use a compiled language:

```{python}
# This is still written in Python, a slow language!
def smarter_filter_python(arr1, arr2):
    result = np.empty(shape=arr1.shape, dtype=arr1.dtype)

    # This scales linearly with the size of arr2:
    arr2_as_set = set(arr2)

    found_items = 0

    for i in range(len(arr1)):
        item = arr1[i]
        # üòé Checking if an item is contained in a Python set takes a fixed
        # amount of time, unlike checking membership in a list or array.
        if item in arr2_as_set:
            result[found_items] = item
            found_items += 1

    result = result[:found_items]
    return result

assert np.array_equal(
    naive_filter_python(SMALL_DATA, SMALL_DATA2),
    smarter_filter_python(SMALL_DATA, SMALL_DATA2),
)
```

Let's compare `naive_filter_compiled()`‚Äîimplemented in a fast language (Numba)‚Äîto `smarter_filter_python()`, which is written in a slow language (Python).
We'll use the `benchit` library to generate a graph of run time over a series of inputs of increasing size:

```{python}
#| output: false
import benchit

inputs = {
    size: (np.arange(0, size, dtype=np.uint64),
           np.arange(3, size + 3, dtype=np.uint64))
    for size in range(100, 1500, 200)
}

timings = benchit.timings(
    [naive_filter_compiled, smarter_filter_python],
    inputs,
    multivar=True,
    input_name="Array length",
)
```

Here's what the resulting run time looks like; lower values on the Y axis are better:

```{python}
timings.plot(logx=False, logy=False, dpi=100, figsize=(8, 4), fontsize=12)
```

For small inputs, `smarter_filter_python()` is slower than `naive_filter_compiled()`, with the choice of programming language being the bottleneck.
Python is slower than Numba, and that's what matters.
But once inputs are large enough, the `smarter_filter_python()` function becomes faster than `naive_filter_compiled()`, despite being implemented in slower Python.

Once input length $n$ gets large enough, it's the algorithm scalability that matters, and the linear algorithm is faster than the quadratic algorithm.
This is why it's so important to pick a scalable algorithm: once your inputs are large enough, a non-scalable algorithm will overwhelm any benefits you get from switching to a faster programming language.

Once you've picked a scalable algorithm, what else can you do to speed up your code?
That's what we'll be covering in the rest of the book.
