# Use a scalable algorithm

As explained in the introduction, this book focuses on situations where you've already identified which parts of the code are slow, broadly speaking, and where you've determined the performance bottleneck is computation.
Given those assumptions, there are two high-level reasons the code might be slow:

1. The choice of algorithm.
2. How you implemented the algorithm.
   For example, low-level languages like C usually run the same algorithm much faster than Python can.

In the rest of the book we'll focus on the second part, how exactly the algorithm is implemented.
But in this chapter we'll focus on the first decision, the choice of algorithm.

Because an algorithm can have multiple implementations, we can't really measure its speed.
Instead, we can consider how it scales with its input size.
For example, an algorithm processing a 1,000,000-base DNA sequence will almost certainly take longer to run than if it were processing a 1,000-base sequence.
The question is, how much longer?

Depending on the algorithm's scalability, it might take 3√ó as long, or 1,000√ó as long, or even 1,000,000√ó as long.
That last case is a warning: if your algorithm scales badly enough, it will be so slow for larger inputs that your implementation speed simply won't matter.

```{python}
#| echo: false
%load_ext book_magics
```

## An example: quadratic algorithms scale badly

Let's consider a specific example: we'll compare a quadratic algorithm and a linear algorithm, both of which solve the same problem.
Quadratic algorithms are a good example because they're easy to write accidentally, to the point that there is [a blog devoted purely to showcasing examples](https://accidentallyquadratic.tumblr.com/).

Before we can show the example, we need to briefly discuss the technology stack we'll be using for examples in this book, starting with the NumPy library.
For more details on the technologies used, see the appendix.
NumPy's core data structure is the `ndarray`, which are N-dimensional arrays.
An `ndarray` is a bit like a standard Python list, except that an `ndarray` is:

1. Fixed in size: you can't append entries.
2. Only contains items of a single data type or "dtype", e.g. 32-bit unsigned integers.
   This allows `ndarray`s to be accessed efficiently from compiled languages, unlike Python lists.
3. Potentially N-dimensional, e.g. it might describe a 3D volume.

Let's implement an algorithm that operates on a pair on `ndarray`s.
Specifically, we'll implement a function that filters one array down to only those items that are present in a second array.
We'll assume both arrays have approximately the same length.

```{python}
import numpy as np

def naive_filter_python(arr1, arr2):
    # Create a new array that is the same size and has the same
    # type as the first input array:
    result = np.empty(shape=arr1.shape, dtype=arr1.dtype)

    found_items = 0

    # This has len(arr1) steps. Because this for loop is implemented in
    # Python, it will be pretty slow, as it can't take advantage of NumPy's
    # faster APIs.
    for i in range(len(arr1)):
        item = arr1[i]
        # üôÅ Checking if an item is contained in an array may require iterating
        # over the whole array, so this if statement takes len(arr2) steps. We
        # could also say "if item in arr2:", which would be faster, but that
        # would still internally operate the same way, comparing the items one
        # by one, so for scalability purposes it won't make a difference.
        for j in range(len(arr2)):
            if item == arr2[j]:
                result[found_items] = item
                found_items += 1
                break

    # The result array may be too large, so shrink it down to the number of
    # items we found. A simple slice of a NumPy array doesn't copy the data, it
    # creates a view pointing at the same underlying memory, so this is a cheap
    # operation:
    result = result[:found_items]
    return result
```

::: {.callout-note}
You may be wondering why I'm doing `for i in range(len(arr1))` and not `for item in arr1:`.
The reason involves maximizing Numba performance to work around current limitations in the Numba compiler; I talk about this in a later chapter.

In other cases, the code examples choose to use easier-to-read APIs rather than more idiomatic NumPy APIs, in order to minimize how much knowledge you need to know to understand the code.
:::

How long does this take to run for different input sizes?

```{python}
# Create an array with values from 0 to 999:
SMALL_DATA = np.arange(0, 100, dtype=np.uint64)
SMALL_DATA2 = np.arange(3, 103, dtype=np.uint64)
LARGER_DATA = np.arange(0, 3_000, dtype=np.uint64)
LARGER_DATA2 = np.arange(0, 3_003, dtype=np.uint64)
```

```{python}
#| echo: false
%%compare_timing
naive_filter_python(SMALL_DATA, SMALL_DATA2)
naive_filter_python(LARGER_DATA, LARGER_DATA2)
```

When the data is 30√ó larger, the run time is rather more than 30√ó slower!
Why is that?

## Understand Big-O notation

In this chapter we're focusing on how an algorithm's performance scales with the size of the input.
`naive_filter_python()` might run slower or faster on different computers, and below we'll reimplement the same algorithm in a compiled language.
But across all these different environments and implementations, the scalability won't change, because the scalability is tied to the algorithm, not the implementation.

At its core, our algorithm does `len(arr1) ¬∑ len(arr2)` comparisons.
For every item in `arr1`, it checks if the item is in `arr2` by comparing it with all the items in `arr2` one by one; in the worst case this involves traversing every item in `arr2`.
To simplify our discussion, let's assume that both arrays have approximately the same length $n$.
Then the algorithm will, worst case, take $n^2$ steps; more succinctly, we can say the algorithm is $O(n^2)$.

Big-O syntax indicates an upper bound on how the algorithm run time scales with the input size.
In this case, we're saying that the runtime of `naive_filter_python()` is at worst proportional to ${n^{2}}$.
Maybe we'll get lucky and all the entries from `arr1` will be at the start of `arr2` and we'll do better than this, but we can't guarantee that.

## A faster language won't make an algorithm scale

So far we've been using Python, which is pretty slow; next we'll try an implementation of the same $O(n^2)$ algorithm in a compiled language.
In particular, we will compile the code using [Numba](https://numba.pydata.org/).
Numba is a programming language that implements a subset of Python, compiles to machine code, and has native support for NumPy arrays.
Keep in mind that this is not a book about Numba: the concepts the book covers also apply to C, C++, Cython, Rust, and other low-level languages.

Numba is easy to install and simple to use: you can install the `numba` package with `pip`/`conda`/your package manager of choice.
To make a function faster, you decorate it with `@numba.jit`.
The first time you call this function with a set of arguments with specific types, Numba will generate machine code customized for those particular types.
Subsequent runs will use the pre-compiled version and hopefully run much more quickly.

```{python}
from numba import jit

# üòé This is approximately the same code as naive_filter_python() above. It
# uses Numba because of the `@jit` decorator. The first time it is called with
# particular types of inputs (e.g. two int64 arrays) it will be compiled to
# machine code.
@jit
def naive_filter_compiled(arr1, arr2):
    # When NumPy APIs are used, Numba uses an equivalent, Numba-specific
    # reimplemented version.
    result = np.empty(shape=arr1.shape, dtype=arr1.dtype)

    found_items = 0
    for i in range(len(arr1)):
        item = arr1[i]
        for j in range(len(arr2)):
            if arr2[j] == item:
                result[found_items] = item
                found_items += 1
                break

    result = result[:found_items]
    return result

assert np.array_equal(
    naive_filter_python(SMALL_DATA, SMALL_DATA2),
    naive_filter_compiled(SMALL_DATA, SMALL_DATA2)
)
```

How does this version scale?

```{python}
#| echo: false
%%compare_timing
naive_filter_python(SMALL_DATA, SMALL_DATA2)
naive_filter_python(LARGER_DATA, LARGER_DATA2)
naive_filter_compiled(SMALL_DATA, SMALL_DATA2)
naive_filter_compiled(LARGER_DATA, LARGER_DATA2)
```

This is a lot faster than our original Python algorithm.
But because it uses the same $O(n^{2})$ algorithm it is still going to scale badly as the input size grows.

## Linear algorithms beat quadratic algorithms

Let's try a different approach: a more scalable algorithm.
Instead of a run time that scales with $O(n^{2})$ given input size $n$, the following algorithm scales with $O(n)$, i.e. we're switching from quadratic scaling to linear scaling.
We will implement this more scalable algorithm in Python, without bothering to use a compiled language:

```{python}
# This is still written in Python, a slow language!
def smarter_filter_python(arr1, arr2):
    result = np.empty(shape=arr1.shape, dtype=arr1.dtype)

    # This scales linearly with the size of arr2:
    arr2_as_set = set(arr2)

    found_items = 0

    for i in range(len(arr1)):
        item = arr1[i]
        # üòé Checking if an item is contained in a Python set takes a fixed
        # amount of time, unlike checking membership in a list or array.
        if item in arr2_as_set:
            result[found_items] = item
            found_items += 1

    result = result[:found_items]
    return result

assert np.array_equal(
    naive_filter_python(SMALL_DATA, SMALL_DATA2),
    smarter_filter_python(SMALL_DATA, SMALL_DATA2),
)
```

Let's compare `naive_filter_compiled()`‚Äîimplemented in a fast language (Numba)‚Äîto `smarter_filter_python()`, which is written in a slow language (Python).
We'll use the `benchit` library to generate a graph of run time over a series of inputs of increasing size:

```{python}
#| output: false
import benchit

inputs = {
    size: (np.arange(0, size, dtype=np.uint64),
           np.arange(3, size + 3, dtype=np.uint64))
    for size in range(100, 1500, 200)
}

timings = benchit.timings(
    [naive_filter_compiled, smarter_filter_python],
    inputs,
    multivar=True,
    input_name="Array length",
)
```

Here's what the resulting run time looks like; lower values on the Y axis are better:

```{python}
timings.plot(logx=False, logy=False, dpi=100, figsize=(8, 4), fontsize=12)
```

For small inputs, `smarter_filter_python()` is slower than `naive_filter_compiled()`, with the choice of programming language being the bottleneck.
Python is slower than Numba, and that's what matters.
But once inputs are large enough, the `smarter_filter_python()` function becomes faster than `naive_filter_compiled()`, despite being implemented in slower Python.

## Use a scalable algorithm, if you can

As we saw in the above examples, once input length $n$ gets large enough, a slow implementation of a scalable algorithm can beat a fast implementation of a non-scalable algorithm.
Of course, a fast implementation of scalable algorithm is even better.
And in some situations there is no scalable algorithm, and the only thing you can do is focus on speed of implementation:

```{python}
#| echo: false
%%maybe_table
|                     | Non-scalable algorithm | Scalable algorithm    |
|--------------------:|:----------------------:|:---------------------:|
| Slow implementation | Worst                  | Definitely improvable |
| Fast implementation | Might be improvable    | Best                  |
```

Once you've picked an algorithm, then, the next thing to do is to speed up your implementation.
That's what we'll be covering in the rest of the book.
