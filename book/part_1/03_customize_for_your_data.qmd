# Fine-tune algorithms to your particular data

Libraries like NumPy and SciPy are general purpose: they need to run correctly for many different users, with a variety of different requirements.
When you are writing a custom application, for a specific kind of data, your problem is more constrained.
And that means opportunities to speed up your code, by customizing your calculations, algorithms, and data structures specifically for your particular situation and data.

```{python}
#| echo: false
%load_ext book_magics
```

```{python}
# Import the dependencies we'll need:
import numpy as np
from numba import jit
import pandas as pd
```

## Trade precision and/or accuracy for speed

One potential optimization—thought not the only one—is relaxing the precision or even accuracy of your code.
In many cases, a performance optimization that changes results slightly may be acceptable and even desirable:

* The results you're calculating may be more "precise" than the error caused by your input data.
  If that extra precision is slowing your code down, you can reduce your precision and still got results that are faster and still just as accurate.
  A generic library has no way of knowing what level of precision is acceptable, so it can't make this sort of compromise.
* In other cases, less accurate results may also be acceptable.
  For example, the monetary cost of inaccuracy may be much lower than the monetary cost savings from a faster run time.

Let's look at some example data from the realm of public transportation: bus "headways", the time between the arrival of the previous and the next bus.
Headways need to be as low as possible so that passengers can rely on the bus: if the bus arrives every 5 minutes that's a lot better than the bus arriving every 10 minutes.

Our goal is to compute the median headway experienced by passengers on my local transit authority's buses:

```{python}
BUS_HEADWAYS = pd.read_parquet(
    "../data/MBTA-bus-2022-05.parquet",
    columns=["headway"]
)["headway"].dropna().values

print("Size:", len(BUS_HEADWAYS))
print("dtype:", BUS_HEADWAYS.dtype)
```

NumPy has built-in support for calculating the median, so we could just use that:

```{python}
print("Median (seconds):", np.median(BUS_HEADWAYS))
```

Let's see if we can build a custom algorithm that is faster, by customizing it to our particular data.

### Customizing the algorithm

Let's look at the data:

```{python}
print("Min value (seconds):", int(BUS_HEADWAYS.min()))
print("Max value (seconds):", int(BUS_HEADWAYS.max()))
print(
    "99.9th percentile (seconds):",
    int(np.quantile(BUS_HEADWAYS, 0.999))
)
```

Based on these results, we can build a custom median algorithm using two reasonable assumptions:

**Super-long headways don't have to be very accurate** As we can see above, 99.9% of headways are less than 68 minutes (4067 seconds).
And from a passenger experience perspective, the impact of headways is much more meaningful for frequent buses.
For example, a change from a 5-minute to a 15-minute headway can have a major negative impact on the usefulness of a bus route.
On the other hand, a bus route that runs every 2 hours is almost as awful as a bus that runs every 3 hours.

**Reducing accuracy to 15 seconds is reasonable:** The data we have is at a 1-second resolution, but it's unlikely that a headway of 67 seconds really make a difference versus a headway of 66 seconds.
Plus, the arrival and departure of a bus at a stop takes a few seconds, and it's doubtful that the timing data is actually as accurate as it is precise.

Using these two constraints, we can write a much faster median algorithm, by using a histogram:

```{python}
@jit
def histogram_median(bus_headways):
    # Histogram by number of quarter-minutes (15 seconds), with a maximum of
    # 500 × 15 = 7,500 seconds.
    by_quarter_minutes = np.zeros((500,), dtype=np.uint32)
    for headway in bus_headways:
        number_quarter_minutes = np.uint16(headway / 15)
        # If the data is too large, just use the highest bucket. Since 7500 is
        # higher than the 99.9th percentile value, we're underestimating less
        # than 0.1% of the data.
        by_quarter_minutes[min(number_quarter_minutes, 499)] += 1

    # Now that we know how many samples there are for every quarter minute
    # index, add up the cumulative counts until we hit 50% of values:
    median_location = len(bus_headways) / 2
    cumulative_samples = 0
    median = 0
    for i in range(len(by_quarter_minutes)):
        cumulative_samples += by_quarter_minutes[i]
        if cumulative_samples >= median_location:
            median = i
            break

    # Convert back to seconds:
    return median * 15.0
```

We can compare our algorithm to the built-in NumPy `np.median()`, and see that the results are pretty close.
This is partially luck, but even in the worst case we won't be off by more than half a minute:

```{python}
print("Median:", np.median(BUS_HEADWAYS), "secs")
print("Approximate median:", histogram_median(BUS_HEADWAYS), "secs")
```

And if we compare the run time of the two, our custom algorithm is much faster:

```{python}
#| echo: false
%%compare_timing
np.median(BUS_HEADWAYS)
histogram_median(BUS_HEADWAYS)
```

Our new algorithm wouldn't work with generic inputs: it makes many assumptions about the distribution and range of the data.
But for this _specific_ situation and data, it's good enough—and much faster than a general purpose algorithm.
