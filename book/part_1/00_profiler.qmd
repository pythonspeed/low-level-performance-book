# Identify the bottleneck in your code before you optimize it

In most of this book we focus on optimizing computational code, and in particular compiled code.
But taking a step back, there are many other reasons your code could be slow: if the bottleneck is reading from the network, there is no point in optimizing computation.
So before you start optimizing your code, it's important to understand that other bottlenecks exist, and to do the work to identify which specific bottleneck is impacting your code.

```{python}
#| echo: false
%load_ext book_magics
```

## Identify bottlenecks with a profiler

By using one of the many performance profilers for Python—Scalene, Py-Spy, Austin, PyInstrument, VizTracer, and many more—you can identify which parts of your code are taking the most time.

The following example code matches the basic high-level structure of many data processing programs.
If you are not familiar with Pandas, that's fine, it's the high-level part that matters:

```{python}
import pandas as pd

def read_data(file_or_url):
    return pd.read_csv(file_or_url)

def process_data(input_df):
    # Return the distribution of the first letter of the first name; if you're
    # unfamiliar with Pandas that's fine, you don't need to understand this
    # particular code.
    first_letter = input_df["FIRST NAME"].dropna().apply(
        lambda s: s[0]
    )
    return first_letter.value_counts()

def generate_report(output):
    output.to_csv("result.csv")

def full_pipeline(file_or_url):
    input_df = read_data(file_or_url)
    result = process_data(input_df)
    generate_report(result)
```

We can profile this code to see where it's spending the most time.
We'll use [`line_profiler`](https://pypi.org/project/line-profiler/) to profile our code, as it's probably the simplest profiler you can use.
You can use `line_profiler` on the command-line, or inside a Jupyter notebook or equivalent:

```{python}
# Load the line_profiler extension into a Jupyter notebook:
%load_ext line_profiler

# Birth name and date for people born in Missouri between 1980 and 1989:
url = "https://archive.org/download/Missouri_Birth_Index_-_1980-1989/Reclaim_The_Records_-_Missouri_Birth_Index_-_1980-1989.csv"

# Run the expression full_pipeline(url), and show profiled time for the
# full_pipeline and process_data functions:
%lprun -f full_pipeline -f process_data full_pipeline(url)
```

As you can see from looking at the output for `full_pipeline()`, reading in the CSV is the main bottleneck, using up 99% of the time.
This presumably is some combination of bandwidth (downloading the file) and computation (parsing the CSV).
Some profilers—like the [Sciagraph profiler](https://sciagraph.com) I created—can tell you whether code was spending it's time waiting or doing computation.
Lacking that information, you'll need to figure this out from some combination of context, detailed profiling, and experiments.

The profiling also suggests which parts of `process_data()` are slower.
However, since loading the data uses the vast majority of the time there is no point in trying to optimize `process_data()`, at least initially.
If we wanted faster results, we'd need to figure out a faster way to load the data.

Keep in mind that profilers can distort the results somewhat, so low overhead is an important feature in a profiler.
For this sort of line-by-line profiling, a more sophisticated and lower-overhead profiler is [Scalene](https://github.com/plasma-umass/scalene).

## Avoid non-computational performance bottlenecks

As in the example above, computation isn't always your code's main bottleneck.
Here are some examples of non-computational bottlenecks:

### Waiting for a remote service

Imagine your program processes the result of a database query.
If that database query takes 10 minutes and your processing takes 1 minute, there's no point in optimizing your processing code.
Instead, you should be focusing your efforts on figuring on speeding up the database query.

### Network latency and bandwidth

Your program may end up spending much of its time reading and writing data over the network.
This can be due to bandwidth limitations: you can download at 10MB/sec, and you're downloading a 5000MB file.
Compression can help reduce bandwidth bottlenecks.

Or, the bottleneck may be due to latency, the time it takes to send a message to the remote server.
If you're downloading 100,000 1KB files sequentially, with a latency of 10ms, it will take you at best 1,000 seconds to download them all, even with infinite bandwidth.
It will be faster to download them in parallel, or better yet in a large batch.

### Reading and writing from disk

Like the network, reading and writing from disk can suffer from slowness caused by both bandwidth and latency.
Spinning hard drives in particular have very high latency; more modern SSDs and NVMe disks do better.
Compressing the data on disk can help reduce bandwidth issues, and reading in batches can reduce latency issues.

### Swapping

If your data doesn't fit in RAM, the operating system will move some data out of RAM and on to disk; this is known as swapping.
If you swap out data you aren't currently using, this won't be noticeable.

But if you are continuously reading and writing in-memory data that doesn't fit in RAM, it will be continuously moved back and forth from disk.
Beyond a certain point your whole computer will grind to a halt as it spends all its time swapping.
