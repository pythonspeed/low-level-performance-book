# Introduction {.unnumbered}

This book is specifically written for scientists, data scientists, and software developers who use Python to do numeric computing or similar large-scale data processing.
Python[^cpython] is notoriously slow for computation, yet often used to handle large-scale data processing.
Getting faster results typically involves either creating or using existing Python extensions written in a fast, low-level, compiled language: C, C++, Fortran, Cython, Numba, Rust, and others.

[^cpython]: In general I'm talking about the default implementation, rather than the language. To distinguish the two, the Python interpreter is often known as CPython. The PyPy interpreter is a different implementation of the language that uses just-in-time compilation do math much faster, but it adds overhead when interoperating with NumPy and other similar libraries, and lags behind on language features. A future version of CPython might also include some just-in-time compilation speedups.

Speeding up your Python code with a compiled extension can seem like magic: switch to a compiled language, _CODE GO FAST!!!_

But if you think of compiled code as magic, you may be missing significantâ€”and sometimes quite simple!â€”optimizations that can make your code even faster.
Or to put it a different way, even if you're using a compiled language, you might still be unknowingly writing slow code.
If you really want your code to run faster, you will need a deeper understanding of:

1. Your code.
2. The compiler.
3. The CPU it runs on.

The goal of this book is to help you build a mental model of all of these, so that can write faster code by default.

```{python}
#| echo: false

# Disable SIMD, just so it doesn't obscure branch prediction effects.
import os
os.environ["NUMBA_SLP_VECTORIZE"] = "1"
os.environ["NUMBA_LOOP_VECTORIZE"] = "1"
del os

%load_ext book_magics
```

## Understanding your code's performance

Not all bottlenecks are computational.
Your code may end up waiting for the network, or for a database query, or for a user to respond to a question.
Switching to a compiled language won't help in these cases.
We'll briefly cover this issue in the next chapter; for the rest of the book, the assumption is that you've already determined that computation is the performance bottleneck you need to solve.

### Use a scalable algorithm

If the bottleneck is computation, you need to make sure your algorithms and data structures are scalable.
If they're not, sooner or later your code will be far too slow, no matter what programming language it's written in:

```{python}
# ðŸ˜¢ This is extremely unscalable!
def find_intersection(a: list, b: list) -> list:
    result = []
    for value in a:
        if value in b:
            result.append(value)
    return result

# ðŸ˜Ž This is much better.
def find_intersection_scalable(a: list, b: list) -> list:
    result = []
    b_set = set(b)
    for value in a:
        if value in b_set:
            result.append(value)
    return result

LIST1 = list(range(200))
LIST2 = [17, 23, 5, 1, 4, 2, 8, 9, 10, 15, 16, 12, 13]

assert (
    find_intersection(LIST1, LIST2) ==
    find_intersection_scalable(LIST1, LIST2)
)
```

Here's how the two versions compare in terms of speed:

```{python}
#| echo: false
%%compare_timing
find_intersection(LIST1, LIST2)
find_intersection_scalable(LIST1, LIST2)
```

We'll briefly cover algorithm scalability in a later chapter.

### Avoid wasted effort and unnecessary precision

Even once you've chosen a scalable algorithm, it's quite possible your code is doing repetitive and unnecessary work.
While a compiler can help with this, it can only do so much.
And to some extent what counts as unnecessary work is something you have to decide:

* Does all the data need to be processed?
* How precise does the answer need to be?

Only you can answer these questions.

We'll cover this is in a number of chapters; for example, in one chapter we'll see how you can optimize this algorithm:

```{python}
# ðŸ˜¢ The Sieve of Eratosthenes algorithm for finding primes, implemented
# inefficiently.
def find_primes(up_to_value: int) -> list[bool]:
    is_prime = [True] * up_to_value
    is_prime[0] = False
    is_prime[1] = False

    for i in range(2, up_to_value):
        if not is_prime[i]:
            continue
        for j in range(i * 2, up_to_value, i):
            is_prime[j] = False

    return is_prime
```

With a little bit of work, we can switch to a more efficient variant:

```{python}
# ðŸ˜Ž The Sieve of Eratosthenes, with less redundant work.
def find_primes_optimized(up_to_value: int) -> list[bool]:
    is_prime = [True] * up_to_value
    is_prime[0] = False
    is_prime[1] = False

    for i in range(4, up_to_value, 2):
        is_prime[i] = False

    for i in range(3, up_to_value, 2):
        if not is_prime[i]:
            continue
        for j in range(i * i, up_to_value, 2 * i):
            is_prime[j] = False

    return is_prime

assert find_primes(1000) == find_primes_optimized(1000)
```

And this optimized version is almost twice as fast:

```{python}
#| echo: false
%%compare_timing
find_primes(1000)
find_primes_optimized(1000)
```

## Understanding how compilers work

Once you have a scalable algorithm without repetitive or unnecessary work, switching to a compiled language can, at least some of the time, make your code even faster.
A compiler takes some source code and turns into machine code instructions that run directly on the CPU.

But of course the Python interpreter itself is also written in a compiled language, so why is running Python code often so much slower than the equivalent compiled code?

### Static typing allows for faster code

One reason is that compiled languages allow a compiler to convert your source code to machine code in a very specialized way.

Consider the following Python function:

```{python}
def add(a: int, b: int) -> int:
    return a + b
```

As a reader, you know it's adding two integers, but Python doesn't actually take advantage of the `int` type annotations.
The type annotations are hints, not instructions, and if you wanted to you could call `add("abc", "def")` and the code would run just fine.

When you do `add(2, 3)`, Python will:

1. Figure out at runtime which type each of the objects is.
2. Figure out which function implements addition for those types.
3. Because Python integers can be arbitrarily large, the function for adding two Python then needs to at minimum figure out if the integers are small enough to use less generic routines.
4. Finally, this function does the actual arithmetic.
5. Then, the resulting integer has to be wrapped in a Python integer.

All of this work adds up!

Compare that to a similar function written in the compiled Rust programming language (in a slightly non-idiomatic way[^rust]):

```rust
fn add(a: i64, b: i64) -> i64 {
    return a + b;
}
```

[^rust]: Usually Rust would use an implicit return, rather than explicit return. However, for people who don't know Rust, this version is easier to read, and it's still valid Rust.

In this case, the Rust compiler does take advantage of the types.
What's more, we're dealing with a specific integer type: 64-bit signed integers[^signed].
And CPUs have instructions for directly adding 64-bit integers.
That means the compiler doesn't have to deal with different object types, or different integer types, it can just generate machine code for adding two 64-bit signed integers.

How do these two functions compare?

* The Python `add()` will use tens of thousands of CPU instructions to run.
* On x86-64, when compiled in release mode, the Rust function compiles down to just 3 CPU instructions.

Unsurprisingly, the Rust function is much faster.

[^signed]: A signed integer is one that can be either negative or positive; an unsigned integer can only be positive.

### The compiler will optimize your code...

Compilers do more than just translate your code to machine code: they also optimize it along the way.
In particular, they will heuristically transform your code into a (hopefully) faster implementation before it gets transformed into machine code.
In many cases this means that your can run faster, with no additional effort on your part, as we'll see in a later chapter.

For example, in this book we'll be using Numba, a compiled language based on Python.
The Numba compiler turns a function into compiled code by using the `@numba.jit` decorator:

```{python}
from numba import jit

# A compiled function that takes two int64 arguments and returns an int64
# result:
@jit("int64(int64, int64)")
def add_n_times(value, n):
    result = 0
    for _ in range(100_000):
        for _ in range(n):
            result += value
    return result
```

When the compiler compiles this code, it will essentially turn it into an equivalent, much faster, function that looks like this:

```{python}
@jit("int64(int64, int64)")
def multiply(value, n):
    return 100_000 * value * n

assert add_n_times(17, 3) == multiply(17, 3)
```

We can see they're equivalent, at least as far as performance goes, by benchmarking both of them.
The elapsed time is mostly just function call and argument parsing overhead, it's not really doing any computational work:

```{python}
#| echo: false
%%compare_timing
add_n_times(17, 3)
multiply(17, 3)
```

### ...except sometimes the compiler can't optimize your code

The compiler's ability to optimize your code has an important caveat: the compiler will only transform your code in ways that ensure the resulting code behaves in the _exact_ same way.
Sometimes, a slightly different version of the code could be much faster, but the compiler won't be able to choose it.
That means you can make your code faster by spotting these problems, and either transforming your code so the compiler can optimize it, or just doing the optimization yourself.

Some examples of problems that can prevent the compiler from optimizing your code:

* **Aliasing:** If it's possible that the program has multiple writable references to the same memory, the compiler won't be able to optimize reads to that memory.
  The problem, and some solutions, is covered in one chapter of the book.
* **Floating point math:** Changing the order of floating calculations can change the results, which limits what the compiler can do.
  The problem and solutions are covered in multiple chapters.

As an example, the following pair of functions is almost the same as the previous pair, with the exception that they're using floats:

```{python}
@jit("float64(float64, int64)")
def add_n_times_float(value, n):
    result = 0.0
    for _ in range(100_000):
        for _ in range(n):
            result += value
    return result

@jit("float64(float64, int64)")
def multiply_float(value, n):
    return 100_000 * value * n

assert add_n_times_float(12.0, 3) == multiply_float(12.0, 3)
```

Unlike the integer version, the Numba compiler cannot optimize the float version of `add_n_times_float()` into the equivalent of `multiply_float()`.
The latter will be much faster:

```{python}
#| echo: false
%%compare_timing
add_n_times_float(12.0, 3)
multiply_float(12.0, 3)
```

### And more!

Other useful compiler performance topics covered by this book include:

* Avoiding undefined behavior; if your program is wrong, it doesn't matter how fast it is.
* Hoisting type conversions out of inner loops.
* Avoiding integer overflows.
* And more!

## Understanding how CPUs work

Modern CPUs work in ways that can speed up your code significantly, if you make sure to write your code appropriately.
But if you're writing regular Python, the general slowness of the Python interpreter mean these particular effects are usually not worth thinking about.
Once you're writing faster compiled code, however, you can take advantage of these features to run even faster.

### Automatic parallel execution: instruction-level parallelism

Consider the following code:

```python
a += 1;
b += 2;
```

In Python, you would expect those two operations to happen sequentially.
First `a` gets incremented, then `b`.

In a compiled language, the code may well be compiled into two sequential operations.
But when the CPU runs the code, the CPU may run those two instructions _in parallel_ on the same core.
This is completely unrelated to threads or processes, the usual way we think of parallelism in general purpose computers.
This is parallelism at the level of CPU instructions, on a single CPU core, no threads or processes involvedâ€”and the CPU does it automatically.

Consider these two functions:

```{python}
@jit
def add_once(number):
    result = 0.0
    for _ in range(1_000_000):
        result += number
    return result

@jit
def add_twice(number):
    result1 = 0.0
    result2 = 0.0
    for _ in range(1_000_000):
        result1 += number
        result2 += number
    return (result1, result2)

once = add_once(3.2)
assert add_twice(3.2) == (once, once)
```

Despite `add_twice()` using twice as many CPU instructions, it finishes in the same amount of time, thanks to instruction-level parallelism:

```{python}
#| echo: false
%%compare_timing --measure=instructions
add_once(3.2)
add_twice(3.2)
```

In some cases the CPU won't be able to use parallelism (or as much parallelism, at least) because the order really does matter, for example:

```python
a += 1;
# This can only run after `a` has been incremented:
b += a;
```

That means you can potentially speed up your code by restructuring it remove logical dependencies that prevent parallelism.
We'll cover this in a later chapter.

### Speculative execution and branch (mis)prediction

In order to maximize instruction-level parallelism, the CPU will use "speculative execution" when it encounters a branch.
Consider the following code:

```python
@jit
def is_bigger_than_half(values):
    # ... create result array ...
    for i in range(len(values)):
        if values[i] > 0.5:
            result[i] = True
    # ...
```

The CPU will try to predict the future.
In particular, it will heuristically guess if `values[i] > 0.5` is true, and if decides it is, it will run `result[i] = True` in parallel to the `if` statement.
If this guess turns out to be right, our code runs faster without any work on our part.

But if this guess turns out to be wrong, what's known as a "branch misprediction", the CPU will need to undo the incorrect work it did.
Then, it will have to calculate the correct result given the real result of the `if` statement.
Ultimately the computation will match what would have happened if the prediction had been correct, but the program will run much slower because of all the extra work involved.

```{python}
#| echo: false
import numpy as np

@jit
def is_bigger_than_half(arr):
    result = np.zeros((len(arr),), dtype=np.bool_)
    total = 0
    for i in range(len(arr)):
        if arr[i] > 0.5:
            result[i] = True
        total += 1
    return result.sum()

PREDICTABLE = np.ones((1_000_000,), dtype=np.float64)
PREDICTABLE[500_000:] = 0
RANDOM_NUMS = np.random.random((1_000_000,))
assert is_bigger_than_half(PREDICTABLE) == 500_000
```

Most of the time branches are predictable, and the cost of mispredictions is therefore low.
But what happens when you have data that is hard to predict?
Your code can get very very slow.

The demonstrate this, we'll test our function with two arrays, `PREDICTABLE` and `RANDOM_NUMS` (not shown), with an equal number of values above 0.5.
`PREDICTABLE` has a long sequence of identical values, making it easy for the CPU to predict the `if values[i] > 0.5` branch.
In contrast, `RANDOM_NUMS` is an array of randomly generated values between 0 and 1, so it's impossible to predict whether a given value will be bigger than 0.5.

```{python}
# Same length:
assert len(PREDICTABLE) == len(RANDOM_NUMS)
# The number of values > 0.5 differs by less than 1%:
assert abs(
    is_bigger_than_half(PREDICTABLE) -
    is_bigger_than_half(RANDOM_NUMS)
) / len(PREDICTABLE) < 0.01
```

If we benchmark the function with both inputs, the number of CPU instructions used is almost the same.
Nonetheless `is_bigger_than_half(RANDOM_NUMS)` is much slower, because there are lots and lots of branch mispredictions:

```{python}
#| echo: false
%%compare_timing --measure=instructions,branch_mispredictions
is_bigger_than_half(PREDICTABLE)
is_bigger_than_half(RANDOM_NUMS)
```


We'll cover ways to avoid the performance cost of branches in a number of chapters.

### Memory access patterns impact performance

Reading and writing memory can be fastâ€”or slow!
It depends on how much memory is involved, and the access pattern you're using.
For example, we can access an array of data linearly, from start to finish:

```{python}
@jit
def scan(arr, multiplier):
    index = 0
    size = len(arr)
    for _ in range(1_000_000):
        arr[index] += 1
        index = (multiplier * index + 1) % size
    return arr[0]

def linear_scan(arr):
    return scan(arr, 1)
```

Or, we can access an array pseudo-randomly, jumping around:

```{python}
def random_scan(arr):
    return scan(arr, 22695477)
```

Given the same array of one million entries (not shown), the linear scan is faster than the random scan even though both functions use almost the same number of CPU instructions:

```{python}
#| echo: false
import numpy as np

DATA = np.ones((100_000_000,), dtype=np.uint8)
_ = linear_scan(DATA)
_ = random_scan(DATA)
```

```{python}
#| echo: false
%%compare_timing --measure=instructions
linear_scan(DATA)
random_scan(DATA)
```

We'll discuss memory's impact on performance in a number of chapters.

### And more!

There's far more to learn about how CPUs work, from SIMD (minimally introduced in a couple of chapters) to details that are beyond the scope of this book (SIMD in depth, Âµops, instruction latency, and more).

## Let's get started

The above is just some of what this book covers, but it gives you the big picture of optimizing single-threaded computational code.
In particular, to maximize the performance of computation, you should write code:

1. Using a scalable algorithm.
2. Without wasted effort.
3. Using a compiled language.
4. Without preventing the compiler from optimizing your code.
4. Without getting in the way of the CPU's performance features (instruction-level parallelism, speculative execution, memory caches and pre-fetch, and more).

Beyond that, though it's not covered in this book, you can and should also use parallelism across multiple CPU cores to go even faster, by utilizing multiple threads or processes.

While you could jump straight to parallelism, optimization is still immensely useful:

* The performance benefit of optimizing your code is multiplicative with parallelism: if you can make your code 10Ã— faster on a single thread, and then 10Ã— faster with parallelism, you will get results 100Ã— faster.
* Both optimization and parallelism reduce electricity usage, and therefore carbon emissions; the reduction seems at least somewhat multiplicative.
* Unlike parallelism, optimization can save you money, since you get faster results without paying for more expensive hardware.

If possible, then, you should be applying both techniques.

Ready to get faster results from your code?
Let's get started!
