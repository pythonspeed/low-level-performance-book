# Introduction {.unnumbered}

Writing fast code is different from the normal way you write code.
Usually you focus on correctness: does your calculation give the right result?
And both when working with other people's tools and libraries, and within your own code if it's complex enough, you rely on _abstractions_, which allow you to ignore implementation details and instead focus on the high-level behavior of the libraries and tools you use.

If you're calculating 2 + 2, you want the answer to be 4, no matter if you are:

* Writing Python or C++.
* Using Linux or macOS.
* Running on an x86-64 CPU (from Intel or AMD) or an ARM CPU (like newer Macs, or a Raspberry Pi).

Unfortunately, writing fast code requires more work.
You can no longer just ask _what_ the abstractions you're using do; you also need to ask _how_ they do it.
How your code works, how your programming language works, how your libraries work, even how the computer hardware works.
You don't need to understand every detail of how they work, a mental model will do.

I'll call this mental model of how an abstraction works the _mechanism_.
If you can understand the mechanisms of the abstractions you rely on, you can write code that will allow you to process more data, run more experiments, and get results faster.
In short, you will be able to unleash the astounding computational speed that computers give you.

## Why this book is different

Is this book for you?
Here's a checklist:

* **Who you are:** You're a scientist, data scientist, or software developer who does numeric computing or similar large-scale data processing.
* **What you need to know:** You know how to write Python; unlike most other books covering these topics, you do not need to know the C programming language.
* **What you want to do:** You need to speed up your code's _computation_.
    That is, the math, business logic, or other calculations that don't involve disk or network operations.

While Python[^cpython] is notoriously slow for computation, it is often used to handle large-scale data processing, which requires fast code.
This requirement is typically met by creating or using existing Python extensions written in a fast, low-level, compiled language.
To write a compiled extension, you can use C, C++, Fortran, Cython, Numba, Rust, or other languages.

But this is not a book about writing Python extensions.
Rather, the goal is to help you write faster computational codeâ€”both for Python extensions, and for standalone programs.
And while the code you'll be reading is mostly written in a specific compiled language, the focus is on the underlying similarities between different compiled languages, and on the underlying hardware that runs your software.

[^cpython]: In general I'm talking about the default implementation, rather than the language. To distinguish the two, the Python interpreter is often known as CPython. The PyPy interpreter is a different implementation of the language that uses just-in-time compilation do math much faster, but it adds overhead when interoperating with NumPy and other similar libraries, and lags behind on language features. A future version of CPython might also include some just-in-time compilation speedups.

### From abstraction to mechanism: Three ways to speed up your code

To speed up computational code, there are three sets of abstractions whose actual implementation details you will need to understand, at least at a high level:

1. **Your code's calculations:** By avoiding wasted effort, or reducing unnecessary precision, you can make your code faster, regardless of programming language.
2. **The compiler:** Once you switch to a compiled language, you need to know at least a little about what the compiler speeds up your codeâ€”and just as importantly, what it can't do.
3. **The CPU:** By understanding the sometimes counter-intuitive ways CPUs and related hardware like memory caches work, you can ensure your code isn't being unexpectedly slowed down.

As motivation to what you can achieve, and to preview the topics covered in this book, I'll briefly go over each mechanism (the calculations in your code, the compiler, and the CPU) in turn.

```{python}
#| echo: false

# Disable SIMD, just so it doesn't obscure branch prediction effects.
import os
os.environ["NUMBA_SLP_VECTORIZE"] = "1"
os.environ["NUMBA_LOOP_VECTORIZE"] = "1"
del os

%load_ext book_magics
```

## Mechanism #1: How your code works

Not all bottlenecks are computational.
Your code may end up waiting for the network, or for a database query, or for a user to respond to a question.
Switching to a compiled language won't help in these cases.

### Use a scalable algorithm

If the bottleneck is computation, you need to make sure your algorithms and data structures are scalable.
If they're not, sooner or later your code will be far too slow, no matter what programming language it's written in:

```{python}
# ðŸ˜¢ This is extremely unscalable!
def find_intersection(a: list, b: list) -> list:
    result = []
    for value in a:
        if value in b:
            result.append(value)
    return result

# ðŸ˜Ž This is much better.
def find_intersection_scalable(a: list, b: list) -> list:
    result = []
    b_set = set(b)
    for value in a:
        if value in b_set:
            result.append(value)
    return result

LIST1 = list(range(200))
LIST2 = [17, 23, 5, 1, 4, 2, 8, 9, 10, 15, 16, 12, 13]

assert (
    find_intersection(LIST1, LIST2) ==
    find_intersection_scalable(LIST1, LIST2)
)
```

Here's how the two versions compare in terms of speed:

```{python}
#| echo: false
%%compare_timing
find_intersection(LIST1, LIST2)
find_intersection_scalable(LIST1, LIST2)
```

### Avoid wasted effort and unnecessary precision

Even once you've chosen a scalable algorithm, it's quite possible your code is doing repetitive and unnecessary work.
For example, consider the following algorithm for counting odd and even numbers:

```{python}
# ðŸ˜¢ No need to calculate both:
def even_and_odd(values: list[int]) -> tuple[int, int]:
    even = 0
    odd = 0
    for value in values:
        if value % 2 == 0:
            even += 1
        else:
            odd += 1
    return (even, odd)
```

With a little bit of work, you can implement a more efficient variant (or at least, more efficient when writing Python):

```{python}
# ðŸ˜Ž Less redundant work:
def even_and_odd_optimized(values: list[int]) -> tuple[int, int]:
    odd = 0
    for value in values:
        if value % 2:
            odd += 1
    even = len(values) - odd
    return (even, odd)

VALUES = list(range(1000))
assert even_and_odd(VALUES) == even_and_odd_optimized(VALUES)
```

And this optimized version is almost twice as fast:

```{python}
#| echo: false
%%compare_timing
even_and_odd(VALUES)
even_and_odd_optimized(VALUES)
```

While a compiler can help get rid of unnecessary work, it can only do so much.
And to some extent what counts as unnecessary work is something you have to decide:

* Does you need to process all the data?
* How precise does the answer need to be?

Only you can answer these questions.

## Mechanism #2: How compilers work

Once you have a scalable algorithm without repetitive or unnecessary work, switching to a compiled language can, at least sometimes, make your code even faster.
A compiler takes some source code and turns into machine code instructions that run directly on the CPU.

But of course the Python interpreter itself is also written in a compiled language, so why is running Python code often so much slower than the equivalent compiled code?

### Static typing enables faster code

One reason that compiled languages enable faster by code is by providing the information the compiler needs to convert your source code to machine code in a very specialized way.

Consider the following Python function:

```{python}
def add(a: int, b: int) -> int:
    return a + b
```

As a reader, you know it's adding two integers, but Python doesn't actually take advantage of the `int` type annotations.
The type annotations are hints, not instructions, and if you wanted to you could call `add("abc", "def")` and the code would run just fine.

When you do `add(2, 3)`, Python will:

1. Figure out at runtime which type each of the objects is.
2. Figure out which function implements addition for those types.
3. Because Python integers can be arbitrarily large, the function for adding two Python then needs to at minimum figure out if the integers are small enough to use less generic routines.
4. Finally, this function does the actual arithmetic.
5. Then, Python converts resulting C integer back into a Python object.

All this work adds up!

Compare that to a similar function written in the compiled Rust programming language[^rust]:

```rust
fn add(a: i64, b: i64) -> i64 {
    return a + b;
}
```

[^rust]: Usually Rust would use an implicit return, rather than explicit return. However, for people who don't know Rust, this version is easier to read, and it's still valid Rust.

In this case, the Rust compiler can take advantage of the types; in fact, the code won't compile if you don't set types for function inputs.
The specific type for both input and output is `i64`, a 64-bit signed integer[^signed], and CPUs have instructions for directly adding 64-bit integers.
That means the compiler doesn't have to deal with different object types, or different integer types, or arbitrarily sized integers.
It can just generate machine code for adding two 64-bit signed integers.

How do these two functions compare?

* The Python `add()` will use tens of thousands of CPU instructions to run.
* On x86-64, when compiled in release mode, the Rust function compiles down to just 3 CPU instructions.

Unsurprisingly, the Rust function is much faster!

[^signed]: A signed integer is one that can be either negative or positive; an unsigned integer can only be positive.

### The compiler will optimize your code...

Compilers do more than just translate your code to machine code: they also optimize it along the way.
In particular, they will heuristically transform your code into a (hopefully) faster implementation before it gets transformed into machine code.
Often this means that your can run faster, with no additional effort on your part.

For example, in this book most code examples use Numba, a compiled language based on Python.
The Numba compiler turns a function into compiled code by using the `@numba.jit` decorator:

```{python}
from numba import jit

# A compiled function that takes two int64 arguments and returns an int64
# result:
@jit("int64(int64, int64)")
def add_n_times(value, n):
    result = 0
    for _ in range(100_000):
        for _ in range(n):
            result += value
    return result
```

When the compiler compiles this code, it will essentially turn it into an equivalent, much faster, function that looks like this:

```{python}
@jit("int64(int64, int64)")
def multiply(value, n):
    return 100_000 * value * n

assert add_n_times(17, 3) == multiply(17, 3)
```

One way to check if they're equivalent, at least as far as performance goes, is to benchmark both of them.
The elapsed time is mostly just function call and argument parsing overhead, it's not really doing any computational work:

```{python}
#| echo: false
%%compare_timing
add_n_times(17, 3)
multiply(17, 3)
```

### ...except when the compiler can't optimize your code

The compiler's ability to optimize your code has an important caveat: the optimized code must behave identically to the original code.
You may be perfectly willing to accept a minor difference in behavior in return for a much faster program, but the compiler has no way of knowing that.
That means you can make your code faster by spotting these problems, and either transforming your code so the compiler can optimize it, or just doing the optimization yourself.

For example, changing the order of floating-point calculations can change the results.
That means compilers have a much harder time optimizing floating-point calculations than they do integer calculations.

The following pair of functions is almost the same as the previous pair, just using floats instead of integers:

```{python}
@jit("float64(float64, int64)")
def add_n_times_float(value, n):
    result = 0.0
    for _ in range(100_000):
        for _ in range(n):
            result += value
    return result

@jit("float64(float64, int64)")
def multiply_float(value, n):
    return 100_000 * value * n

assert add_n_times_float(12.0, 3) == multiply_float(12.0, 3)
```

Unlike the integer version, the Numba compiler cannot optimize the float version of `add_n_times_float()` into the equivalent of `multiply_float()`.
As a result, you can see a significant difference in speed:

```{python}
#| echo: false
%%compare_timing
add_n_times_float(12.0, 3)
multiply_float(12.0, 3)
```

## Mechanism #3: How CPUs work

Modern CPUs work in ways that can speed up your code significantly, if you make sure to write your code appropriately.
But if you're writing regular Python, the general slowness of the Python interpreter mean these particular effects are usually not worth thinking about.
Once you're writing faster compiled code, however, you can take advantage of these features to run even faster.

### Automatic parallel execution: Instruction-level parallelism

Consider the following code:

```python
a += 1;
b += 2;
```

In Python, you would expect those two operations to happen sequentially.
First `a` gets incremented, then `b`.

But while the code itself is sequential, the CPU may run the relevant instructions _in parallel_ on the same core.
This is completely unrelated to the parallelism provided by threads or processes.
This is parallelism at the level of CPU instructions, on a single CPU coreâ€”and the CPU does it automatically.

Consider these two functions:

```{python}
@jit
def add_once(number):
    result = 0.0
    for _ in range(1_000_000):
        result += number
    return result

@jit
def add_twice(number):
    result1 = 0.0
    result2 = 0.0
    for _ in range(1_000_000):
        result1 += number
        result2 += number
    return (result1, result2)

once = add_once(3.2)
assert add_twice(3.2) == (once, once)
```

Despite `add_twice()` using twice as many CPU instructions, it finishes in the same amount of time, thanks to instruction-level parallelism:

```{python}
#| echo: false
%%compare_timing --measure=instructions
add_once(3.2)
add_twice(3.2)
```

You can sometimes significantly speed up your code by ensuring the CPU can take advantage of instruction-level parallelism.

### Speculative execution and branch (mis)prediction

To maximize instruction-level parallelism, the CPU will use "speculative execution" when it encounters a branch.
Consider the following code:

```python
@jit
def is_bigger_than_half(values):
    # ... create result array ...
    for i in range(len(values)):
        if values[i] > 0.5:
            result[i] = True
    # ...
```

The CPU will try to predict the future.
In particular, it will heuristically guess if `values[i] > 0.5` is true, and if decides it is, it will run `result[i] = True` in parallel to the `if` statement.
If this guess turns out to be right, your code runs faster without any work on your part.
Success!

But if this guess turns out to be wrong, what's known as a "branch misprediction," the CPU will need to undo the incorrect work it did.
Then, it will have to calculate the correct result given the real result of the `if` statement.
Ultimately the computation will match what would have happened if the prediction had been correct, but the program will run much slower because of all the extra work involved.

Most of the time branches are predictable, and the cost of mispredictions is therefore low.
But what happens when you have data that is hard to predict?
Your code can get very, very slow.

```{python}
#| echo: false
import numpy as np

@jit
def is_bigger_than_half(arr):
    result = np.zeros((len(arr),), dtype=np.bool_)
    total = 0
    for i in range(len(arr)):
        if arr[i] > 0.5:
            result[i] = True
        total += 1
    return result

# An array with predictable values: the first half are all 0s, the second half
# are all 1s.
PREDICTABLE = np.ones((1_000_000,), dtype=np.float64)
PREDICTABLE[500_000:] = 0
assert is_bigger_than_half(PREDICTABLE).sum() == 500_000

# An array with random values between 0 and 1.
RANDOM_NUMS = np.random.random((1_000_000,))

# Same length:
assert len(PREDICTABLE) == len(RANDOM_NUMS)
# The number of values > 0.5 differs by less than 1%:
assert abs(
    is_bigger_than_half(PREDICTABLE).sum() -
    is_bigger_than_half(RANDOM_NUMS).sum()
) / len(PREDICTABLE) < 0.01
```

I benchmarked `is_bigger_than_half()` with two different inputs:

* `PREDICTABLE` has long sequences of identical values, making it easy for the CPU to predict the `if values[i] > 0.5` branch.
* `RANDOM_NUMS` has randomly generated values between 0 and 1, so it's impossible to predict whether a given value will be bigger than 0.5.

Notice that the number of CPU instructions used is almost the same.
But `is_bigger_than_half(RANDOM_NUMS)` is much slower, because it triggers far more branch mispredictions:

```{python}
#| echo: false
%%compare_timing --measure=instructions,branch_mispredictions
is_bigger_than_half(PREDICTABLE)
is_bigger_than_half(RANDOM_NUMS)
```

### Memory access patterns impact performance

Reading and writing memory can be fastâ€”or slow!
It depends on how much memory your code is reading and writing, and the access pattern you're using.
For example, you can access an array of data linearly, from start to finish:

```{python}
@jit
def _scan(arr, multiplier):
    index = 0
    size = len(arr)
    for _ in range(1_000_000):
        arr[index] += 1
        index = (multiplier * index + 1) % size
    return arr[0]

def linear_scan(arr):
    return _scan(arr, 1)
```

Or, you can access an array pseudo-randomly, jumping around:

```{python}
def random_scan(arr):
    return _scan(arr, 22695477)
```

Given the same array of one million entries (not shown), the linear scan is faster than the random scan even though both functions use almost the same number of CPU instructions:

```{python}
#| echo: false
import numpy as np

DATA = np.ones((100_000_000,), dtype=np.uint8)
_ = linear_scan(DATA)
_ = random_scan(DATA)
```

```{python}
#| echo: false
%%compare_timing --measure=instructions
linear_scan(DATA)
random_scan(DATA)
```

## Writing faster code

To speed up your computational code, you should optimize it by:

1. Using a scalable algorithm.
2. Avoiding wasted effort.
3. Using a compiled language.
4. Enabling the compiler to optimize your code.
5. Enabling the CPU's performance features (instruction-level parallelism, speculative execution, memory caches and pre-fetch, and more).

Beyond that, though it's not covered in this book, you can and should also use parallelism across multiple CPU cores to go even faster, by utilizing multiple threads or processes.
While you could jump straight to parallelism, optimization is still immensely useful:

* The performance benefit of optimizing your code is multiplicative with parallelism: if you can make your code 10Ã— faster on a single thread, and then 10Ã— faster with parallelism, you will get results 100Ã— faster.
* Both optimization and parallelism reduce electricity usage, and therefore carbon emissions; the reduction seems at least somewhat multiplicative.
* Unlike parallelism, optimization can save you money, since you get faster results without paying for more expensive hardware.

If possible, then, you should be applying both techniques.

Ready to get faster results from your code?
Let's get started!
