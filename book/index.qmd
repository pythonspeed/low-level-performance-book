# Introduction {.unnumbered}

This book is specifically written for scientists, data scientists, and software developers who use Python to do numeric computing or similar large-scale data processing.
Python[^cpython] is notoriously slow for computation, yet often used to handle large-scale data processing.
Getting faster results typically involves writing Python extensions in a fast, low-level, compiled language: C, C++, Fortran, Cython, Numba, Rust, and others.

[^cpython]: In general I'm talking about the default implementation, rather than the language. To distinguish the two, the Python interpreter is often known as CPython. The PyPy interpreter is a different implementation of the language that uses just-in-time compilation do math much faster, but it adds overhead when interoperating with NumPy and other similar libraries, and lags behind on language features. A future version of CPython might also include some just-in-time compilation speedups.

Speeding up your Python code with a compiled extension can seem like magic: switch to a compiled language, CODE GO FAST!!!

But if you just think of compiled code as magic, you may be missing significantâ€”and sometimes quite simple!â€”optimizations that can make your code even faster.
Or to put it a different way, you might still be unknowingly writing slow code.
If you really want your code to run faster, you will need a deeper understanding of:

1. Your code.
2. The compiler.
3. The CPU it runs on.

```{python}
#| echo: false
%load_ext book_magics
```

## Understanding your code's performance

Not all bottlenecks are computational.
Your code may end up waiting for the network, or for a database query, or for a user to respond to a question.
Switching to a compiled language won't help in this case.
We'll briefly cover this in the next chapter, and otherwise I'll assume you've already determined that computation is the performance bottleneck you need to solve.

### Use a scalable algorithm

If the bottleneck is computation, you need to make sure your algorithms and data structures are scalable.
If they're not, sooner or later your code will be far too slow, no matter what programming language it's written in:

```{python}
# ðŸ˜¢ This is extremely unscalable!
def find_intersection(a: list, b: list) -> list:
    result = []
    for value in a:
        if value in b:
            result.append(value)
    return result

# ðŸ˜Ž This is much better.
def find_intersection_scalable(a: list, b: list) -> list:
    result = []
    b_set = set(b)
    for value in a:
        if value in b_set:
            result.append(value)
    return result

LIST1 = list(range(200))
LIST2 = [17, 23, 5, 1, 4, 2, 8, 9, 10, 15, 16, 12, 13]

assert (
    find_intersection(LIST1, LIST2) ==
    find_intersection_scalable(LIST1, LIST2)
)
```

Here's how the two versions compare in terms of speed:

```{python}
#| echo: false
%%compare_timing
find_intersection(LIST1, LIST2)
find_intersection_scalable(LIST1, LIST2)
```
We'll briefly cover algorithm scalability in a later chapter.

### Avoid wasted effort and unnecessary precision

Even once you've chosen a scalable algorithm, it's quite possible your code is doing repetitive and unnecessary work.
While a compiler can help with this, it can only do so much.
And to some extent what counts as unnecessary work is something you have to decide:

* Does all the data need to be processed?
* How precise does the answer need to be?

Only you can answer these questions.

We'll cover this is in a number of chapters; for example, in one chapter we'll see how you can make this algorithm 80% faster:

```python
# ðŸ˜¢ The Sieve of Eratosthenes algorithm for finding primes, implemented
# inefficiently.
def find_primes(up_to_value: int) -> list[bool]:
    is_prime = [True] * up_to_value
    is_prime[0] = False
    is_prime[1] = False

    for i in range(2, up_to_value):
        if not is_prime[i]:
            continue
        for j in range(i * 2, up_to_value, i):
            is_prime[j] = False

    return is_prime
```

## Understanding how compilers work

Once you have a scalable algorithm without repetitive or unnecessary work, switching to a compiled language can, at least some of the time, make your code even faster.
A compiler takes some source code and turns into machine code instructions that run directly on the CPU.
But of course Python is also written in a compiled language, so why is running Python code often so much slower than the equivalent compiled language code?

### Static typing allows for faster code

One reason is that compiled languages allow a compiler to convert your source code to machine code in a very specialized way.

Consider the following Python function:

```{python}
def add(a: int, b: int) -> int:
    return a + b
```

As a reader, you know it's adding two integers, but Python doesn't actually take advantage of the `int` type annotations; they're hints, not instructions, and if you want to you could call `add("abc", "def")` and the code would run just fine.
That means Python needs to figure out at runtime which type each of the objects is, and then figure out which function implements addition for those types.
In addition, Python integers can be arbitrarily large, so Python needs to at minimum figure out if the integers are small enough to use less generic routines.
All of this work adds up.

Compare that to a similar Rust function[^rust]:

```rust
fn add(a: i64, b: i64) -> i64:
    return a + b;
```

[^rust]: This is not idiomatic, insofar as Rust typically uses implicit return. However, for people who don't know Rust it's easier to read.

In this case, the Rust compiler can take advantage of the types, and what's more, we're dealing with a _specific_ integer type: 64-bit signed integers[^signed].
CPUs have instructions for adding these types, so the compiler doesn't have to deal with different object types, or different integer types, it can just generate machine code for adding two 64-bit signed integers.

How do these two functions compare?

* The Python `add()` will use tens of thousands of CPU instructions to run.
* On x86-64, when compiled in release mode, the Rust function compiles down to just 3 CPU instructions.

[^signed]: A signed integer is one that can be either negative or positive; an unsigned integer can only be positive.

### The compiler will optimize your code...

Compilers do more than just translate your code to machine code: they also optimize it along the way.
In particular, they will transform your code into (hopefully) faster code.
In many cases this means that with no effort on your part your code will just be faster, as we'll see in a later chapter.

For example, in this book we'll be using Numba, a compiled language based on Python.
The Numba compiler turns a function into compiled code by using the `@numba.jit` decorator:

```{python}
from numba import jit

# A compiled function that takes two int64 arguments and returns an int64
# result:
@jit("int64(int64, int64)")
def add_n_times(value, n):
    result = 0
    for _ in range(10_000):
        for _ in range(n):
            result += value
    return result
```

When the compiler compiles this code, it will essentially turn it into an equivalent, much faster, function that looks like this:

```{python}
@jit("int64(int64, int64)")
def multiply(value, n):
    return 10_000 * value * n

assert add_n_times(17, 3) == multiply(17, 3)
```

We can see they're equivalent if we compare their performance; the elapsed time is mostly just function call overhead:

```{python}
#| echo: false
%%compare_timing
add_n_times(17, 3)
multiply(17, 3)
```

### ...except when it can't

The compiler's ability to optimize your code has an important caveat: the compiler will only transform your code in ways that ensure the resulting code behaves in the _exact_ same way.
Sometimes, a slightly different version of the code could be much faster, but the compiler won't be able to choose it.
That means you can make your code faster by spotting these problems, and either transforming your code so the compiler can optimize it, or just doing the optimization yourself.

Some examples of problems that can prevent the compiler from optimizing your code:

* **Aliasing:** If it's possible that the program has multiple writable references to the same memory, the compiler won't be able to optimize reads to that memory.
  The problem, and some solutions, is covered in one chapter of the book.
* **Floating point math:** Changing the order of floating calculations can change the results, which limits what the compiler can do.
  The problem and solutions are covered in multiple chapters.

As an example, the following pair of functions is almost the same as the previous pair, with the exception that they're using floats:

```{python}
@jit("float64(float64, int64)")
def add_n_times_float(value, n):
    result = 0.0
    for _ in range(10_000):
        for _ in range(n):
            result += value
    return result

@jit("float64(float64, int64)")
def multiply_float(value, n):
    return 10_000 * value * n

assert add_n_times_float(12.0, 3) == multiply_float(12.0, 3)
```

Unlike the integer version, the Numba compiler will _not_ be able to optimize the float version of `add_n_times_float()` into the equivalent of `multiply_float()`; the latter will be much faster:

```{python}
#| echo: false
%%compare_timing
add_n_times_float(12.0, 3)
multiply_float(12.0, 3)
```

### And more!

Other useful topics covered by this book include:

* Stack memory vs heap memory.
* Avoiding undefined behavior; if your program is wrong, it doesn't matter how fast it is.
* Hoisting type conversions out of inner loops.
* Avoiding integer overflows.

## Understanding how CPUs work

Modern CPUs work in ways that can speed up your code significantly, if you make sure to write your code appropriately.
But if you're writing regular Python, the general slowness of the Python interpreter mean these particular effects are usually not worth thinking about.
Once you're writing faster compiled code, however, you can take advantage of these features to run even faster.

### Automatic parallel execution: instruction-level parallelism

Consider the following code:

```rust
a += 1;
b += 2;
```

In Python, you would expect those two operations to happen sequentially.
First `a` gets incremented, then `b`.

In a compiled language, the code may well be compiled into two sequential operations.
But when the CPU runs the code, the CPU may run those two instructions _in parallel_ on the same core.
This is completely unrelated to threads or processes, the usual way we think of parallelism in general purpose computers.
This is parallelism at the level of CPU instructions, on a single CPU core, no threads or processes involvedâ€”and the CPU does it automatically.

In some cases the CPU won't be able to use parallelism (or as much parallelism, at least) because the order really does matter, for example:

```rust
a += 1;
// This can only run after `a` has been incremented:
b += a;
```

That means you can potentially speed up your code by restructuring it remove logical dependencies that prevent parallelism.
We'll cover this in a later chapter.

### Avoiding branches

In order to maximize instruction-level parallelism, the CPU will use speculative execution when it encounters a branch.
In the following code:

```rust
a += 1;
if a > 2 {
    b += 1;
}
```

The CPU will heuristically guess if `a > 2` will be true, and if decides it is, it will run `b += 1` in parallel to `a += 1` even before the `if` statement runs (or rather, the relevant generated CPU instructions run).
If this guess turns out to be wrong, the work will have to be undone... and this is slow.

As a result, branches where the result is unpredictable can make your program run much more slowly.
In addition, branches can prevent compiler optimizations, and are often simply unnecessary.
We'll cover ways to avoid branches in a number of chapters.

### Memory access patterns impact performance

Reading and writing memory can be fastâ€”or slow!
It depends on how much memory is involved, and the access pattern you're using.
For example, we can access an array of data linearly, from start to finish:

```{python}
@jit
def linear_scan(arr, multiplier):
    index = 0
    size = len(arr)
    for _ in range(1_000_000):
        arr[index] += 1
        index = (multiplier * index + 1) % size
    return arr[0]
```

Or, we can access an array pseudo-randomly, jumping around:

```{python}
@jit
def random_scan(arr):
    index = 0
    size = len(arr)
    for _ in range(1_000_000):
        arr[index] += 1
        index = (22695477 * index + 1) % size
    return arr[0]
```

Given the same array of one million entries (not shown), the linear scan is faster than the random scan even though it uses the same number of CPU instructions:

```{python}
#| echo: false
import numpy as np

DATA = np.ones((100_000_000,), dtype=np.uint8)
_ = linear_scan(DATA, 1)
_ = random_scan(DATA)
```

```{python}
#| echo: false
%%compare_timing --measure=instructions
linear_scan(DATA, 1)
random_scan(DATA)
```

We'll discuss memory's impact on performance in a number of chapters.

### And more!

There's far more to learn about how CPUs work, from SIMD (minimally introduced in a couple of chapters) to details that are beyond the scope of this book (SIMD in depth, Âµops, instruction latency, and more).

## Let's get started

The above is just some of what this book covers, but it gives you the big picture of optimizing single-threaded computational code.
You should write code:

1. Using a scalable algorithm
2. Without wasted effort.
3. Using a compiled language.
4. Without preventing the compiler from optimizing your code.
4. Without getting in the way of the CPU's performance features (instruction-level parallelism, speculative execution, memory caches and pre-fetch, and more).

Beyond that, though it's not covered in this book, you can and should also use parallelism across multiple CPU cores to go even faster, by utilizing multiple threads or processes.

While you could jump straight to parallelism, optimization is still immensely useful:

* The performance benefit of optimizing your code is multiplicative with parallelism: if you can make your code 10Ã— faster on a single thread, and then 10Ã— faster with parallelism, you will get results 100Ã— faster.
* Both optimization and parallelism reduce electricity usage, and therefore carbon emissions; the reduction seems at least somewhat multiplicative.
* Unlike parallelism, optimization can save you money, since you get faster results without paying for more expensive hardware.

If possible, then, you should be applying both techniques.

Ready to get faster results from your code?
Let's get started!
