# Introduction {.unnumbered}

Speeding up Python code with a compiled extension can seem like magic:

1. Switch languages to Rust, Cython, C++, Numba, or others.
2. CODE GO FAST!!!

But if you just think of compiled code as magic, you may be missing significantâ€”and sometimes quite simple!â€”optimizations.
Or to put it a different way, you might still be unknowingly writing slow code.

If you really want your code to run faster, you will need a deeper understanding of:

1. Your code.
2. The compiler.
3. The CPU it runs on.

```{python}
#| echo: false
%load_ext book_magics
```

## Understanding your code's performance

Not all bottlenecks are computational.
Your code may end up waiting for the network, or for a database query, or for a user to respond to a question.
Switching to a compiled language won't help in this case.
We'll briefly cover this in the next chapter.

Even if the bottleneck is computation, you need to make sure your algorithms and data structures are scalable.
If they're not, sooner or later your code will be far too slow, no matter what programming language it's written in:

```{python}
# ðŸ˜¢ This is extremely unscalable!
def find_intersection(a: list, b: list) -> list:
    result = []
    for value in a:
        if value in b:
            result.append(value)
    return result

# ðŸ˜Ž This is much better.
def find_intersection_scalable(a: list, b: list) -> list:
    result = []
    b_set = set(b)
    for value in a:
        if value in b_set:
            result.append(value)
    return result

LIST1 = list(range(200))
LIST2 = [17, 23, 5, 1, 4, 2, 8, 9, 10, 15, 16, 12, 13]

assert (
    find_intersection(LIST1, LIST2) ==
    find_intersection_scalable(LIST1, LIST2)
)
```

Here's how the two versions compare in terms of speed:

```{python}
#| echo: false
%%compare_timing
find_intersection(LIST1, LIST2)
find_intersection_scalable(LIST1, LIST2)
```
We'll briefly cover algorithm scalability in a later chapter.

Even once you've chosen a scalable algorithm, it's quite possible your code is doing repetitive and unnecessary work.
While a compiler can help with this, it can only do so much.
And to some extent what counts as unnecessary work is something you have to decide:

* Does all the data need to be processed?
* How precise does the answer need to be?

Only you can answer these questions.

We'll cover this is in a number of chapters; for example, in one chapter we'll see how you can make this algorithm 80% faster:

```python
# ðŸ˜¢ The Sieve of Eratosthenes algorithm for finding primes, implemented
# inefficiently.
def find_primes(up_to_value: int) -> list[bool]:
    is_prime = [True] * up_to_value
    is_prime[0] = False
    is_prime[1] = False

    for i in range(2, up_to_value):
        if not is_prime[i]:
            continue
        for j in range(i * 2, up_to_value, i):
            is_prime[j] = False

    return is_prime
```

## Understanding how compilers work

Once you have a scalable algorithm without repetitive or unnecessary work, switching to a compiled language can, at least some of the time, make your code even faster.
A compiler takes some source code and turns into machine code instructions that run directly on the CPU.
But of course Python[^cpython] is also written in a compiled language, so why is running Python code often so much slower than the equivalent compiled language code?

[^cpython]: In general I'm talking about the default implementation, rather than the language. To distinguish the two, the Python interpreter is often known as CPython. The PyPy interpreter is a different implementation of the language that uses just-in-time compilation do math much faster, but it adds overhead when interoperating with NumPy and other similar libraries, and lags behind on language features. A future version of CPython might also include some just-in-time compilation speedups.

### Static typing allows for faster code

One reason is that compiled languages allow a compiler to convert your source code to machine code in a very specialized way.

Consider the following Python function:

```{python}
def add(a: int, b: int) -> int:
    return a + b
```

As a reader, you know it's adding two integers, but Python doesn't actually take advantage of the `int` type annotations; they're hints, not instructions, and if you want to you could call `add("abc", "def")` and the code would run just fine.
That means Python needs to figure out at runtime which type each of the objects is, and then figure out which function implements addition for those types.
In addition, Python integers can be arbitrarily large, so Python needs to at minimum figure out if the integers are small enough to use less generic routines.
All of this work adds up.

Compare that to a similar Rust function[^rust]:

```rust
fn add(a: i64, b: i64) -> i64:
    return a + b;
```

[^rust]: This is not idiomatic, insofar as Rust typically uses implicit return. However, for people who don't know Rust it's easier to read.

In this case, the Rust compiler can take advantage of the types, and what's more, we're dealing with a _specific_ integer type: 64-bit signed integers, i.e. they can be negative or positive.
CPUs have instructions for adding these types, so the compiler doesn't have to deal with different object types, or different integer types, it can just generate machine code for adding two 64-bit signed integers.
On x86-64, when compiled in release mode, this function compiles down to just 3 CPU instructions.
In comparison, the Python `add()` uses approximately 68,000 instructions!

### The compiler will optimize your code (except when it can't)

Compilers do more than just translate your code to machine code: they also optimize it along the way.
In particular, they will transform your code into (hopefully) faster code.
In many cases this means your code is made faster, with no work on your part, as we'll see in a later chapter.

For example, the Rust compiler will automatically transform this function:

```rust
fn add_n_times(value: i64, n: i64) -> i64 {
    let mut result = 0;
    // Add value to result, n times:
    for _ in 0..n {
        result += value;
    }
    return result;
}
```

to the equivalent but much faster function:

```rust
fn add_n_times(value: i64, n: i64) -> i64 {
    return value * times;
}
```

This optimization has a caveat: the compiler will only transform your code in ways that ensure the resulting code behaves in the _exact_ same way.
Sometimes, a slightly different version of the code could be much faster, but the compiler won't be able to choose it.
That means you can make your code faster by spotting these problems, and either transforming your code so the compiler can optimize it, or just doing the optimization yourself.

Some examples:

* **Aliasing:** If it's possible that the program has multiple writable references to some memory, the compiler won't be able to optimize reads to that memory.
  The problem, and some solutions, is covered in one chapter of the book.
* **Floating point math:** Changing the order of floating calculations can change the results, which limits what the compiler can do.
  The problem and solutions are covered in multiple chapters.

The following function is almost the same as the previous function, with the exception that it's using floats.
Unlike the integer version, the Rust compiler will _not_ be able to optimize it into multiplication:

```rust
fn add_n_times(value: f64, n: i64) -> f64 {
    let mut result = 0.0;
    for _ in 0..n {
        result += value;
    }
    return result;
}
```

## Understanding how CPUs work

Once you're dealing with compiled code, you have access to additional ways to optimize your code.
Modern CPUs work in ways that can speed up your code significantly, if you make sure to write your code appropriately.
If you're writing regular Python, the general slowness of the Python interpreter mean these particular effects are usually not worth thinking about.
But once you're writing faster compiled code, you can take advantage of these features to run even faster.

### Automatic parallel execution: instruction-level parallelism

Consider the following code:

```rust
a += 1;
b += 2;
```

In Python, you would expect those two operations to happen sequentially.
First `a` gets incremented, then `b`.

In a compiled language, the code may well be compiled into two sequential operations.
But when the CPU runs the code, the CPU may run those two instructions _in parallel_ on the same core.
This is completely unrelated to threads or processes, the usual way we think of parallelism in general purpose computers.
This is parallelism at the level of CPU instructions, and the CPU does it automaticallyâ€”if it can.

In some cases the CPU won't be able to use parallelism (or as much parallelism, at least) because the order really does matter, for example:

```rust
a += 1;
// This can only run after `a` has been incremented:
b += a;
```

We'll cover this in a later chapter.

### Avoiding branches

In order to maximize this sort of instruction-level parallelism, the CPU will use speculative execution in branches.
In the following code:

```rust
a += 1;
if a > 2 {
    b += 1;
}
```

The CPU will heuristically guess if `a > 2` is true, and if it is, it will run `b += 1` in parallel.
If this is guess turns out to be wrong, the work will have to be undone... and this is slow.
As a result, branches where the result is unpredictable can make your program run much more slowly.

More broadly, branches can prevent compiler optimizations, and are often unnecessary.
We'll cover ways to avoid branches in a number of chapters.


### Manual parallel execution with Single Instruction Multiple Data

### Memory access patterns impact performance

<!--
Your software is too slow.
Now what?

In many ways this depends on what kind of software you're writing: web applications, video games, and scientific computing all have different bottlenecks.
This book is specifically written for scientists, data scientists, and software developers who use Python to do numeric computing or data processing.

There are also many potential reasons your software might be slow.
I provide a quick review of the bigger picture of performance in the first chapter of the book, together with references to additional resources.
The rest of this book focuses on one particularly pervasive bottleneck: computation.
That is, the work your CPU does to calculate results, as well as related bottlenecks like memory.

Assuming your performance bottleneck is computation, how do you speed it up?

## Optimizing Python-based data processing computation

Regardless of your choice of programming language, you will want to:

1. Use scalable algorithms and data structures.
   That's mostly a topic for a different book, but we will touch on it in one chapter.
2. Avoid unnecessary and repetitive work.

Python[^cpython] is notoriously slow for computation, yet often used to handle large-scale data processing.
Getting faster results typically involves writing Python extensions in a fast, low-level, compiled language: C, C++, Fortran, Cython, Numba, Rust, and others.

There are many pre-written compiled extensions available as free libraries: NumPy, SciPy, Pandas, and Polars, to name just a few.
Typically they work by operating on batches of dataâ€”an array, or a dataframe.
Sometimes, however, preexisting libraries aren't fast enough, or aren't memory efficient enough, or simply don't implement the algorithms you need.
In that case you need to write your own Python extensions.

## From Python code to compiled code

But if you want fast code, using a compiled language is usually helpful, but not always sufficient.

In addition, once you make the switch to a compiled language, you can achieve major performance improvements by taking into account compiler and CPU behavior.
In particular, you can write code that:

1. Enables the compiler to generate even faster code, for example by avoiding aliasing.
2. Takes advantage of the sometimes unexpected ways modern CPUs work, from instruction-level parallelism to branch prediction to the CPU memory hierarchy.

We'll cover all of these in this book, focusing in particular on avoiding unnecessary work, and building a better mental model of compilers and CPUs.

## Optimization and parallelism: you want both!

This book focuses on optimizing your code on a single CPU core.
But though it's not covered in the book, you can and should also use parallelism across multiple cores.

And while you could jump straight to parallelism, optimization is still immensely useful:

* The performance benefit of optimizing your code is multiplicative with parallelism: if you can make your code 10Ã— faster on a single thread, and then 10Ã— faster with parallelism, you will get results 100Ã— faster.
* Both optimization and parallelism reduce electricity usage, and therefore carbon emissions; the reduction seems at least somewhat multiplicative.
* Unlike parallelism, optimization can save you money, since you get faster results without paying for more expensive hardware.

If possible, you should be applying both techniques.

Ready to get faster results from your code?
Let's get started!

[^cpython]: Technically, it's not the language that's slow, it's the default implementation. To distinguish the two, the Python interpreter is often known as CPython. The PyPy interpreter is a different implementation of the language that can do math much faster, but it adds overhead when interoperating with NumPy and other similar libraries, and lags behind on language features.
-->
