# Introduction {.unnumbered}

There are many kinds of software you can write: web applications, video games, scientific computing, and much more.
This book doesn't address all these types of software.
Instead, this book is specifically written for scientists, data scientists, and software developers who use Python to do numeric computing or data processing.

Even with that limited focus, there are still many potential reasons your software might be slow.
Your bottleneck might be computation, or network latency, or disk bandwidth.
Once again, this book won't cover all of these.
This book focuses on one particular subset of the performance problem: computation.

In particular, in most of this book I'm assuming that:

1. You've already profiled your code to find the key bottleneck.
2. You've discovered the bottleneck is computation, i.e. the amount of CPU time the code uses.
   If the bottleneck was something else, like network latency, this book won't help.

I'll be covering these last two in the end of the book; if you're not familiar with the process, you may wish to [start by reading that section first](jump to the last part of the book).
So assuming your bottleneck is computation, how do you speed it up?

## Speeding up Python-based data processing computation

Python[^cpython] is notoriously slow, but often used to handle large-scale data processing.
In order to do so in a reasonable amount of time, the solution has traditionally involved writing Python extensions in a fast, low-level, compiled language: C, C++, Fortran, Cython, Numba, Rust, and others.

There are many pre-existing compiled extensions available as free libraries: NumPy, SciPy, Pandas, and Polars, to name just a few.
If those libraries are fast enough for you, no need to read the rest of this book!

Sometimes, however, preexisting libraries aren't fast enough, or aren't memory efficient enough, or simply don't implement the algorithms you need.
In that case, you will need to write your own code in a low-level language.
And if that isn't sufficient, the next common advice you'll get is to switch to using parallelism.

## The missing step: optimization

There's a missing step in this process, though.
A better plan is as follows:

1. Re-write your code in compiled language.
2. **Optimize your code.**
   This is the main focus of this book.
3. If it's still necessary, take advantage of parallelism, using multiple CPUs or multiple machines.

Why not jump straight to parallelism?

Imagine you've done step 1, and you still need to make your code 100× faster.
That means that if you're going to rely on optimism, you will need to use 100 CPU cores, which can be very expensive, whether you're running in the cloud or buying a local machine.
You might also need to switch to a solution that involves multiple machines, which makes your software that much more complex.

If, however, you can optimize your code first and make it 10× faster, you only need to use 10 cores to reach that final 100× speedup.
That means you've dropped the financial cost of computing by 90%.
It also means you can stick to running on a single machine, making your life much easier.

## Reading the book

Most of the examples in this book will use the Numba language and compiler, because it is easy to install and uses the same syntax as Python.
However, this is not a book about Numba: the concepts the book covers also apply to C, C++, Cython, Rust, and other low-level languages.
I will discuss the functionality of these other languages where relevant.

If you like starting with the big picture, you might want to [jump to the last part of the book](part_9/01_when.qmd) and then come back to the beginning later.
Otherwise, let's leap right in to speeding up your computation.

[^cpython]: Technically, it's not the language that's slow, it's the default implementation. To distinguish the two, the Python interpreter is often known as CPython. The PyPy interpreter is a different implementation of the language that can do math much faster, but it adds overhead when interoperating with NumPy and other similar libraries, and lags behind on language features.
