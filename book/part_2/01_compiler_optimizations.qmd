# Rely on the compiler to optimize your code

When your code gets compiled, the compiler runs it through a series of optimization passes, with each pass transforming the code in the some way.
The hope is that the cumulative series of transformations will result in code that is more efficient by:

1. Removing redundant code execution, our focus in this chapter.
2. Utilizing the CPU as efficiently as possible.
   We won't consider these optimizations here, but we'll cover at least some of them in later chapters.

```{python}
# Disable automatic usage of SIMD so it doesn't distort or hide certain
# effects; we'll talk about SIMD in later chapters.
import os
os.environ["NUMBA_LOOP_VECTORIZE"] = "0"

from numba import njit
import numpy as np
```

```{python}
#| echo: false
%load_ext book_magics
```

## Get a sense of what the compiler can do

### Precalculating mathematical expressions

If you have a fixed mathematical expression, the compiler can often just execute it at compilation time and replace the expression with the result.
For example, notice how the first function is slower, while the latter two take the same amount of time:

```{python}
@njit
def calc(a, b, c, d, e, f):
    return ((a + b + c) ** d / e) + f

@njit
def calc_fixed():
    return ((1 + 2 + 3) ** 7 / 6.0) + 5

@njit
def just_the_result():
    return 46661.0

# All three functions give the same result:
assert calc_fixed() == calc(1, 2, 3, 7, 6.0, 5)
assert calc_fixed() == just_the_result()
```

Here's how their speed compares:

```{python}
#| echo: false
%%compare_timing

calc(1, 2, 3, 7, 6.0, 5)
calc_fixed()
just_the_result()
```

### Hoisting expressions out of inner loops

The compiler can also hoist repetitive expressions out of loops, so they're called once instead of many times.
For example, the expression `((n ** 2) * (1.5 / n) + n)` in the `compiler_will_hoist()` is repeated in every iteration, but it also always give the same result.
The compiler is therefore able to move it out of the loop and run it only once, as we do manually in `manually_hoisted()`.
Thus both functions take the same amount of time to run:

```{python}
@njit
def compiler_will_hoist(m, n):
    total = 0
    for i in range(m):
        total = m * ((n ** 2) * (1.5 / n) + n)
    return total

@njit
def manually_hoisted(m, n):
    total = 0
    constant = (n ** 2) * (1.5 / n) + n
    for i in range(m):
        total = m * constant
    return total

# The results are the same:
assert compiler_will_hoist(100, 17) == manually_hoisted(100, 17)
```

Notice they run at the same speed, because the compiler automatically optimizes the first version by hoisting the constant expression:

```{python}
#| echo: false
%%compare_timing

compiler_will_hoist(1_000_000, 3)
manually_hoisted(1_000_000, 3)
```

While both versions have the same speed, the manually hoisted version is arguably clearer about what is going on in the calculation, and the compiler won't always be able to hoist expressions, so doing it manually might still be worth it.

### Inlining

Calling a function takes some time, even in a low-level language.
What's, it's difficult for the compiler to optimize across function boundaries.
Thus, the compiler will sometimes take the contents of a function and put them inside the calling function, so that it can do more optimization passes.
This is known as "function inlining".

```{python}
@njit
def in_to_cm(cm):
    return cm * 0.394

@njit
def cm3_to_liters(cm3):
    return cm3 / 1000

@njit
def volume_liters(width_in, height_in, depth_in):
    width_cm = in_to_cm(width_in)
    height_cm = in_to_cm(height_in)
    depth_cm = in_to_cm(depth_in)
    return cm3_to_liters(width_cm * height_cm * depth_cm)

@njit
def volume_liters_manual_inlining(width_in, height_in, depth_in):
    width_cm = width_in * 0.394
    height_cm = height_in * 0.394
    depth_cm = depth_in * 0.394
    return (width_cm * height_cm * depth_cm) / 1000

assert (
    volume_liters(17.0, 23.0, 52.0) ==
    volume_liters_manual_inlining(17.0, 23.0, 52.0)
)
```

We can see that the extra function calls in `volume_liters()` don't add overhead, presumably because the compiler inlined them.

```{python}
#| echo: false
%%compare_timing
volume_liters(17.0, 23.0, 53.0)
volume_liters_manual_inlining(17.0, 23.0, 53.0)
```

There are many other optimizations the compiler can do, with more added or tweaked with every compiler release.

### Using faster CPU instructions

For any given piece of code there are different ways the compiler could translate the code into CPU instructions.
This is relevant because some CPU instructions are faster than others.
For example, division is a complex operation, so in situations where the compiler can avoid using it, it will try to use other CPU instructions.

One such useful instruction is bitshifting: a bitshift of 3 to the right will change 97 to 12, as it's shifting the bits of `1100001` three to the right, resulting in `1100`.
Put another way, bitshifting N to the right is equivalent to dividing by $2^N$.

```{python}
print(97 >> 3)
```

```{python}
print(97 // (2 ** 3))
```

If the compiler is asked to do a generic division, it can't really switch to other CPU instructions, so it's stuck with slow division instead of a fast bitshift.

```{python}
@njit
def generic_division(divisor, size):
    arr = np.empty((size,), dtype=np.uint64)
    for i in range(len(arr)):
        arr[i] = i // divisor
    return arr

@njit
def generic_bitshift(bits, size):
    arr = np.empty((size,), dtype=np.uint64)
    for i in range(len(arr)):
        arr[i] = i >> bits
    return arr

assert np.array_equal(
    generic_division(8, 1000),
    generic_bitshift(3, 1000)
)
```

```{python}
#| echo: false
%%compare_timing
generic_division(8, 100_000)
generic_bitshift(3, 100_000)
```

If the compiler knows you're dividing by a power of 2, it can choose to use faster operations than division to get the same result.
To see whether bitshifts were actually used we'd have to look at the generated CPU instructions, but even without doing that we can see that in this case the compiler was able to generate faster code to get the _equivalent_ of division by 8:

```{python}
@njit
def specific_division_by_8(size):
    arr = np.empty((size,), dtype=np.uint64)
    for i in range(len(arr)):
        arr[i] = i // 8
    return arr

assert np.array_equal(
    specific_division_by_8(1000),
    generic_division(8, 1000)
)
```

```{python}
#| echo: false
%%compare_timing
generic_division(8, 100_000)
specific_division_by_8(100_000)
```

::: {.callout-note}
For Numba, you can see the underlying compiler's optimized output—specifically, the LLVM intermediate representation—by setting this option before you import `numba` for the first time:

```{python}
#| eval: false
os.environ["NUMBA_DUMP_OPTIMIZED"] = "1"
```

You can then search for `OPTIMIZED DUMP your_functions_name` in the output (replace `your_functions_name` with your function's name as relevant).
:::

## Understand the compiler's key constraint: optimized code must have identical behavior

A key requirement for the compiler's optimization passes is that your code continues to behave the same way, even as it gets faster.
**To an outside observer, the results of running optimized code must be indistinguishable from running the original code.**

Sometimes the compiler won't apply seemingly obvious optimizations because the optimization might change your code's behavior; when in doubt, the compiler will err on the side of conservatism.
In the following chapters we'll consider some of the reasons the compiler might fail to optimize your code, and what you can do about it.
