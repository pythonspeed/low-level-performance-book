# Rely on the compiler to optimize your code

A compiler translates your code—structured text that is intended to be read and written by humans—to machine code, the instructions the CPU understands.
But the compiler does more than that.
When your code gets compiled, the compiler also runs it through a series of optimization passes, with each pass transforming the code in the some way.
The hope is that the cumulative series of transformations will result in code that is more efficient by:

1. Removing redundant code execution, our focus in this chapter.
2. Utilizing the CPU as efficiently as possible.
   We won't consider these optimizations here, but we'll cover at least some of them in later chapters.

```{python}
# Disable automatic usage of SIMD so it doesn't distort or hide certain
# effects; we'll talk about SIMD in later chapters.
import os
os.environ["NUMBA_LOOP_VECTORIZE"] = "0"

from numba import jit
import numpy as np
```

```{python}
#| echo: false
%load_ext book_magics
```

## Get a sense of what the compiler can do

In later chapters we'll consider some of the reasons the compiler might fail to optimize your code, and what you can do about it.
But before seeing the reasons compilations won't work, it's worth understanding some of what the compiler can do.

### Hoisting expressions out of inner loops

Consider these two functions, that do the same calculation in different ways; for now we're sticking to normal Python:

```{python}
def python_unhoisted(m, n):
    total = 0
    for i in range(m):
        total += m * ((n ** 2) * (1.5 / n) + n)
    return total

@jit
def python_manually_hoisted(m, n):
    total = 0
    constant = (n ** 2) * (1.5 / n) + n
    for i in range(m):
        total += m * constant
    return total

# The results are the same:
assert python_unhoisted(100, 17) == python_manually_hoisted(100, 17)
```

The expression `((n ** 2) * (1.5 / n) + n)` in the `python_unhoisted()` function is repeated in every iteration, but it always give the same result.
Unfortunately, this version of Python isn't smart enough to take advantage of that, so `python_unhoisted()` does a lot more work:

```{python}
#| echo: false
%%compare_timing

python_unhoisted(1_000, 3)
python_manually_hoisted(1_000, 3)
```

Now let's switch to compiled code:

```{python}
@jit
def compiler_will_hoist(m, n):
    total = 0
    for i in range(m):
        total += m * ((n ** 2) * (1.5 / n) + n)
    return total

@jit
def manually_hoisted(m, n):
    total = 0
    constant = (n ** 2) * (1.5 / n) + n
    for i in range(m):
        total += m * constant
    return total

# The results are the same:
assert compiler_will_hoist(100, 17) == manually_hoisted(100, 17)
```

The compiler is smart enough to notice that `((n ** 2) * (1.5 / n) + n)` is a repeated expression whose result doesn't change.
It is therefore able to move it ("hoist" it) out of the loop in `compiler_will_hoist()` and run it only once, as we do manually in `manually_hoisted()`.
Both functions therefore run at around the same speed:

```{python}
#| echo: false
%%compare_timing

compiler_will_hoist(1_000_000, 3)
manually_hoisted(1_000_000, 3)
```

While both versions have the same speed, the manually hoisted version is arguably clearer about what is going on in the calculation, and the compiler won't always be able to hoist expressions, so doing it manually might still be worth your time.

### Inlining

Calling a function takes some time, even in a low-level language.
In addition, it's more difficult for the compiler to optimize across function boundaries.
Thus, the compiler will sometimes take the contents of a function's code and put them inside the calling function, so that it can do more optimization passes.
This is known as "function inlining".

```{python}
@jit
def in_to_cm(cm):
    return cm * 2.54

@jit
def cm3_to_liters(cm3):
    return cm3 / 1000

# The calculation is split across multiple different functions. However, we can
# expect the compiler to automatically combine them into a single function, so
# there's no additional overhead from those function calls.
@jit
def volume_liters(width_in, height_in, depth_in):
    total = 0
    for i in range(1, 10_000):
        width_cm = in_to_cm(width_in + i)
        height_cm = in_to_cm(height_in + i)
        depth_cm = in_to_cm(depth_in + i)
        total += cm3_to_liters(width_cm * height_cm * depth_cm)
    return total

# All calculations are manually unified into a single function:
@jit
def volume_liters_manual_inlining(width_in, height_in, depth_in):
    total = 0
    for i in range(1, 10_000):
        width_cm = (width_in + i) * 2.54
        height_cm = (height_in + i) * 2.54
        depth_cm = (depth_in + i) * 2.54
        total += (width_cm * height_cm * depth_cm) / 1000
    return total

assert (
    volume_liters(17.0, 23.0, 52.0) ==
    volume_liters_manual_inlining(17.0, 23.0, 52.0)
)
```

By relying on inlining, we can choose to break up our code across multiple functions without having to worry about performance implications.

```{python}
#| echo: false
%%compare_timing

volume_liters(17.0, 23.0, 52.0)
volume_liters_manual_inlining(17.0, 23.0, 52.0)
```
How do we know whether the function will be inlined?
By default, compilers will use compiler and language-specific heuristics to decide whether or not to inline, often tied to function size.
Function as trivial as `in_to_cm()` and `cm3_to_liters()` within the same file are likely to be inlined in pretty much any compiler.
To override the compiler's heuristics, most compiled languages allow you to add hints to functions, telling the compiler that it should (or shouldn't) inline that function.[^numba]

[^numba]: Numba has an inlining hint, but it's not relevant for performance optimization, as it's intended for a different part of the compilation process.

### Replacing generic implementations with faster, specialized implementations

For any given piece of code there are different ways the compiler could translate the code into CPU instructions.
If you specify the work to be done in a more constrained way, this gives the compiler opportunities to switch to a more specialized but faster implementation.
For example, let's say we want to divide numbers by 8; we can do it two ways:

```{python}
# This needs to use generic division code, because maybe you'll want to divide
# by 950124, or 237, or 3; the compiler can't know in advance.
@jit
def generic_division(divisor, size):
    arr = np.empty((size,), dtype=np.uint64)
    for i in range(len(arr)):
        arr[i] = i // divisor
    return arr

# This can generate division specifically specialized for the number 8; because
# it's a power of 2, that can be done very efficiently.
@jit
def specific_division_by_8(size):
    arr = np.empty((size,), dtype=np.uint64)
    for i in range(len(arr)):
        arr[i] = i // 8
    return arr

assert np.array_equal(
    specific_division_by_8(1000),
    generic_division(8, 1000)
)
```

If the compiler is asked to do a generic division, it needs to be able to handle any possible number you give it.
But if the compiler knows you're dividing by a specific number, it can choose to use faster operations than division to get the same result.
For powers of 2, at least, the compiler can use very efficient alternatives to a generic implementation of division:

```{python}
#| echo: false
%%compare_timing
generic_division(8, 100_000)
specific_division_by_8(100_000)
```

::: {.callout-note}
For Numba, you can see the underlying compiler's optimized output—specifically, the LLVM intermediate representation—by setting this option before you import `numba` for the first time:

```{python}
#| eval: false
os.environ["NUMBA_DUMP_OPTIMIZED"] = "1"
```

You can then search for `OPTIMIZED DUMP your_functions_name` in the output (replace `your_functions_name` with your function's name as relevant).
:::
