# Rely on the compiler to optimize your code

A compiler translates your code—structured text that is intended to be read and written by humans—to machine code, the instructions the CPU understands.
But the compiler does more than that.
When your code gets compiled, the compiler also runs it through a series of optimization passes, with each pass transforming the code in the some way.
The hope is that the cumulative series of transformations will result in code that is more efficient by:

1. Removing redundant code execution, our focus in this chapter.
2. Utilizing the CPU as efficiently as possible.
   We won't consider these optimizations here, but we'll cover at least some of them in later chapters.

```{python}
# Disable automatic usage of SIMD so it doesn't distort or hide certain
# effects; we'll talk about SIMD in later chapters.
import os
os.environ["NUMBA_LOOP_VECTORIZE"] = "0"

from numba import jit
import numpy as np
```

```{python}
#| echo: false
%load_ext book_magics
```

## Get a sense of what the compiler can do

In later chapters we'll consider some of the reasons the compiler might fail to optimize your code, and what you can do about it.
But before seeing the reasons compilations won't work, it's worth understanding some of what the compiler can do.

### Hoisting expressions out of inner loops

The compiler can move or "hoist" repetitive expressions out of loops, so they're called once instead of many times.
Consider these two functions, that do the same calculation in different ways:

```{python}
@jit
def compiler_will_hoist(m, n):
    total = 0
    for i in range(m):
        total = m * ((n ** 2) * (1.5 / n) + n)
    return total

@jit
def manually_hoisted(m, n):
    total = 0
    constant = (n ** 2) * (1.5 / n) + n
    for i in range(m):
        total = m * constant
    return total

# The results are the same:
assert compiler_will_hoist(100, 17) == manually_hoisted(100, 17)
```

The expression `((n ** 2) * (1.5 / n) + n)` in the `compiler_will_hoist()` function is repeated in every iteration, but it always give the same result.
The compiler is therefore able to move it out of the loop and run it only once, as we do manually in `manually_hoisted()`.
Both functions therefore run at the same speed:

```{python}
#| echo: false
%%compare_timing

compiler_will_hoist(1_000_000, 3)
manually_hoisted(1_000_000, 3)
```

While both versions have the same speed, the manually hoisted version is arguably clearer about what is going on in the calculation, and the compiler won't always be able to hoist expressions, so doing it manually might still be worth your time.

### Inlining

Calling a function takes some time, even in a low-level language.
Plus, it's more difficult for the compiler to optimize across function boundaries.
Thus, the compiler will sometimes take the contents of a function's code and put them inside the calling function, so that it can do more optimization passes.
This is known as "function inlining".

```{python}
@jit
def in_to_cm(cm):
    return cm * 2.54

@jit
def cm3_to_liters(cm3):
    return cm3 / 1000

# The calculation is split across multiple different functions:
@jit
def volume_liters(width_in, height_in, depth_in):
    width_cm = in_to_cm(width_in)
    height_cm = in_to_cm(height_in)
    depth_cm = in_to_cm(depth_in)
    return cm3_to_liters(width_cm * height_cm * depth_cm)

# All calculations are unified into a single function:
@jit
def volume_liters_manual_inlining(width_in, height_in, depth_in):
    width_cm = width_in * 2.54
    height_cm = height_in * 2.54
    depth_cm = depth_in * 2.54
    return (width_cm * height_cm * depth_cm) / 1000

assert (
    volume_liters(17.0, 23.0, 52.0) ==
    volume_liters_manual_inlining(17.0, 23.0, 52.0)
)
```

By relying on inlining, we can choose to break up our code across multiple functions without having to worry about performance implications.

How do we know whether the function will be inlined?
By default, compilers will use compiler and language-specific heuristics to decide whether or not to inline, often tied to function size.
A function this trivial within the same file is likely to be inlined in pretty much any compiler.
To override the compiler's heuristics, most compiled languages allow you to give hints to the compiler indicating that it should (or shouldn't) inline a function.[^numba]

[^numba]: Numba has an inlining hint, but it's not relevant for performance optimization, as it's intended for a different part of the compilation process.

### Using faster CPU instructions

For any given piece of code there are different ways the compiler could translate the code into CPU instructions.
This is relevant because some CPU instructions are faster than others.
For example, division is a complex operation, so in situations where the compiler can avoid using it, it will try to use other CPU instructions.

One such useful instruction is bitshifting: a bitshift of 3 to the right will change 97 to 12, as it's shifting the bits of `1100001` three to the right, resulting in `1100`.
Put another way, bitshifting N to the right is equivalent to dividing by $2^N$.

```{python}
assert 97 >> 3 == 12
assert 97 // (2 ** 3) == 12
```

We can compare the speed up division and bitshifting using the following code:

```{python}
@jit
def generic_division(divisor, size):
    arr = np.empty((size,), dtype=np.uint64)
    for i in range(len(arr)):
        arr[i] = i // divisor
    return arr

@jit
def generic_bitshift(bits, size):
    arr = np.empty((size,), dtype=np.uint64)
    for i in range(len(arr)):
        arr[i] = i >> bits
    return arr

assert np.array_equal(
    generic_division(8, 1000),
    generic_bitshift(3, 1000)
)
```

```{python}
#| echo: false
%%compare_timing
generic_division(8, 100_000)
generic_bitshift(3, 100_000)
```

If the compiler is asked to do a generic division, it can't hardcode a faster implementation that's specific to powers of 2.
After all, you might call that function with some other number.

But if the compiler knows you're dividing by a power of 2, it can choose to use faster operations than division to get the same result.
To see whether bitshifts were actually used we'd have to look at the generated CPU instructions, but even without doing that we can see that in this case the compiler was able to generate faster code to get the _equivalent_ of division by 8:

```{python}
@jit
def specific_division_by_8(size):
    arr = np.empty((size,), dtype=np.uint64)
    for i in range(len(arr)):
        arr[i] = i // 8
    return arr

assert np.array_equal(
    specific_division_by_8(1000),
    generic_division(8, 1000)
)
```

```{python}
#| echo: false
%%compare_timing
generic_division(8, 100_000)
specific_division_by_8(100_000)
```

::: {.callout-note}
For Numba, you can see the underlying compiler's optimized output—specifically, the LLVM intermediate representation—by setting this option before you import `numba` for the first time:

```{python}
#| eval: false
os.environ["NUMBA_DUMP_OPTIMIZED"] = "1"
```

You can then search for `OPTIMIZED DUMP your_functions_name` in the output (replace `your_functions_name` with your function's name as relevant).
:::

## The compiler ensures optimized code has identical behavior to the original code

The compiler can optimize your code, but there are limitations.
One key requirement for the compiler's optimization passes is that your code continues to have the _exact_ same behavior.
**To an outside observer, the results of running optimized code must be indistinguishable from running the original code.**

Sometimes the compiler won't apply seemingly obvious optimizations because the optimization might change your code's behavior; when in doubt, the compiler will err on the side of safer, more conservative changes.
We'll see one situation that limits the compiler in our next chapter, and discuss some others when we talk about floating point number.
