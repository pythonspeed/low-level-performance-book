# The compiler will optimize your code

The compiler doesn't just translate your code to machine code: it also optimizes the code as it goes.
In this chapter the optimizations we're focusing on are those you could do yourself, following the principle of avoiding wasted work.
Having the compiler do them saves you the effort, and the compiler will often apply optimizations you wouldn't have thought of.

```{python}
# Disable automatic usage of SIMD so it doesn't distort or hide certain
# effects; we'll talk about SIMD in later chapters.
import os
os.environ["NUMBA_LOOP_VECTORIZE"] = "0"

from numba import njit
import numpy as np
```

```{python}
#| echo: false
%load_ext book_magics
```

## Compiler optimizations and their limitations

When your code gets compiled, the compiler runs it through a series of optimization passes, with each pass transforming the code in the some way.
The hope is that the cumulative series of transformations will result in code that is more efficient by:

1. Removing redundant code execution, our focus in this chapter.
2. Utilizing the CPU as efficiently as possible.
   We won't consider these optimizations here, but we'll cover at least some of them in later chapters.

## Some examples of what the compiler can do

Even with this constraint mind, there is still plenty the compiler can do to speed up your code.

#### Precalculating mathematical expressions

If you have a fixed mathematical expression, the compiler can often just execute it at compilation time and replace the expression with the result.
For example, notice how the first function is slower, while the latter two take the same amount of time:

```{python}
@njit
def calc(a, b, c, d, e, f):
    return ((a + b + c) ** d / e) + f

@njit
def calc_fixed():
    return ((1 + 2 + 3) ** 7 / 6.0) + 5

@njit
def just_the_result():
    return 46661.0

# All three functions give the same result:
assert calc_fixed() == calc(1, 2, 3, 7, 6.0, 5)
assert calc_fixed() == just_the_result()
```

Here's how their speed compares:

```{python}
#| echo: false
%%compare_timing

calc(1, 2, 3, 7, 6.0, 5)
calc_fixed()
just_the_result()
```

#### Hoisting expressions out of inner loops

The compiler can also hoist repetitive expressions out of loops, so they're called once instead of many times.
For example, the expression `((n ** 2) * (1.5 / n) + n)` in the `compiler_will_hoist()` is repeated in every iteration, but it also always give the same result.
The compiler is therefore able to move it out of the loop and run it only once, as we do manually in `manually_hoisted()`.
Thus both functions take the same amount of time to run:

```{python}
@njit
def compiler_will_hoist(m, n):
    total = 0
    for i in range(m):
        total = m * ((n ** 2) * (1.5 / n) + n)
    return total

@njit
def manually_hoisted(m, n):
    total = 0
    constant = (n ** 2) * (1.5 / n) + n
    for i in range(m):
        total = m * constant
    return total

# The results are the same:
assert compiler_will_hoist(100, 17) == manually_hoisted(100, 17)
```

Notice they run at the same speed, because the compiler automatically optimizes the first version by hoisting the constant expression:

```{python}
#| echo: false
%%compare_timing

compiler_will_hoist(1_000_000, 3)
manually_hoisted(1_000_000, 3)
```

While both versions have the same speed, the manually hoisted version is arguably clearer about what is going on in the calculation, and the compiler won't always be able to hoist expressions, so doing it manually might still be worth it.

#### Inlining

TODO

Calling a function has overhead, even in a low-level language.
And perhaps more importantly, it's difficult for the compiler to optimize across function boundaries: combining different parts 

There are many other optimizations the compiler can do, with more added or tweaked with every compiler release.

## What the compiler won't do

A key requirement for the compiler's optimization passes is that your code continues behave the same way, even as it gets faster.
**To an outside observer, the results of running optimized code must be indistinguishable from running the original code.**

As a result, even if you had a perfect compiler that could identify every single optimization possibility, there would still be optimizations that couldn't be applied.
Only you, the author of the code, can know whether or not it is acceptable to make a change to the code's results and side-effects.

Sometimes the compiler won't apply seemingly obvious optimizations because the optimization might change your code's behavior; when in doubt, the compiler will err on the side of conservatism.
In the following chapters we'll consider some of the reasons the compiler might fail to optimize your code, and what you can do about it.

## Beware undefined behavior

The guarantee of identical behavior only applies to code that the compiler considers to be well-defined.
In Rust, so long as you don't use the `unsafe` keyword, all your code has well-defined behavior, so you don't have to worry about this.
In other languages, and especially C, there are _many_ ways to write code that the compiler will happily compile, but which does something the compiler considers impossible.

When your code is doing something he compiler considers impossible, the compiler's assumptions when optimizing might be very wrong.
The result will be completely unexpected behavior by the compiled program, from crashes to corrupted data, or worse.

TODO example
