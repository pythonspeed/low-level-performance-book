# Avoid aliasing to enable compiler optimizations

To understand the problem we'll cover in this chapter, we first need to cover a couple of definitions:

* A "pointer" is a piece of data that points at a memory address.
  For example, in Numba every NumPy array or view has a pointer that points at the location in memory where the array's underlying data is kept.
* Aliasing means having multiple pointers referencing the same memory address at the same time.

When you slice a NumPy array, for example, by default the data doesn't get copied.
Instead, you get a view object that has a pointer that points at the same memory as the original array.
Now you have two pointers to the same memory address: one from the original array, and one from the view.

```{python}
#| echo: false
%load_ext book_magics
```

```{python}
import numpy as np
from numba import jit

arr = np.array([1, 2, 3], dtype=np.int64)
print("Original array", arr)
# This is a view to the original array:
arr_view = arr[:2]
print("Original view", arr_view)
# If we mutate the original array...
arr[0] = 5
print("Updated array", arr)
# ...the view sees the same information:
print("Updated view", arr_view)
```

Here's the problem with aliasing: the _possibility_ that aliasing might occur can limit what the compiler is able to do to speed up your code.
That's because seemingly identically-behaved functions will actually behave differently when inputs are aliased.

## An example of the problem

Consider the following function:

```{python}
@jit
def minmax(mins_out, maxs_out, a, b):
    for i in range(len(a)):
        mins_out[i] = min(np.sin(a[i]), np.sin(b[i]))
        maxs_out[i] = max(np.sin(a[i]), np.sin(b[i]))
    return mins_out, maxs_out
```

`minmax()` has an obvious optimization: calculate `np.sin(a[i])` and `np.sin(b[i])` only once.
An optimized version looks like this:

```{python}
@jit
def minmax_optimized(mins_out, maxs_out, a, b):
    for i in range(len(a)):
        val_a, val_b = a[i], b[i]
        mins_out[i] = min(np.sin(val_a), np.sin(val_b))
        maxs_out[i] = max(np.sin(val_a), np.sin(val_b))
    return mins_out, maxs_out

def empty():
    return np.empty((1_000_000, ), dtype=np.float64)

DATA1 = np.random.random(1_000_000)
DATA2 = np.random.random(1_000_000)

# Both functions give the same result (in this case at least):
assert np.array_equal(
    minmax(empty(), empty(), DATA1, DATA2),
    minmax_optimized(empty(), empty(), DATA1, DATA2),
)
```

Despite this optimization being obvious to a human reader, the original `minmax()` is much slower than the optimized version.

```{python}
#| echo: false
%%compare_timing

minmax(empty(), empty(), DATA1, DATA2)
minmax_optimized(empty(), empty(), DATA1, DATA2)
```

## Aliasing can result in unexpected behavior

Why doesn't `minmax()` get automatically optimized to match `minmax_optimized()`?
Because the two functions are not truly identical: they behave differently for inputs that involve aliasing.
For example, if we pass the same array to both arguments of the functions, we get different results:

```{python}
arr = np.ones((3, ), dtype=np.float64)
arr2 = np.zeros((3, ), dtype=np.float64)
print("minmax()[1] gives:", minmax(arr, arr2, arr, arr2)[1])

arr = np.ones((3, ), dtype=np.float64)
arr2 = np.zeros((3, ), dtype=np.float64)
print(
    "minmax_optimized()[1] gives:",
    minmax_optimized(arr, arr2, arr, arr2)[1]
)
```

Here's what happens when we run the code in `minmax(arr, arr2, arr, arr2)`:

```python
# This code...
mins_out[i] = min(np.sin(a[i]), np.sin(b[i]))
maxs_out[i] = max(np.sin(a[i]), np.sin(b[i]))

# .. becomes this code:
arr[i] = min(np.sin(arr[i]), np.sin(arr2[i]))
# At this point arr[i] has been modified, so np.sin(arr[i]) is different than
# the previous np.sin(arr[i]):
arr2[i] = max(np.sin(arr[i]), np.sin(arr2[i]))
```

Because `minmax()` and `minmax_optimized()` behave differently, the compiler cannot optimize one into the other.
And that means slower code.

The obstacle to optimization in this particular case is the potential for aliasing.
How can we avoid that?

## Solution #1: Manual optimization

Rather than relying on the compiler, you can implement the optimizations yourself.
In our example, you could write the verison of the code in `minmax_optimized()` instead of `minmax()`, assuming that matches the semantics you want in the case when the data is aliased.

Unfortunately, there may be less obvious optimizations that you miss.
And some optimizations are much more difficult for you to implement yourself, in particular those that involve generating particular machine code.

## Solution #2: Ensure aliasing doesn't impact the result

Just because two pointers point at the same memory doesn't necessarily prevent the compiler from applying optimizations.
In this case, the issue is that writing to `mins_out[i]` had a chance of affecting the `a[i]` or `b[i]`.
If we prevent that, and only that, the compiler can optimize more on its own:

```{python}
@jit
def minmax_alias_safe(mins_out, maxs_out, a, b):
    for i in range(len(a)):
        val_a, val_b = a[i], b[i]
        # We're still calculating the same sin() twice, but this time the
        # compiler can optimize the duplicate calculation away, since it knows
        # that writing to mins_out[i] won't impact the calculation:
        mins_out[i] = min(np.sin(val_a), np.sin(val_b))
        maxs_out[i] = max(np.sin(val_a), np.sin(val_b))
    return mins_out, maxs_out

assert np.array_equal(
    minmax_optimized(empty(), empty(), DATA1, DATA2),
    minmax_alias_safe(empty(), empty(), DATA1, DATA2)
)
```

The compiler is now able to optimize away that duplicate `np.sin()` calculations:

```{python}
#| echo: false
%%compare_timing
minmax_optimized(empty(), empty(), DATA1, DATA2)
minmax_alias_safe(empty(), empty(), DATA1, DATA2)
```

## Solution #3: Take advantage of language features

Your programming language may have a way to indicate that aliasing is not an issue.
For example:

* In C, you can use the `restrict` keyword on function arguments to manually indicate that you have enforced a uniqueness constraint.
* The most commonly-used C++ compilers support [a similar language extension](https://en.wikipedia.org/wiki/Restrict#Support_by_C++_compilers).
* In Rust, if you have a writable reference to data, you cannot simultaneously have any other references to it; this is enforced by the compiler.
  The Rust compiler is therefore able to optimize code without having to worry about aliasing.
  When dealing with external data like Python objects, Rust libraries will enforce this at runtime insofar as they are able to.
  See for example [the relevant documentation for the Rust NumPy integration](https://docs.rs/numpy/latest/numpy/borrow/index.html).

Numba is considering adding a solution similar to C's, but at the time of writing it has not yet been merged so I am unable to demonstrate it.
