# Reduce type conversions in your code

One key difference between normal Python and compiled languages is that the latter can take advantage of the types of variables.
Adding two integers uses a different CPU instruction than adding two floats, so the compiler will need to generate different code depending on the type of the variable.

As a result, type conversions (implicit or explicit) can both force the compiler to generate more code, or prevent it from applying certain optimizations.
You can sometimes help the compiler by using explicit type conversions, or choosing better types.

Let's look at an example; we have a microscope image of a cell:

```{python}
#| echo: false
%load_ext book_magics
```
```{python}
import numpy as np
from numba import jit
from skimage import io
from skimage.data import cell

IMAGE = cell()
assert IMAGE.dtype == np.uint8
```

```{python}
#| echo: false
%display_image IMAGE
```

We want to threshold the image, separating the image into two parts, the bright part and the dark part.
This is a useful operation to find the shapes of objects.
A naive algorithm for thresholding is comparing each pixel to the mean of the image.
Here's how we'd do it with NumPy:

```{python}
def mean_threshold_numpy(image):
    threshold = image.mean()
    result = (image > threshold).astype(np.uint8)
    result *= 255
    return result

THRESHOLD_NUMPY = mean_threshold_numpy(IMAGE)
```

And here's what the result looks like:

```{python}
#| echo: false
%display_image THRESHOLD_NUMPY
```

Our goal is to make this code faster.
As a first pass, we can write an implementation using Numba.
Because we're accumulating a large number of `uint8`s to create the total used to calculate the mean, we need to store it in a larger type, so we'll use a `float64`:

```{python}
from numba import float64

@jit
def mean_threshold_1(image):
    # We'll be accumulating many uint8 values to calculate the mean, so we need
    # an accumulator that can hold larger values:
    total = float64(0.0)
    for y in range(image.shape[0]):
        for x in range(image.shape[1]):
            total += image[y, x]
    # The mean is the threshold:
    threshold = total / image.size
    # Store whether or not each value is larger than the threshold:
    result = np.empty(image.shape, dtype=np.uint8)
    for y in range(image.shape[0]):
        for x in range(image.shape[1]):
            # The >= expression becomes either 0 or 1:
            result[y, x] = (image[y, x] >= threshold) * 255
    return result

THRESHOLD_1 = mean_threshold_1(IMAGE)
# Make sure the result is basically the same, with less than 0.1% of pixels being
# different:
assert ((THRESHOLD_NUMPY ^ THRESHOLD_1) > 0).sum() < IMAGE.size / 1000
```

Unfortunately this new version isn't particularly faster:

```{python}
#| echo: false
%%compare_timing --measure=instructions
mean_threshold_numpy(IMAGE)
mean_threshold_1(IMAGE)
```

## Use integers instead of floats when possible

As we'll discuss in a later part of the book, the compiler has a harder time optimizing floating point math than it does optimizing integer math.
So what if instead of accumulating a `float64`, we accumulate a `uint64`?

```{python}
from numba import uint64, uint8

@jit
def mean_threshold_2(image):
    # 😎 Accumulate in a uint64 instead of float64:
    total = uint64(0)
    for y in range(image.shape[0]):
        for x in range(image.shape[1]):
            total += image[y, x]
    threshold = total / image.size
    result = np.empty(image.shape, dtype=np.uint8)
    for y in range(image.shape[0]):
        for x in range(image.shape[1]):
            result[y, x] = (image[y, x] >= threshold) * 255
    return result

THRESHOLD_2 = mean_threshold_2(IMAGE)
# Make sure the result is basically the same, with less than 0.01% of pixels
# being different:
assert ((THRESHOLD_NUMPY ^ THRESHOLD_2) > 0).sum() < (IMAGE.size / 10_000)
```

This version is faster!

```{python}
#| echo: false
%%compare_timing --measure=instructions
mean_threshold_1(IMAGE)
mean_threshold_2(IMAGE)
```

## Hoist type conversions out of loops

We can do better.
Notice above we're comparing the threshold, a `float64`, to the pixels, which are `uint8`.
Somewhere in there, then, the compiler is likely converting both values to the same type, but it's unclear how efficient that conversion is.
There are two ways the conversion could happen:

* `image[y, x]` is converted to a `float64`, which means doing extra work on every loop iteration.
* `threshold` is converted to a `uint8`, in which case hopefully the compiler will hoist the conversion out of the loop.

Rather than hoping that the faster choice is selected, let's just explicitly do it ourselves:

```{python}
@jit
def mean_threshold_3(image):
    total = uint64(0)
    for y in range(image.shape[0]):
        for x in range(image.shape[1]):
            total += image[y, x]
    # 😎 Convert the mean to an uint8 before we do the comparison:
    threshold = uint8(np.round(total / image.size))

    result = np.empty(image.shape, dtype=np.uint8)
    for y in range(image.shape[0]):
        for x in range(image.shape[1]):
            result[y, x] = (image[y, x] >= threshold) * 255
    return result

THRESHOLD_3 = mean_threshold_3(IMAGE)
# Make sure the result is basically the same, with less than 0.01% of pixels being
# different:
assert ((THRESHOLD_NUMPY ^ THRESHOLD_3) > 0).sum() < (IMAGE.size / 10_000)
```

This new version gives slightly different results, but not meaningfully different.
And it is significantly faster:

```{python}
#| echo: false
%%compare_timing --measure=instructions
mean_threshold_2(IMAGE)
mean_threshold_3(IMAGE)
```
