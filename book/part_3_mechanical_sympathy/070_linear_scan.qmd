# Use linear memory reads to get faster memory access

Even if your data is too large to fit in a memory cache, there are still ways to speed up access.
One way to do so is to access memory linearly, in the order it's stored in memory.
Compared to linear reads, reading memory in random order can be much slower—let's see why.

```{python}
#| echo: false
%load_ext book_magics
```

We'll implement a function that can scan memory both linearly and pseudo-randomly, depending which multiplier we pass in.
We pass the multiplier in explicitly so the compiler doesn't optimize some of the work out of existence, as it would with a constant.

```{python}
import numpy as np
from numba import jit

DATA = np.ones((100_000_000,), dtype=np.uint8)

@jit
def scan_memory(arr, multiplier):
    index = 0
    size = len(arr)
    for _ in range(1_000_000):
        arr[index] += 1
        index = (multiplier * index + 1) % size
    return arr[0]

LINEAR = 1
RANDOM = 22695477

_ = scan_memory(np.ones((10,), dtype=np.uint8), LINEAR)
```

Depending on the value of the multiplier, the memory will be traversed in different ways:

1. If the multiplier is 1 (`LINEAR`), this is a linear scan of memory.
2. If you pass in a large number like 22695477 (`RANDOM`), this is a pseudo-random traversal of the array, bouncing around in memory.

Here's the time it takes to reach each variation:

```{python}
#| echo: false
%%compare_timing --measure=instructions
scan_memory(DATA, LINEAR)
scan_memory(DATA, RANDOM)
```

In both cases we're running almost the same number of CPU instructions.
If the CPU were the only bottleneck, both variations should have taken the same amount of time because they're doing the same amount of work.

## Why linear reads are faster than random access reads

Why is a linear scan of memory faster than randomly jumping around?

### Pre-fetching

Modern CPUs will pre-fetch memory into the cache as an optimization technique, even before the running code asks for it.
When this works well, by the time you've gotten to the point of needing to read the next chunk of memory it will already be in the cache.

Linear memory scans are a common usage pattern in software, and they're also easy to detect.
So if you're doing a linear scan it's very likely the CPU will notice and prefetch the data you need into the cache.
Random scans are, of course, much harder to predict, and so prefetching is less likely to help.

If data is being prefetched, there will be fewer cache misses because the CPU will already have loaded the data into the memory caches.
And that means fewer loads from slower caches, or even slower RAM.

Let's rerun our comparison, showing the number of L1D cache misses:

```{python}
#| echo: false
%%compare_timing --measure=l1_memory_cache_miss
scan_memory(DATA, LINEAR)
scan_memory(DATA, RANDOM)
```

My computer's L1D cache is far smaller than the 1MB of data we read, yet we have almost no misses in the linear case.
That's likely because it's being filled by the CPU, automatically.

### Cache lines

Reading memory linearly can also benefit from the way data is loaded into the memory caches, and into the CPU's store of individual memory values ("registers"): in small chunks known as "cache lines", typically 64 bytes at a time.
Each of the values in the example above is an 8-bit integer, i.e. it takes 1 byte.
So if we're loading 64 bytes at a time, we'll also load the 2nd, 3rd, 4th up to the 63rd consecutive value into the L1 cache.

Comparing our two variations:

* **Linear scan:** Each load of an integer also loads the next 63 consecutive integers into a fast, local cache.
  This is helpful, since we're doing a linear scan, so we're going to be inspecting these integers next.
* **Random scan:** Each load of an integer also loads the next 63 consecutive integers, but they're of no immediate use to us since the next value we'll inspect will be elsewhere in the array.
  Quite possibly they'll be dropped from the cache before we actually need to see them.

As a result, in the linear scan case the data we need is much more likely to already be in the much-faster L1 cache.

## Scan N-dimensional arrays in a linear fashion

Whether or not you're scanning memory linearly becomes a little less obvious when you're dealing with N-dimensional arrays: the memory address space is linear, so there's different ways to map the different dimensions on.
Imagine the following array:

```
  X →
Y 1, 2, 3
↓ 4, 5, 6
```

It could be laid out in memory like this:

```
1, 2, 3, 4, 5, 6
```

Or like this:

```
1, 4, 2, 5, 3, 6
```

However it's laid out, we want to iterate over the data in a way that matches that layout, because that will be faster.
In practice, NumPy arrays by default will be laid out so the last dimension is contiguous in memory, so you want to iterate over the dimensions in their listed order, from first to last.

Let's see an example:

```{python}
#| echo: false
%load_ext book_magics
```

```{python}
import numpy as np
from numba import jit

two_d = np.ones((2_000, 2_000), dtype=np.int64)

@jit
def scan_y_first(arr):
    total = 0
    for y in range(arr.shape[0]):
        for x in range(arr.shape[1]):
            total += arr[y, x]
    return total

@jit
def scan_x_first(arr):
    total = 0
    for x in range(arr.shape[1]):
        for y in range(arr.shape[0]):
            total += arr[y, x]
    return total

assert scan_x_first(two_d) == scan_y_first(two_d)
```

Given NumPy's default memory layout, scanning the dimensions in order is faster:

```{python}
#| echo: false
%%compare_timing
scan_y_first(two_d)
scan_x_first(two_d)
```

### Next steps {.unnumbered}

In the next chapter, which you can skip on first reading, I dig deeper into NumPy array's memory layout and their impact on performance.
Or, now that you have some understanding of the Practice of Mechanical Sympathy, you can move on to [The Practice of Parallelism](../part_4/000_what_is.qmd), which allows you to take advantage of multiple CPU cores.
