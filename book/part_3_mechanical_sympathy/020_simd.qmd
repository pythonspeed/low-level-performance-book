# Use SIMD (Single Instruction, Multiple Data) to speed up repetitive computations

Single Instruction Multiple Data, or SIMD, are a type of CPU instructions that can execute the same operation on a sequence primitive values (integers or floats) that are stored next to each other in memory, using a single instruction.
The compiler needs to explicitly generate these instructions, unlike the instruction-level parallelism we previously covered.

For example, if we want to multiply 4 integers by a constant, we can do that with a single SIMD CPU instruction instead of with 4 normal CPU instructions.
Reducing the number of instructions the CPU has to run by a factor of 4 can then lead to a significant speedup.
This is sometimes referred to as "vectorization".

::: {.callout-note}
This is a different meaning than typically used in Python programming, where "vectorization" means a batch operation implemented in a low-level language.
Given we're speeding up Python, what "vectorization" means can be ambiguous, so make sure you understand which of the two meaning applies when reading documentation.
:::

```{python}
#| echo: false
%load_ext book_magics
```

## Unlike instruction-level parallelism, SIMD requires work by the compiler

In the previous chapter we discussed instruction-level parallelism, where the CPU runs multiple instructions in parallel.
And SIMD is also a way to execute work in parallel.
So what is the difference?

* The decision to use instruction-level parallelism happens at runtime, and is made by the CPU.
* The decision to use SIMD happens when the code is compiled.
  If the compiler generate specialized SIMD machine code instructions, then SIMD will be used at runtime.

Consider the following code:

```python
out[0] = a[0] + b[0]
out[1] = a[1] + b[1]
out[2] = a[2] + b[2]
out[3] = a[3] + b[3]
```

If these are arrays of 64-bit integers, and we imagine our CPU has a `add64` instruction, the compiler might turn the above code into 4 regular (non-SIMD) CPU instructions, something like:

```
add64 a[0] b[0] → out[0]
add64 a[1] b[1] → out[1]
add64 a[2] b[2] → out[2]
add64 a[3] b[3] → out[3]
```

When you run these four CPU instructions, the CPU will realize they are independent of each other and run them in parallel.
If it has the hardware to do four additions in parallel, all of the operations can run at the same time.
But _you_ don't get to tell the CPU to run those 4 instructions in parallel; if it can't or won't, there's nothing you can do.

SIMD (single instruction multiple data) is different: it involves a _single_ instruction, as its name implies, doing the same thing to multiple pieces of data.
In particular, SIMD operations run on contiguous chunks of memory of 128-, 256-, or 512-bits, depending on the model of CPU and which operation you are doing.
Different operations require the compiler to generate machine code using different specialized instructions.
For example, you might be able to add an array of 4 64-bit integers to another array of 4 64-bit integers, since 4×64 is 256.

For our code example, rather than generating 4 CPU instructions with 4 adds, the compiler can generate a single SIMD instruction, a hypothetical `simd_add256` instruction.
That single instruction will do 4 additions with a single instruction:

```
simd_add256 a[0:3] b[0:3] → out[0:3]
```

When the CPU core reaches that instruction, it will execute the instruction the machine code told it to execute: 4 additions at the same time.

## The compiler can automatically generate SIMD instructions for you ("auto-vectorization")

In many situations, the compiler can automatically convert your code to use SIMD instructions, speeding up your code (or in some cases, slowing it down!).
This is known as "auto-vectorization".
Numba, for example, will automatically generate SIMD code optimized for your specific CPU model.

The compiler will notice you are doing the same operation on a series of items, and instead of doing the work item by item it will do it in batches of 4 or 8 or 16.

We'll talk about how the batch size is decided in just a bit, but first let's see how SIMD speeds up some example code.
In particular, we have two versions of the exact same code, one compiled with SIMD (the default) and one with SIMD disabled:

```{python}
# Import the dependencies we'll need:
import numpy as np
from numba import jit

# A context manager that let us disable SIMD on the fly when defining
# functions:
from book_utilities import disabled_simd

DATA_FLOAT64 = np.ones((1_000_000,), dtype=np.float64)
DATA_FLOAT32 = np.ones((1_000_000,), dtype=np.float32)

@jit
def simd(x):
    out = np.empty(x.shape, dtype=x.dtype)
    for i in range(len(x)):
        out[i] = x[i] * 2
    return out

assert simd(DATA_FLOAT64)[0] == 2
assert simd(DATA_FLOAT32)[0] == 2

# This is like doing @jit, with some behind-the-scenes magic to disable
# usage of SIMD for this function only.
with disabled_simd() as jit_no_simd:
    @jit_no_simd
    def no_simd(x):
        out = np.empty(x.shape, dtype=x.dtype)
        for i in range(len(x)):
            out[i] = x[i] * 2
        return out

    assert no_simd(DATA_FLOAT64)[0] == 2
    assert no_simd(DATA_FLOAT32)[0] == 2
```

Here's how long each variation takes to run; using SIMD makes the code run a little faster:

```{python}
#| echo: false
%%compare_timing --measure=simd_256bit,instructions
no_simd(DATA_FLOAT32)
simd(DATA_FLOAT32)
no_simd(DATA_FLOAT64)
simd(DATA_FLOAT64)
```

## Smaller data types require fewer SIMD instructions

My CPU has 256-bit SIMD instructions.
That means the instruction operates on 256 bits at once: the code packs as many numbers as will fit into 256 bits, and then the SIMD operation runs on that.

* For 32-bit floats, $256 / 32 = 8$, so we can process 8 `float32` with each 256-bit SIMD instruction.
  $1,000,000 / 8 = 125,000$ instructions.
* For 64-bit floats, $256 / 64 = 4$, so we can process 4 `float64` with each 256-bit SIMD instruction.
  Given an array of 1,000,000 64-bit values, processing it will take $1,000,000 / 4 = 250,000$ instructions.

Notice that these numbers match what we empirically measured in the previous section!

Using half as many instructions is likely part of the reason that processing `float32`s was faster than processing the equivalent number of `float64`.
We'll talk about an additional reason, memory caches and bandwidth, in later chapters.

## Avoid code patterns that prevent auto-vectorization

Auto-vectorization doesn't always happen: sometimes the compiler can't figure out how generate equivalent SIMD code.

* **The potential for aliasing.** If you are dealing with overlapping memory, doing an operation one array item at a time might give different results than doing it in batches in parallel.
  As a result, if there is a potential for aliasing the compiler might choose not to use SIMD instructions in order to ensure the code matches your (presumed) intentions.
* **Complex functions that didn't get inlined.**
  The compiler likely won't pass 128/256/512-bit data batches across function call boundaries, so a function call can limit the ability to use SIMD.
* **Branches.** SIMD instructions do the same thing to multiple pieces of data, which means branching for each item may be difficult.
  Depending on the SIMD instructions your CPU has available, the compiler might have some tricks up its sleeve using "masked" operations, but the more branching your code has the less likely it is that auto-vectorization will work.
* **Floating point numbers.** Changing the order of floating point operations can give different calculation results, which may limit the compiler's ability to parallelize.
* **The potential for memory being non-contiguous.**
  This is particularly an issue with NumPy views, that can skip every N bytes for a variety of reasons, for example a slice across a dimension that doesn't match the data order in memory.
  Code that is given memory that may or may not be contiguous will get optimized differently than code that can assume the data is contiguous.

These issues can affect performance in additional ways, like preventing compiler optimizations and instruction-level parallelism, so we cover them in more detail in other chapters.

:::: {.callout-note}
You can usually get your compiler to give you some hints about why it failed to auto-vectorize your code.
For example, with Numba you can run the following _before_ you import `numba` for the first time:

```{python}
#| eval: false
import llvmlite.binding as llvm
llvm.set_option('', '--debug-only=loop-vectorize')
```
::::

## Don't assume auto-vectorized code is always faster

Unfortunately, not all code runs faster when auto-vectorized; in some cases it will be slower, depending on the inputs, the compiler version, and your specific CPU model.
So run code with and without auto-vectorization and see which version is faster.

::: {.callout-note}
On Numba you can disable auto-vectorization by setting the [environment variables](https://numba.readthedocs.io/en/stable/reference/envvars.html) `NUMBA_LOOP_VECTORIZE` to `0` and `NUMBA_SLP_VECTORIZE` to `0`.
Other compilers have their own specific options.
:::
