# Reduce data size so it fits in CPU memory caches

> TODO Expand to cover swapping as well.
> TODO Brief mention of chunking, indexing, and compression as techniques?

So far we've been mostly ignoring memory reads and writes, implicitly assuming that they're fast operations, and take a consistent amount of time to run.
Neither assumption is completely true, so a more accurate mental model can help you speed up our code.

As it turns out, from a CPU's perspective, reading from RAM—the active memory of the computer—is actually quite slow.
To speed things up, CPU designers provide a series of caches to reduce the need to access RAM.
Recently used data is stored in those caches, and that speeds up memory access—for data that is already in the cache.
If the data you need isn't in a fast cache, you might end up with a slow memory read from RAM.

That means memory access is sometimes fast, and sometimes slow.
You can make your code run faster, or at least more consistently, by keeping these caches in mind when you design your data structures and data access patterns.

## Visualizing the memory cache hierarchy

To simplify somewhat, here's how the CPU talks to RAM:

::: {.content-hidden when-format="markdown"}
```{dot}
digraph G {
  rankdir = "TD"
  node [shape = "box"]
  l1i [label="L1 cache (instructions)"]
  l1d [label="L1 cache (data)"]
  l2 [label="L2 cache"]
  l3 [label="L3 cache"]
  CPU -> l1i
  CPU -> l1d
  l1i -> l2
  l1d -> l2
  l2 -> l3
  l3 -> RAM
}
```
:::

::: {.content-hidden unless-format="markdown"}
```
     /--> L1 cache (instructions)
    /                            \
CPU                               --> L2 cache --> L3 cache --> RAM
    \                            /
     \--> L1 cache (data) ------/
```
:::

RAM is the slowest (a reference is around 100ns), L3 is a bit faster, L2 more so, and L1 is the fastest (a reference is <1ns).
Typically L1 is per CPU core, L2 may or may not be per core, and L3 is shared across cores.
There are actually two L1 caches, one for instructions, i.e. the code you're running, and one for the data your program is using.

## How large are the CPU's memory caches?

Given data size can impact performance results, it's useful to know how memory caches are organized, and their general sizes.

To give a sense of scale, the i7-12700 has 12 physical CPU cores; 8 "performance" (faster) and 4 "efficiency" (slower) cores.
The 8 performance CPU cores in my i7-12700K have:

1. 48KiB L1 data cache and 32KiB L1 instruction cache per core.
2. 1280KiB L2 cache for each core; this includes both data and instructions.
3. 25MiB L3 cache shared by all the cores, including both the performance cores and the 4 efficiency cores.

::: {.callout-note}
The `lstopo` command included in the [`hwloc`](https://www.open-mpi.org/projects/hwloc/) package can draw nice diagrams of your CPU's memory cache configuration.
:::

## More data means slower access

The CPU's memory caches are faster than RAM, but can also store less data.
So the smaller the data, the more likely you are to be able to use the smaller, faster caches.
But if your data gets larger, your process will end up using slower caches, or even RAM.
And that will impact how fast your code runs.

Let's consider an example: we're going to pick 100,000 samples from an array, with repeats allowed.

```{python}
#| echo: false
%load_ext book_magics
```

```{python}
import numpy as np
from numba import jit

GENERATOR = np.random.default_rng(0)

@jit
def random_sample(random_gen, arr):
    result = np.empty((100_000,), dtype=arr.dtype)
    for i in range(100_000):
        chosen_index = random_gen.integers(0, len(arr))
        result[i] = arr[chosen_index]
    return result

# Pre-compile Numba code:
random_sample(GENERATOR, np.array([1, 2], dtype=np.uint8))

SMALL_DATA = np.arange(0, 1_000, dtype=np.uint8)
LARGE_DATA = np.arange(0, 100_000_000, dtype=np.uint8)
```

Since we're doing the same amount of computational work regardless of array size, one might expect the run time to be the same for both `SMALL_DATA` and `LARGE_DATA`.
In fact:

```{python}
#| echo: false
%%compare_timing --measure=instructions
random_sample(GENERATOR, SMALL_DATA)
random_sample(GENERATOR, LARGE_DATA)
```

Running the function on a large array takes 1.5× as long, even though the number of CPU instructions is virtually identical.
What's going on?

In the case of `SMALL_DATA`, the data fits in the CPU's memory caches, so repeated access is fast.
`LARGE_DATA` however is too large to fit in the caches, so it needs to be read from RAM, which is slower.

This leads to two conclusions:

* **If data fits into the memory cache, our code will run faster.**
  Shrinking how much data we access can speed up our code.
* **It's important to benchmark code with realistic amounts of data.**
  If real-world data is 100MB, and our test data is 1KB, the benchmark may will be unrealistically fast.

## Pick smaller integer data types when possible

One way to reduce memory usage is to use smaller data types.

Unlike Python, which only has one integer and one float data type, compiled languages have a range of options.
For integers there are 8-bit, 16-bit, 32-bit, and 64-bit integers (and sometimes 128-bit), and each comes in signed and unsigned versions.
Signed types can support negative numbers, whereas unsigned types only support positive numbers.

For example:

* 8-bit signed integers range from -128 to 127.
* 16-bit unsigned integers range from 0 to 65,535.

Which one should you use?
At minimum, you need to fit your data into the data type.
If you need to store `123001`, you can't use 8-bit or 16-bit integers: the number simply won't fit.
So one easy approach is to always use 64-bit integers.

However, when storing large numbers of values, this results in higher memory usage:

```{python}
print("How much memory (in bytes) is used by an array with 1,000,000 values?")
print("int8: ", np.ones((1_000_000), dtype=np.int8).nbytes)
print("int16:", np.ones((1_000_000), dtype=np.int16).nbytes)
print("int32:", np.ones((1_000_000), dtype=np.int32).nbytes)
print("int64:", np.ones((1_000_000), dtype=np.int64).nbytes)
```

So if your data safely fits into a smaller data type, the reduction in memory can speed up your code if it enables more efficient memory cache usage.
For example, given an `int64` and an `int8` array with the same number of values:

```{python}
INT64 = np.ones((1_000_000,), dtype=np.int64)
INT8 = np.ones((1_000_000,), dtype=np.int8)

# Pre-compile code for Numba:
example1 = random_sample(GENERATOR, INT64)
example2 = random_sample(GENERATOR, INT8)
```

Running `random_sample()` on the `int8` array will be faster, because the data fits in the smaller but faster caches:

```{python}
#| echo: false
%%compare_timing --measure=instructions
random_sample(GENERATOR, INT64)
random_sample(GENERATOR, INT8)
```
