# Remove data dependencies to enable instruction-level parallelism

Choosing more scalable algorithms and removing redundant work don't require a particularly sophisticated mental model of how CPUs or compilers work.
But if you're going to write faster code, you need a more realistic model of CPUs work.

Let's do our initial setup, and then learn about:

1. Instruction-level parallelism (ILP).
2. Branch prediction, and the cost of misprediction.
3. How to take advantage of these CPU features.

In order to better demonstrate the effects of these CPU features, we will disable SIMD code generation in the code examples in this chapter; we'll talk about what SIMD means and what it does in the next chapter.

```{python}
# Disable SIMD, so it doesn't hide other effects:
import os
os.environ["NUMBA_LOOP_VECTORIZE"] = "0"
os.environ["NUMBA_SLP_VECTORIZE"] = "0"

# Import the dependencies we'll need:
import numpy as np
from numba import njit
```

```{python}
#| echo: false
%load_ext book_magics
```

## Faster results with instruction-level parallelism

A reasonable minimal mental model of CPUs is that they execute one instruction at a time.
Putting aside parallelism between multiple CPU cores, which results in some very complex interactions that we won't be getting into, from the outside the CPU will indeed operate _as if_ it were executing one instruction at a time, in order.

Consider the following function.
Our expectation is that if we call `pythagorean_theorem(3, 4)`, we will get 5.
We don't care how the CPU executes the resulting code: so long as we get the correct result, faster is better.

```{python}
#| eval: false
from math import sqrt

@njit
def pythagorean_theorem(x_length, y_length):
    x_squared = x_length ** 2
    y_squared = y_length ** 2
    return sqrt(x_squared + y_squared)
```

A simplistic CPU will execute each instruction in order.
First a multiply, then another, then an addition, the calculation of the square root:

```{mermaid}
flowchart TD
 A("x_squared = x_length ** 2") --> B("y_squared = y_length ** 2")
 B --> C("__temp = x_squared + y_squared")
 C --> D("result = sqrt(__temp)")
```

Modern CPUs, like those used in laptops, desktops, and servers, can run your code faster by running multiple instructions in parallelâ€”so long as that won't affect the result.
In this case, calculating `x_squared` and `y_squared` is completely independent, so your CPU is likely to run both at once if it can.
This happens transparently: the compiler doesn't have to do anything special, the CPU will do this all on its own.

Importantly, this is distinct from any benefit you get from using multiple CPU cores with threads or multiple processes: this is parallelism within a single core.
If you later switch to a parallel implementation, each individual CPU core will still be able to do instruction-level parallelism.

```{mermaid}
flowchart TD
 A("x_squared = x_length ** 2") --> C("__temp = x_squared + y_squared")
 B("y_squared = y_length ** 2") --> C
 C --> D("result = sqrt(__temp)")
```

Not all code can be transparently run in parallel.
For one thing, a calculation can't be run if its inputs aren't yet available:

* The CPU _cannot_ run the instructions for `x_squared + y_squared` until both inputs have been calculated.
* Similarly, the `sqrt()` can't be run until `x_squared + y_squared` finishes.

We can say that these are "data dependencies" that prevent instruction-level parallelism, and therefore reduce the code's potential runtime speed.

## Example: Removing data dependencies

To see the speed impact of ILP, let's look at an example: generating an array of random numbers.
For educational purposes, we find [an example of how to do this](https://nuclear.llnl.gov/CNP/rng/rngman/node4.html) and write the following code:

```{python}
from numba import uint32, uint64

# 64-bit linear congruent generator; we'll use the top 32-bits only for the
# result to maximize the cycle length and therefore perceived randomness.
@njit("uint32(uint32)")
def lcg(seed):
    temp = uint64(seed) * 2862933555777941757 + 3037000493
    return temp >> 32

@njit
def generate_random_numbers(n):
    result = np.empty((n,), dtype=np.uint32)
    result[0] = uint32(1)
    for i in range(1, n):
        result[i] = lcg(result[i - 1])
    return result
```

To get a sense of the randomness, we can visualize the result as an image.
If we see any patterns, the data definitely isn't random (though lack of patterns isn't conclusive, of course).

```{python}
def to_image(f):
    return f(256 * 256 // 4).view(np.uint8).reshape((256, 256))

RAND1 = to_image(generate_random_numbers)
```

Our first result looks pretty good:

```{python}
#| echo: false
%display_image RAND1
```

Unfortunately, this calculation does not allow a lot of parallelism.
Imagine we asked for 4 values:

* `result[3] = lcg(result[2])`, so we need `result[2]` before proceeding.
* `result[2] = lcg(result[1])`, so we need `result[1]`.
* `result[1] = lcg(result[0])`, so we need `result[0]`.

In short, each value in the array has a data dependency on the previous value in the array; they cannot be calculated in parallel.

### Removing a calculation data dependency

Let's try a different variant, where we calculate the random number based just on `i`:

```{python}
@njit
def generate_not_so_random_numbers(n):
    result = np.empty((n,), dtype=np.uint32)
    for i in range(1, n):
        result[i] = lcg(i)
    return result

RAND2 = to_image(generate_not_so_random_numbers)
```

In this case we've broken the dependency chain.
These calculations are completely unrelated and can be calculated in parallel:

* `result[3] = lcg(3)`
* `result[2] = lcg(2)`

Unfortunately, when visualized we can see the results have an obvious pattern, so this isn't a good random number generator:

```{python}
#| echo: false
%display_image RAND2
```

However, this change does enable a massive speedup; the number of instructions is higher, but the speed at which they run drops dramatically thanks to ILP:

```{python}
#| echo: false
%%compare_timing --measure=instructions
generate_random_numbers(1_000_000)
generate_not_so_random_numbers(1_000_000)
```

### Interleaving to avoid a data dependency

We can preserve the pseudo-randomness, while breaking the data dependency.
Specifically, we can interleave multiple streams that don't share a data dependency.
This should allow us to do a bunch of the calculation in parallel, while still getting better results:

```{python}
@njit
def generate_random_numbers_3(n):
    result = np.empty((n,), dtype=np.uint32)

    # Make sure we have 4 sufficiently different starting points:
    result[0] = uint32(1)
    result[1] = lcg(lcg(result[0]) + 1)
    result[2] = lcg(lcg(result[1]) + 1)
    result[3] = lcg(lcg(result[2]) + 1)

    # Do 4 unrelated calculations that don't share data dependencies with each
    # other, allowing the CPU to run them in parallel:
    for i in range(1, n // 4):
        result[i * 4    ] = lcg(result[(i - 1) * 4    ])
        result[i * 4 + 1] = lcg(result[(i - 1) * 4 + 1])
        result[i * 4 + 2] = lcg(result[(i - 1) * 4 + 2])
        result[i * 4 + 3] = lcg(result[(i - 1) * 4 + 3])

    # Calculate the remaining last few values:
    for i in range(n % 4):
        result[n - (3 - i)] = lcg(result[n - (3 - i) - 1])

    return result

RAND3 = to_image(generate_random_numbers_3)
```

This is how it looks:

```{python}
#| echo: false
%display_image RAND3
```

This is faster than the first version, but still pretty slow:

```{python}
#| echo: false
%%compare_timing --measure=instructions
generate_random_numbers(1_000_000)
generate_random_numbers_3(1_000_000)
```

### Simplifying the calculation

So why is our latest version slower than our second attempt?
The fact that the number of instructions is so much higher seems like a problem.

To try to reduce the number of instructions, instead of reading and writing intermediate values from the array, we can store them in temporary variables.
In addition to doing less math, variables with numeric variables are easier for the compiler to optimize away compared to memory reads and writes:

```{python}
@njit
def generate_random_numbers_4(n):
    result = np.empty((n,), dtype=np.uint32)
    random_number1 = uint32(1)
    random_number2 = lcg(lcg(random_number1) + 1)
    random_number3 = lcg(lcg(random_number2) + 1)
    random_number4 = lcg(lcg(random_number3) + 1)

    for i in range(n // 4):
        random_number1 = lcg(random_number1)
        random_number2 = lcg(random_number2)
        random_number3 = lcg(random_number3)
        random_number4 = lcg(random_number4)
        result[i * 4    ] = random_number1
        result[i * 4 + 1] = random_number2
        result[i * 4 + 2] = random_number3
        result[i * 4 + 3] = random_number4

    for i in range(n % 4):
        result[n - (3 - i)] = lcg(result[n - (3 - i) - 1])

    return result

RAND4 = to_image(generate_random_numbers_4)
```

This is how it looks:

```{python}
#| echo: false
%display_image RAND4
```

And now our code is much faster, with approximately the same number of instructions as our original version, but with significant parallelism:

```{python}
#| echo: false
%%compare_timing --measure=instructions
generate_random_numbers(1_000_000)
generate_random_numbers_4(1_000_000)
```

## Remove accumulator data dependencies with MapReduce

Let's look at another data dependency bottleneck: an accumulator.
If we sum the values in an array, we keep adding values to the same place, `total`.
Because we keep accumulating results in the same variable, that variable acts as a data dependency, and prevents instruction-level parallelism:

```{python}
DATA = np.arange(0, 1_000_000, dtype=np.uint64)

@njit
def sum1(arr):
    total = 0
    for i in range(len(arr)):
        total += arr[i]
    return total

_ = sum1(DATA)
```

We can solve this by having two accumulators, and switching back and forth between them.
At the end of the algorithm we add the two accumulators.

```{python}
@njit
def sum2(arr):
    total1, total2 = 0, 0
    for i in range(len(arr)):
        total1 += arr[i]
        # Swap the accumulators so the next iteration adds to the other one:
        total2, total1 = total1, total2
    return total1 + total2

assert sum1(DATA) == sum2(DATA)
```

This is a variation of the MapReduce design pattern:

1. **Map:** Run multiple copies of the same algorithm on different parts of the data to get parallelism.
   In this case, the parallelism comes from CPU's ILP.
2. **Reduce:** Combine all the partial results into one final result.

And as we hoped, it allows more scope for parallelism:

```{python}
#| echo: false
%%compare_timing --measure=instructions
sum1(DATA)
sum2(DATA)
```

## TODO MOVE ME TODO Bottleneck #2: Conditionals and failed branch prediction

Another code structure that potentially prevents instruction-level parallelism is a conditional statement: an `if` or equivalent.
Given there are two possible sets of future instructions, the two branches of the `if`, how can the CPU know which set it should be executing in parallel?

Instead of just stopping execution until the `if` statement can be calculated, the CPU will instead make a guess, a "branch prediction".
Based on this prediction, the CPU keeps executing in parallel as if that prediction is correct.
If it turns out to be correct, all is well.
If the guess turns out to be wrongâ€”a branch mispredictionâ€”then the work done so far has to be undone.
And that can get expensive.

However, many conditionals have very consistent answers for long stretches of time, so branch prediction will end up being very accurate.
TODO make the below a more complete code example, explaining why loops don't prevent instruction parallelism

For example, if we're iterating over a range of numbers from `0` to `N`, the CPU keeps asking "is `i` smaller than `N`?"

* Is 0 smaller than `N`?
* Is 1 smaller than `N`?
* Is 3 smaller than `N`?
* etc.

The conditional will be true for the first `N` times; only when the iteration ends does the conditional go the other way.
Given how common this sort of loop is, you can assume the branch predictor will get things right.

Some branches are unpredictable, however, and those can significantly slow down execution.
We'll talk about how to deal with those in a later chapter.
