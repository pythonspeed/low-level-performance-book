# Remove data dependencies to enable instruction-level parallelism

Your CPU is capable of transparently running multiple instructions at the same time on the same core, making your code run that much faster... but only if your code structure enables it.
In order to better demonstrate the effects of this CPU feature, we will disable SIMD code generation in the code examples in this chapter; we'll talk about what SIMD means and what it does in the next chapter.

## Data dependencies are a bottleneck for instruction-level parallelism

Consider the following function.
Our expectation is that if we call `pythagorean_theorem(3, 4)`, we will get 5.
We don't care how the CPU executes the resulting code: so long as we get the correct result, faster is better.

```{python}
#| eval: false
from math import sqrt

@jit
def pythagorean_theorem(x_length, y_length):
    x_squared = x_length ** 2
    y_squared = y_length ** 2
    return sqrt(x_squared + y_squared)
```

In our original, simplistic mental model, we assumed the CPU core will execute each instruction in order.
First a multiply, then another multiply, then an addition, then whatever instructions are necessary to calculate the square root:

::: {.content-hidden when-format="markdown"}
```{mermaid}
flowchart TD
 A("x_squared = x_length ** 2") --> B("y_squared = y_length ** 2")
 B --> C("__temp = x_squared + y_squared")
 C --> D("... result = sqrt(__temp) ...")
```
:::

::: {.content-hidden unless-format="markdown"}
```
x_squared = x_length ** 2
          ‚Üì
y_squared = y_length ** 2
          ‚Üì
__temp = x_squared + y_squared
          ‚Üì
... result = sqrt(__temp) ...
```
:::

However, modern CPUs, like those used in laptops, desktops, servers, and smartphones, can run your code faster by running multiple instructions in parallel‚Äîso long as that won't affect the result.

Importantly, this is distinct from any benefit you get from using multiple CPU cores with threads or multiple processes.
This is parallelism within a _single_ CPU core.
If you later switch to a parallel implementation running with multiple threads or processes on multiple cores, each individual core will still be able to do instruction-level parallelism internally.

In this case, calculating `x_squared` and `y_squared` is completely independent, so your CPU is likely to run both at once.
This happens transparently: the compiler doesn't have to do anything special, the CPU will do this all on its own.

::: {.content-hidden when-format="markdown"}
```{mermaid}
flowchart TD
 A("x_squared = x_length ** 2") --> C("__temp = x_squared + y_squared")
 B("y_squared = y_length ** 2") --> C
 C --> D("result = sqrt(__temp)")
```
:::

::: {.content-hidden unless-format="markdown"}
```
x_squared = x_length ** 2       y_squared = y_length ** 2
          \                                /
           \______________________________/
                          ‚Üì
          __temp = x_squared + y_squared
                          ‚Üì
          ... result = sqrt(__temp) ...
```
:::

Not all code can be transparently run in parallel.
For one thing, a calculation can't be run if its inputs aren't yet available:

* The CPU cannot run the instructions for `x_squared + y_squared` until both inputs have been calculated.
* Similarly, the `sqrt()` can't be run until `x_squared + y_squared` finishes.

We can say that these are "data dependencies" that prevent instruction-level parallelism, and therefore reduce the code's potential runtime speed.

## Example #1: Removing data dependencies from a recursive algorithm

To see the speed impact of ILP, let's look at an example: generating an array of random numbers.
For educational purposes, we find [an example of how to do this](https://nuclear.llnl.gov/CNP/rng/rngman/node4.html) and write the following code:

```{python}
# Tell Numba to disable SIMD, so it doesn't hide other effects; we'll talk
# about SIMD in the next chapter.
import os
os.environ["NUMBA_LOOP_VECTORIZE"] = "0"
os.environ["NUMBA_SLP_VECTORIZE"] = "0"

# Import the dependencies we'll need:
import numpy as np
from numba import jit, uint32, uint64

# 64-bit linear congruent generator; we'll use the top 32-bits only, in order
# to maximize the cycle length and therefore perceived randomness.
@jit("uint32(uint32)")
def lcg(seed):
    temp = uint64(seed) * 2862933555777941757 + 3037000493
    return temp >> 32

@jit
def generate_random_numbers(n):
    result = np.empty((n,), dtype=np.uint32)
    result[0] = uint32(1)
    for i in range(1, n):
        # üôÅ result[i] has a data dependency on result[i - 1], preventing
        # instruction-level parallelism:
        result[i] = lcg(result[i - 1])
    return result
```

```{python}
#| echo: false
%load_ext book_magics
```

To get a sense of the randomness, we can visualize the result as an image.
While a lack of patterns doesn't necessarily mean the data is random, if we do see patterns then it definitely is not random.

```{python}
def to_image(f):
    return f(256 * 256 // 4).view(np.uint8).reshape((256, 256))

RAND1 = to_image(generate_random_numbers)
```

Our first result looks like noise, which is what we want in a random number generator:

```{python}
#| echo: false
%display_image RAND1
```

Unfortunately, this calculation does not allow a lot of parallelism.
Imagine we asked for 4 values:

* `result[3] = lcg(result[2])`, so we need `result[2]` before proceeding.
* `result[2] = lcg(result[1])`, so we need `result[1]`.
* `result[1] = lcg(result[0])`, so we need `result[0]`.

In short, each value in the array has a data dependency on the previous value in the array; they cannot be calculated in parallel.

### Experiment to find the impact of data dependencies

How much speed are we losing because of the data dependencies above?
To find out, we'll tweak the code to completely remove all data dependencies.
The result won't be useful code, but it will let us see how much of a speed-up we can potentially get.

We're going to calculate `result[i]` purely based on `i`:

```{python}
@jit
def generate_not_so_random_numbers(n):
    result = np.empty((n,), dtype=np.uint32)
    for i in range(1, n):
        # ü§î As a temporary experiment, remove all data dependencies that are
        # preventing ILP, allowing us to see how much faster we can go:
        result[i] = lcg(i)
    return result

RAND2 = to_image(generate_not_so_random_numbers)
```

These calculations are completely unrelated and can be calculated in parallel:

* `result[3] = lcg(3)`
* `result[2] = lcg(2)`

This version of the code is not a good random number generator, as we can see when we visualize the results:

```{python}
#| echo: false
%display_image RAND2
```

But this experiment can give us a sense of how fast we could potentially go.
In the following performance comparison I've included the number of CPU instructions the code ran.
Notice that the number of CPU instructions increased, even as the runtime decreased.
That's because `generate_not_so_random_numbers()` allows the CPU to run those extra instructions in parallel, speeding up the actual run time.

```{python}
#| echo: false
%%compare_timing --measure=instructions
generate_random_numbers(1_000_000)
generate_not_so_random_numbers(1_000_000)
```

### Use interleaving to reduce the impact of data dependencies

Our performance experiment was informative, but not useful as a random number generator.
But there are less naive approaches that should preserve the pseudo-randomness, while breaking the data dependency.

Instead of having a single random number generator, we're going to have 4, and we're going to interleave their calculations.
The result will be slightly different, but it's a random number generator, so that's fine.

For other algorithms we might be able to get the same kind of performance improvement by interleaving calculations from different areas of the data.
For example, in a row-oriented algorithm processing 2D data we could interleave the calculations for multiple rows.

Here's our new version:

```{python}
@jit
def generate_random_numbers_3(n):
    result = np.empty((n,), dtype=np.uint32)

    # Make sure we have 4 sufficiently different starting points:
    result[0] = uint32(1)
    result[1] = lcg(lcg(result[0]) + 1)
    result[2] = lcg(lcg(result[1]) + 1)
    result[3] = lcg(lcg(result[2]) + 1)

    # üòé Do 4 unrelated calculations that don't share data dependencies with
    # each other, allowing the CPU to run them in parallel:
    for i in range(1, n // 4):
        result[i * 4    ] = lcg(result[(i - 1) * 4    ])
        result[i * 4 + 1] = lcg(result[(i - 1) * 4 + 1])
        result[i * 4 + 2] = lcg(result[(i - 1) * 4 + 2])
        result[i * 4 + 3] = lcg(result[(i - 1) * 4 + 3])

    # Calculate the remaining last few values:
    for i in range(n % 4):
        result[n - (3 - i)] = lcg(result[n - (3 - i) - 1])

    return result

RAND3 = to_image(generate_random_numbers_3)
```

Our image is random-looking again:

```{python}
#| echo: false
%display_image RAND3
```

This is faster than the first version, but still pretty slow:

```{python}
#| echo: false
%%compare_timing --measure=instructions
generate_random_numbers(1_000_000)
generate_not_so_random_numbers(1_000_000)
generate_random_numbers_3(1_000_000)
```

### Try to use temporary variables instead of reading and writing to arrays

Why is `generate_random_numbers_3()` slower than our `generate_not_so_random_numbers()` experiment?
For one thing, it's running twice as many CPU instructions.
Why is that?

Without looking at the generated machine code, my theory is that it is all the `result[(i - 1) * 4]` calculations, plus extra reads from the array.
Now, you might ask why the compiler doesn't notice that the value it is writing in iteration `i` is a value it will immediately read back in iteration `i + 1`.
Can't it cache that value in a compiler-generated temporary variable, and skip the array reads and corresponding math?

In theory, it probably could.
In practice, we've told the compiler a write to `result[i * 4]` followed by `i += 1` followed by a read to `result[(i - 1) * 4]`, with a whole bunch of other array reads and writes in the middle.
This particular version of the compiler may simply not notice the possibility of omitting the extra calculations and extra reads.

To try to reduce the number of instructions, instead of reading back intermediate values from the array, we can explicitly store them in temporary variables.
This reduces the amount of arithmetic and array reads we're asking the compiler to do, so it should be faster on that account.
And since the code we're giving the compiler is simpler, it may also enable the compiler to do additional optimizations:

```{python}
@jit
def generate_random_numbers_4(n):
    result = np.empty((n,), dtype=np.uint32)
    # üòé Use simple variables to make it faster to access the previous value:
    random_number1 = uint32(1)
    random_number2 = lcg(lcg(random_number1) + 1)
    random_number3 = lcg(lcg(random_number2) + 1)
    random_number4 = lcg(lcg(random_number3) + 1)

    for i in range(n // 4):
        random_number1 = lcg(random_number1)
        random_number2 = lcg(random_number2)
        random_number3 = lcg(random_number3)
        random_number4 = lcg(random_number4)
        # üòé Less math, no more array reads:
        result[i * 4    ] = random_number1
        result[i * 4 + 1] = random_number2
        result[i * 4 + 2] = random_number3
        result[i * 4 + 3] = random_number4

    for i in range(n % 4):
        result[n - (3 - i)] = lcg(result[n - (3 - i) - 1])

    return result

RAND4 = to_image(generate_random_numbers_4)
```

This is how it looks:

```{python}
#| echo: false
%display_image RAND4
```

And now our code is much faster, with approximately the same number of instructions as our original version, but with significant parallelism:

```{python}
#| echo: false
%%compare_timing --measure=instructions
generate_random_numbers(1_000_000)
generate_random_numbers_4(1_000_000)
```

## Example #2: Removing accumulator data dependencies with MapReduce

Let's look at another data dependency bottleneck: an accumulator.
If we sum the values in an array, in every iteration of the loop we add values to the same variable, `total`.
That variable acts as a data dependency, and prevents instruction-level parallelism:

```{python}
DATA = np.arange(0, 1_000_000, dtype=np.uint64)

@jit
def sum1(arr):
    total = 0
    for i in range(len(arr)):
        # üôÅ total is updated in every loop iteration, so it's a data
        # dependency.
        total += arr[i]
    return total

_ = sum1(DATA)
```

We can solve this by having multiple partial accumulators, that get data added in parallel.
At the end of the function we add the accumulators to get the final sum:

```{python}
@jit
def sum2(arr):
    # üòé Four accumulators instead of one:
    total1, total2, total3, total4 = 0, 0, 0, 0
    for i in range(len(arr) // 4):
        total1 += arr[i * 4]
        total2 += arr[i * 4 + 1]
        total3 += arr[i * 4 + 2]
        total4 += arr[i * 4 + 3]
    # Add the remaining items at the end:
    for i in range(len(arr) - len(arr) % 4, len(arr)):
        total1 += arr[i]
    return total1 + total2 + total3 + total4

assert sum1(DATA) == sum2(DATA)
```

This is a variation of the MapReduce design pattern:

1. **Map:** Run multiple copies of the same algorithm on different parts of the data to get parallelism.
   In this case, the parallelism comes from CPU's ILP.
2. **Reduce:** Combine all the partial results into one final result.

And as we hoped, the second version allows more scope for parallelism:

```{python}
#| echo: false
%%compare_timing --measure=instructions
sum1(DATA)
sum2(DATA)
```

::: {.callout-note}
If you were to run the above code with SIMD enabled, the `sum1()` function would actually be faster than `sum2()` when run on `uint64`s.
That's because the compiler would figure out it can use SIMD in the first case, but not the second, and using SIMD is faster than instruction-level parallelism in this case, on my specific computer.

This technique is still worth trying even if SIMD is enabled, as we'll see in an example in a later chapter.
:::
