# Use SIMD (Single Instruction, Multiple Data) to speed up repetitive computations

Single Instruction Multiple Data, or SIMD, are CPU instructions that can execute the same operation on a sequence primitive values (integers or floats) that are stored next to each other in memory, using a single instruction.
The compiler needs to explicitly generate these instructions, unlike the instruction-level parallelism we previously covered.

For example, if we want to multiply 4 integers by a constant, we can do that with a single SIMD CPU instruction instead of with 4 normal CPU instructions.
Reducing the number of instructions to run by a factor of 4 can then lead to a significant speedup.
This is sometimes referred to as "vectorization".

::: {.callout-note}
This is a different meaning than typically used in Python programming, where "vectorization" means a batch operation implemented in a low-level language.
Given we're speeding up Python, the usage can sometimes be ambiguous, so make sure you understand which of the two meaning applies when reading documentation.
:::

We'll set up our normal imports:

```{python}
# Import the dependencies we'll need:
import numpy as np
from numba import njit
```

```{python}
#| echo: false
%load_ext book_magics
```

We'll also import a context manager that let us disable SIMD on the fly when defining functions:

```{python}
from book_utilities import disabled_simd
```

## Unlike instruction-level parallelism, SIMD requires work by the compiler

In the previous chapter we discussed instruction-level parallelism, where the CPU runs multiple instructions in parallel.
And SIMD is also a way to execute work in parallel.
So what is the difference?

* The decision to use instruction-level parallelism happens at runtime, and is made by the CPU.
* The decision to use SIMD happens when the code is compiled.
  If the compiler generate specialized SIMD machine code instructions, then SIMD will be used at runtime.

Consider the following code:

```python
out[0] = a[0] + b[0]
out[1] = a[1] + b[1]
out[2] = a[2] + b[2]
out[3] = a[3] + b[3]
```

If these are arrays of 64-bit integers, and we imagine our CPU has a `add64` instruction, the compiler might turn the above code into 4 regular (non-SIMD) CPU instructions, something like:

```
add64 a[0] b[0] → out[0]
add64 a[1] b[1] → out[1]
add64 a[2] b[2] → out[2]
add64 a[3] b[3] → out[3]
```

When you run these four CPU instructions, the CPU will realize they are independent of each other and run them in parallel.
If it has the hardware to do four additions in parallel, all of the operations can run at the same time.
But _you_ don't get to tell the CPU to run those 4 instructions in parallel; if it can't or won't, there's nothing you can do.

SIMD (single instruction multiple data) is different: it involves a _single_ instruction, as its name implies, doing the same thing to multiple pieces of data.
In particular, SIMD operations run on contiguous chunks of memory of 128-, 256-, or 512-bits, depending on the model of CPU and which operation you are doing.
Different operations require the compiler to generate machine code using different specialized instructions.
For example, you might be able to add an array of 4 64-bit integers to another array of 4 64-bit integers, since 4×64 is 256.

For our code example, rather than generating 4 CPU instructions with 4 adds, the compiler can generate a single SIMD instruction, a hypothetical `simd_add256` instruction.
That single instruction will do 4 additions with a single instruction:

```
simd_add256 a[0:3] b[0:3] → out[0:3]
```

When the CPU core reaches that instruction, it will execute the instruction the machine code told it to execute: 4 additions at the same time.

## The compiler can automatically generate SIMD instructions for you ("auto-vectorization")

In many situations, the compiler can automatically convert your code to use SIMD instructions, speeding up your code (or in some cases, slowing it down!).
This is known as "auto-vectorization".
Numba, for example, will automatically generate SIMD code optimized for your specific CPU model.

The compiler will notice you are doing the same operation on a series of items, and instead of doing the work item by item it will do it in batches of 4 or 8 or 16.

We'll talk about how the batch size is decided in just a bit, but first let's see how SIMD speeds up some example code.
In particular, we have two versions of the exact same code, one compiled with SIMD (the default) and one with SIMD disabled:

```{python}
DATA_FLOAT64 = np.ones((1_000_000,), dtype=np.float64)
DATA_FLOAT32 = np.ones((1_000_000,), dtype=np.float32)

@njit
def simd(x):
    out = np.empty(x.shape, dtype=x.dtype)
    for i in range(len(x)):
        out[i] = x[i] * 2
    return out

assert simd(DATA_FLOAT64)[0] == 2
assert simd(DATA_FLOAT32)[0] == 2

# This is like doing @njit, with some behind-the-scenes magic to disable
# usage of SIMD for this function only.
with disabled_simd() as njit_no_simd:
    @njit_no_simd
    def no_simd(x):
        out = np.empty(x.shape, dtype=x.dtype)
        for i in range(len(x)):
            out[i] = x[i] * 2
        return out

    assert no_simd(DATA_FLOAT64)[0] == 2
    assert no_simd(DATA_FLOAT32)[0] == 2
```

Here's how long each variation takes to run:

```{python}
#| echo: false
%%compare_timing --measure=simd_256bit,instructions
no_simd(DATA_FLOAT32)
simd(DATA_FLOAT32)
no_simd(DATA_FLOAT64)
simd(DATA_FLOAT64)
```

## Smaller data types require fewer SIMD instructions

Looking at the table above, we can see there are 250K floating-point SIMD instructions used to process 1M 64-bit floats, and 125K floating-point SIMD instructions used to proces 1M 32-bit floats.
And those instructions are 256-bit SIMD instructions.

256-bit SIMD means the instruction operates on 256 bits at once: the code packs as many numbers as will fit into 256 bits, and then the SIMD operation runs on that.

* For 64-bit floats, $256 / 64 = 4$, so we can process 4 `float64` with each SIMD instruction.
  And $1000000 / 4 = 250000$.
* For 32-bit floats, $256 / 32 = 8$, so we can process 8 `float32` with each SIMD instruction.
  And $1000000 / 8 = 125000$.

Needing to use half as many instructions is likely part of the reason that processing 1,000,000 `float32`s was faster than processing the equivalent number of `float64`.
We'll talk about an additional reason, memory caches and bandwidth, in a later chapter.

## Avoid code patterns that prevent auto-vectorization

Auto-vectorization doesn't always happen: sometimes the compiler can't figure out how generate equivalent SIMD code.
In fact, many of the same problems that work against instruction-level parallelism also can prevent auto-vectorization:

* **The potential for aliasing.** If you are dealing with overlapping memory, doing an operation one array item at a time might give different results than doing it in batches in parallel.
  As a result, if there is a potential for aliasing the compiler might choose not to use SIMD instructions in order to ensure the code matches your (presumed) intentions.
* **Complex functions that didn't get inlined.**
  The compiler likely won't pass 256-bit data batches across function call boundaries, so the function call will limit the ability to use SIMD.
* **Branches.** SIMD instructions do the same thing to multiple pieces of data, which means branching for each item may be difficult.
  Depending on the SIMD instructions your CPU has available, the compiler might have some tricks up its sleeve, but the more branching your code has the less likely it is that auto-vectorization will work.
* **Floating point numbers.** We'll talk about this in a later chapter.
* **The potential for memory being non-contiguous.**
  This is particularly an issue with NumPy views, that can skip every N bytes for a variety of reasons, for example a slice across a dimension that doesn't match the data order in memory.
  Code that is given memory that may or may not be contiguous will get optimized differently than code that can assume the data is contiguous.

Since these issues can affect performance in other ways as well, preventing compiler optimizations and instruction-level parallelism, we'll cover generic solutions in later chapters.

::: {.callout-info}
You can usually get your compiler to give you some hints about why it failed to auto-vectorize your code.
For example, with Numba you can run the following _before_ you import `numba` for the first time:

```{python}
#| eval: false
import llvmlite.binding as llvm
llvm.set_option('', '--debug-only=loop-vectorize')
```
:::

## Don't assume auto-vectorized code is always faster

Unfortunately, not all code runs faster when auto-vectorized; in some cases it will be slower, depending on the inputs, the compiler version, and your specific CPU model.
So run code with and without auto-vectorization and see which version is faster.

::: {.callout-info}
On Numba you can disable auto-vectorization by setting the [environment variables](https://numba.readthedocs.io/en/stable/reference/envvars.html) `NUMBA_LOOP_VECTORIZE` to `0` and `NUMBA_SLP_VECTORIZE` to `0`.
Other compilers have their own specific options.
:::

## What gets auto-vectorized?

Which code gets auto-vectorized depends very much on which compiler you're using, and it can also change across releases of a single compiler.
But it can be instructive to look at one particular compiler can do, in particular the [documentation for the clang compiler](https://llvm.org/docs/Vectorizers.html) for C and C++.
Clang is based on LLVM, so these kinds of optimizations are available to other LLVM-based compilers, notably Numba and Rust.

Clang supports vectorizing loops, like the ones we saw in the previous example.
It has lots of code for supporting complex expressions inside the loop (including conditionals), reverse iteration, more exotic forms of iteration that can be harder to spot, and so on.

The loop vectorizer is top-down: it notices a larger code structure, the loop, and then tries to see if the loop's contents can be replaced with SIMD instruction.
Clang also supports a different form of vectorization called superword-level parallelism (SLP) which notices similar expressions that can be combined into a single expression or expressions.
Where the loop vector is top-down, the SLP vectorizer is bottom-up: it notices individual expressions that are structurally similar.

```{python}
import numpy as np
from numba import njit

@njit
def add(a, b):
    out = np.zeros((4,), dtype=a.dtype)
    out[0] = a[0] + b[0]
    out[1] = a[1] + b[1]
    out[2] = a[2] + b[2]
    out[3] = a[3] + b[3]
    return out
```


::: {.callout-info}
While SLP is on by default in Clang, some versions of Numba have disabled SLP due to issues with bad code generation (see the [release notes](https://numba.pydata.org/numba-doc/dev/release-notes.html)).
This is one of the problems with auto-vectorization: it can stop working unexpectedly due to changes in the compiler, both intentional and unintentional.
:::
