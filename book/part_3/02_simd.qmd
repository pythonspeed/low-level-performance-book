# Use SIMD (Single Instruction, Multiple Data) to speed up repetitive computations

Single Instruction Multiple Data, or SIMD, are CPU instructions that can execute the same operation on a sequence primitive values (integers or floats) that are stored next to each other in memory, using a single instruction.
The compiler needs to explicitly generate these instructions, unlike the instruction-level parallelism we previously covered.

For example, if we want to multiply 4 integers by a constant, we can do that with a single SIMD CPU instruction instead of with 4 normal CPU instructions.
Reducing the number of instructions to run by a factor of 4 can then lead to a significant speedup.

::: {.callout-note}
This is a different meaning than typically used in Python programming, where "vectorization" means a batch operation implemented in a low-level language.
Given we're speeding up Python, the usage can sometimes be ambiguous, so make sure you understand which of the two meaning applies when reading documentation.
:::

We'll set up our normal imports:

```{python}
# Import the dependencies we'll need:
import numpy as np
from numba import njit
```

```{python}
#| echo: false
%load_ext book_magics
```

We'll also import a context manager that let us disable SIMD on the fly when defining functions:

```{python}
from book_utilities import disabled_simd
```

## Unlike instruction-level parallelism, SIMD requires work by the compiler

In the previous chapter we discussed instruction-level parallelism, where the CPU runs multiple instructions in parallel.
And SIMD is also a way to execute work in parallel.
So what is the difference?

* The decision to use instruction-level parallelism happens at runtime, and is made by the CPU.
* The decision to use SIMD happens when the code is compiled, when the compiler may generate specialized SIMD instructions.

Consider the following code:

```python
#| eval: false
out[0] = a[0] + b[0]
out[1] = a[1] + b[1]
out[2] = a[2] + b[2]
out[3] = a[3] + b[3]
```

If these are arrays of 64-bit integers, and we imagine our CPU has a `add64` instruction, the compiler might turn the above code into 4 regular (non-SIMD) CPU instructions, something like:

```
add64 a[0] b[0] → out[0]
add64 a[1] b[1] → out[1]
add64 a[2] b[2] → out[2]
add64 a[3] b[3] → out[3]
```

When you run these four CPU instructions, the CPU will realize they are independent of each other and run them in parallel.
If it has the hardware to do four additions in parallel, all of the operations can run at the same time.
But _you_ don't get to tell the CPU to run those 4 instructions in parallel; if it can't or won't, there's nothing you can do.

SIMD (single instruction multiple data) is different: it involves a _single_ instruction, as its name implies, doing the same thing to multiple pieces of data.
In particular, SIMD operations run on contiguous chunks of memory of 128-, 256-, or 512-bits, depending on the model of CPU and which operation you are doing.
Different operations require the compiler to generate machine code using different specialized instructions.
For example, you might be able to add an array of 4 64-bit integers to another array of 4 64-bit integers, since 4×64 is 256.

For our code example, rather than generating 4 CPU instructions with 4 adds, the compiler can generate a single SIMD instruction, a hypothetical `simd_add256` instruction.
That single instruction will do 4 additions with a single instruction:

```
simd_add256 a[0:3] b[0:3] → out[0:3]
```

When the CPU executes that instruction, it doesn't have a choice about whether to use paralellism or not.
It must execute what the machine code says it should do: do 4 additions at the same time.

## Auto-vectorization: the compiler can automatically generate SIMD instructions for you

In many situations, the compiler can automatically use SIMD, speeding up your code (or, occasionally, slowing it down).
This is known as "auto-vectorization".
Numba, for example, will automatically generate SIMD code optimized for your specific CPU model.

The compiler will notice you are doing the same operation on a series of items, and instead of doing the work one by one (as your implies) it will do it in batches of 4 or 8 or 16.
We'll talk about how the batch size is decided in just a bit, but first let's see how SIMD speeds up the following example.
In particular, we have two versions of the exact same code, one compiled with SIMD (the default) and one with SIMD disabled:

```{python}
DATA_FLOAT64 = np.ones((1_000_000,), dtype=np.float64)
DATA_FLOAT32 = np.ones((1_000_000,), dtype=np.float32)

@njit
def simd(x):
    out = np.empty(x.shape, dtype=x.dtype)
    for i in range(len(x)):
        out[i] = x[i] * 2
    return out

assert simd(DATA_FLOAT64)[0] == 2
assert simd(DATA_FLOAT32)[0] == 2

# This is like doing @njit, with some behind-the-scenes magic to disable
# usage of SIMD for this function only.
with disabled_simd() as njit_no_simd:
    @njit_no_simd
    def no_simd(x):
        out = np.empty(x.shape, dtype=x.dtype)
        for i in range(len(x)):
            out[i] = x[i] * 2
        return out

    assert no_simd(DATA_FLOAT64)[0] == 2
    assert no_simd(DATA_FLOAT32)[0] == 2
```

Here's how long each variation takes to run:

```{python}
#| echo: false
%%compare_timing --measure=simd_256bit
no_simd(DATA_FLOAT32)
no_simd(DATA_FLOAT64)
simd(DATA_FLOAT32)
simd(DATA_FLOAT64)
```

## TODO SIMD register size

TODO Why 250K/125K instruction? 1M divided by 8 or 4 depending how many fit in 256 bit register.
Since the SIMD instructions on my computer are 256-bit, they are able to add more 4 times as many 16-bit integers as 64-bit integers, making SIMD more competitive versus instruction-level parallelism.

## What prevents auto-vectorization?

Auto-vectorization doesn't always happen: sometimes the compiler can't figure out how generate equivalent SIMD code.
In fact, many of the same problems that work against instruction-level parallelism also can prevent auto-vectorization:

* **The potential for aliasing.** If you are dealing with overlapping memory, doing an operation one array item at a time might give different results than doing it in batches in parallel.
  As a result, if there is a potential for aliasing the compiler might choose not to use SIMD instructions in order to ensure the code matches your (presumed) intentions.
* **Complex functions that didn't get inlined.**
  The compiler likely won't pass 256-bit data batches across function call boundaries, so the function call will limit the ability to use SIMD.
* **Branches.** SIMD instructions do the same thing to multiple pieces of data, which means branching for each item may be difficult.
  Depending on the SIMD instructions your CPU has available, the compiler might have some tricks up its sleeve, but the more branching your code has the less likely it is that auto-vectorization will work.
* **Floating point numbers.** We'll talk about this in a later chapter.
* **The potential for memory being non-contiguous.**
  This is particularly an issue with NumPy views, that can skip every N bytes for a variety of reasons, for example a slice across a dimension that doesn't match the data order in memory.
  Code that is given memory that may or may not be contiguous will get optimized differently than code that can assume the data is contiguous.

Since these issues can affect performance in other ways as well, preventing compiler optimizations and instruction-level parallelism, we'll cover generic solutions in later chapters.

::: {.callout-info}
You can usually get your compiler to give you some hints about why it failed to auto-vectorize your code.
For example, with Numba you can run the following _before_ you import `numba` for the first time:

```{python}
#| eval: false
import llvmlite.binding as llvm
llvm.set_option('', '--debug-only=loop-vectorize')
```
:::
