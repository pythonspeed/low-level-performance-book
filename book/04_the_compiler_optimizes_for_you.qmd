# The compiler can optimize for you (sometimes)

We'll start by going over the compiler's optimization capabilities.
The compiler is able to get rid of some redundant or inefficient code, but certainly not all of it.
So let's get ready to run some examples, and then consider what the compiler is already doing for you, and where it might need your manual intervention.

```{python}
# Disable SIMD so it doesn't distort or hide certain effects:
import os
os.environ["NUMBA_LOOP_VECTORIZE"] = "0"
from numba import njit
import numpy as np
```

```{python}
#| echo: false
%load_ext book_magics
```

Some of these optimizations you will have to do manually, but some can be automatically by the compiler.

## Compiler optimizations and their limitations

When your code gets compiled, the compiler runs it through a series of optimization passes, with each pass transforming the code in the some way.
The hope is that the cumulative series of transformations will result in code that is more efficient by:

1. Removing redundant code execution, our focus in this chapter.
2. Utilizing the CPU as efficiently as possible, our focus in later chapters.

A key requirement for the optimization passes is that your code continues behave the same way, even as it gets faster.
**To an outside observer, the results of running optimized code must be indistinguishable from running the original code.**

As a result, even if you had a perfect compiler that could identify every single optimization possibility, there would still be optimizations that couldn't be applied.
Only you, the author of the code, can know whether or not it is acceptable to make a change to the code's results and side-effects.
In practice, the compiler may only be looking at a single function: it doesn't have a full view of the whole program, especially when you're just writing a Python extension.

### Some examples of what the compiler can do

Even with this constraint mind, there is still plenty the compiler can do to speed up your code.

#### Precalculating mathematical expressions

If you have a fixed mathematical expression, the compiler can often just execute it at compilation time and replace the expression with the result.
For example, notice how the first function is slower, while the latter two take the same amount of time:

```{python}
@njit
def calc(a, b, c, d, e, f):
    return ((a + b + c) ** d / e) + f

@njit
def calc_fixed():
    return ((1 + 2 + 3) ** 7 / 6.0) + 5

@njit
def just_the_result():
    return 46661.0

# All three functions give the same result:
assert calc_fixed() == calc(1, 2, 3, 7, 6.0, 5)
assert calc_fixed() == just_the_result()
```

Here's how their speed compares:

```{python}
#| echo: false
%%compare_timing

calc(1, 2, 3, 7, 6.0, 5)
calc_fixed()
just_the_result()
```

#### Hoisting expressions out of inner loops

The compiler can also hoist repetitive expressions out of loops, so they're called once instead of many times.
For example, the expression `((n ** 2) * (1.5 / n) + n)` in the `compiler_will_hoist()` is repeated in every iteration, but it also always give the same result.
The compiler is therefore able to move it out of the loop and run it only once, as we do manually in `manually_hoisted()`.
Thus both functions take the same amount of time to run:

```{python}
@njit
def compiler_will_hoist(m, n):
    total = 0
    for i in range(m):
        total = m * ((n ** 2) * (1.5 / n) + n)
    return total

@njit
def manually_hoisted(m, n):
    total = 0
    constant = (n ** 2) * (1.5 / n) + n
    for i in range(m):
        total = m * constant
    return total

# The results are the same:
assert compiler_will_hoist(100, 17) == manually_hoisted(100, 17)
```

Notice they run at the same speed, because the compiler automatically optimizes the first version by hoisting the constant expression:

```{python}
#| echo: false
%%compare_timing

compiler_will_hoist(1_000_000, 3)
manually_hoisted(1_000_000, 3)
```

While both versions have the same speed, the manually hoisted version is arguably clearer about what is going on in the calculation, and the compiler won't always be able to hoist expressions, so doing it manually might still be worth it.

#### Using faster CPU instructions

TODO maybe move to later chapter with a more sophisticated model of the CPU

For any given piece of code there are different ways the compiler could translate the code into CPU instructions.
This is relevant because some CPU instructions are faster than others.
For example, division is a complex operation, so in situations where the compiler can avoid using it, it will try to use other CPU instructions.

One such useful instruction is bitshifting: a bitshift of 3 to the right will change 97 to 12, as it's shifting the bits of `1100001` three to the right, resulting in `1100`.
Put another way, bitshifting N to the right is equivalent to dividing by 2^N.

```{python}
print(97 >> 3)
print(97 // (2 ** 3))
```

If the compiler is asked to do a generic division, it can't really switch to other CPU instructions, so it's stuck with slow division instead of a fast bitshift.

```{python}
@njit
def generic_division(divisor):
    arr = np.empty((1000,), dtype=np.int64)
    for i in range(len(arr)):
        arr[i] = i / divisor
    return arr

@njit
def generic_bitshift(bits):
    arr = np.empty((1000,), dtype=np.int64)
    for i in range(len(arr)):
        arr[i] = i >> bits
    return arr

assert np.array_equal(generic_division(8), generic_bitshift(3))
```

```{python}
#| echo: false
%%compare_timing
generic_division(8)
generic_bitshift(3)
```

Notice how division is slower than bitshifts.
However, if the compiler knows you're dividing by a power of 2, it can choose to use faster operations than division.
To see whether bitshifts were actually used we'd have to look at the generated CPU instructions, but even without doing that we can see that in this case the compiler was able to generate faster code to get the _equivalent_ of division by 8:

```{python}
@njit
def specific_division():
    arr = np.empty((1000,), dtype=np.int64)
    for i in range(len(arr)):
        arr[i] = i // 8
    return arr

@njit
def specific_bitshift():
    arr = np.empty((1000,), dtype=np.int64)
    for i in range(len(arr)):
        arr[i] = i >> 3
    return arr

assert np.array_equal(specific_division(), specific_bitshift())
```

```{python}
#| echo: false
%%compare_timing
specific_division()
specific_bitshift()
```

There are many other optimizations the compiler can do, with more added or tweaked with every compiler release.

## What the compiler won't do

Sometimes the compiler won't apply seemingly obvious optimizations because the optimization might change your code's behavior; when in doubt, the compiler will err on the side of conservatism.
Some reasons this can happen include:

* Floating point calculations, which we'll talk about in a later chapter.
* Access to memory that is visible outside the function.
* The potential for aliasing.
* Function calls.

By identifying these patterns, you can learn to write code that avoids them.

### Memory accesses can prevent optimizations

If your code is accessing a local variable that is present only in the function, that is clearly something the compiler can safely optimize and modify.
However, the compiler may consider removing writes to memory that is accessible outside your function to be more questionable, because in theory those writes might have side effects.

```{python}
@njit
def local_stack(n):
    acc = 0
    for i in range(n):
        acc += i
    return acc

@njit
def external_memory(n, arr):
    arr[0] = 0
    for i in range(n):
        arr[0] += i
    return arr[0]


# Both functions give the same result:
ZEROS = np.zeros((1, ), dtype=np.float64)
assert local_stack(5) == external_memory(5, ZEROS)
```

Here's how their speed compares:

```{python}
#| echo: false
%%compare_timing

local_stack(1_000_000)
external_memory(1_000_000, ZEROS)
```

TODO this should really be two optimizations with this section having its own example

Of the two versions, the first version is _vastly_ faster, surprisingly so.
If my i7-12700k CPU runs at a clock frequency of approximately 4GHZ, that means it does about 4 CPU cycles per nanosecond.
It should be able to do a few CPU instructions per cycle thanks to instruction-level parallelism, which we'll talk about the in the next chapter.
So let's say it can do 2000 instructions in 100 nanoseconds, just to be generous.
How can it do a million additions with just 2000 instructions?

The LLVM compiler used by Numba is smart enough to figure out that the sum of 0, 1, 2, ..., N is actually `NÃ—(N + 1)/2`, so it just substitutes the faster formula when it can.
So the runtime of the `local_stack()` function will actually be the same regardless of `N`.

Unfortunately, in the case of using an externally given array as the temporary accumulator, the presence of writes to the array likely prevents that particular optimization.
If the array we're using as an accumulator is local to the function, in this case at least the compiler is once again happy to apply the optimization:

```{python}
@njit
def internal_array(n):
    arr = np.zeros((1, ), dtype=np.uint64)
    for i in range(n):
        arr[0] += i
    return arr[0]

assert local_stack(5) == internal_array(5)
```

And if we time it:

```{python}
#| echo: false
%%compare_timing

internal_array(1_000_000)
```

### The possibility of aliasing can prevent optimizations

Aliasing means having multiple pointers referencing the same memory address at the same time.
The possibility of aliasing can limit what the compiler is able to do to speed up your code, because it means seemingly identically behaved functions can actually behave differently when inputs are aliased.

TODO note source of example, the systems programming book, in footnote or osmething

```{python}
@njit
def add_twice(first_arr, second_arr):
    for i in range(len(first_arr)):
        first_arr[i] += second_arr[i]
        first_arr[i] += second_arr[i]
    return first_arr

@njit
def add_doubled(first_arr, second_arr):
    for i in range(len(first_arr)):
        first_arr[i] += 2 * second_arr[i]
    return first_arr

def zeros():
    return np.zeros((1_000_000, ), dtype=np.uint64)

DATA2 = np.linspace(1_000_000, 0, 1_000_000, dtype=np.uint64)

# Both functions give the same result (in this case at least):
assert np.array_equal(
    add_twice(zeros(), DATA2),
    add_doubled(zeros(), DATA2)
)
```

The second implementation is faster, because it does less work and the compiler doesn't optimize that work away:

```{python}
#| echo: false
%%compare_timing

add_twice(zeros(), DATA2)
add_doubled(zeros(), DATA2)
```

On the face of it, the two implementations are semantically the same, and so it's surprising that the compiler won't transform the first implementation into the second, slightly faster form.

The likely issue is that the compiler can't know at compile time if the two arrays are actually the same array: it needs to take aliasing into account.
This matters because the two functions behave differently for inputs that involve aliasing, which means the compiler can't swap the two implementations out.
For example, if we pass the same array to both arguments of the functions, we get different results:

```{python}
arr = np.ones((3, ), dtype=np.uint64)
print("add_twice() gives:", add_twice(arr, arr))

arr = np.ones((3, ), dtype=np.uint64)
print("add_doubled() gives:", add_doubled(arr, arr))
```

You have multiple ways to address this optimization blocker:

1. Rewrite your code so aliasing won't affect the calculation, allowing the compiler more scope for optimization.
   In the example above, instead of mutating the first array the results could instead be written to an output array newly created by the function itself.
   As a new array, the compiler would be able to tell that aliasing was not possible, enabling a larger scope for optimizations.
2. Restructure the code to do the optimizations manually, or at least those that you can easily identify.
3. Your programming language may have a way to indicate that aliasing is not an issue (see below).

::: {.callout-info}
In Rust aliasing is usually not a performance blocker for the compiler.
To ensure memory safety, the compiler ensures that there is either a single writable pointer to a particular memory address, or any number of read-only pointers, but you can never have both.
As a result, if you rewrote the above example in Rust, the compiler would know that `first_arr` and `second_arr` cannot alias, and that information would be conveyed to the optimization layer.

In C you can use the `restrict` keyword to manually indicate that you have enforced a uniqueness constraint.
Additionally, when compiling C and C++ there are a variety of ways the compiler can handle aliasing scenarios, including pretending it won't happen, pretending but warning you, or choosing the safer-but-slower option.
Depending on your compiler you can use compiler flags to control this behavior.
:::

### Function calls can prevent optimizations

When your code calls another function, the compiler may be uncertain whether that other function has side-effects.
If it does have side-effects, removing calls to that function would change the program's behavior, so the compiler will err on the side of conservatism when optimizing.

In practice, the compiler will often "inline" small functions, essentially copy/pasting their code into the calling function.
This allows the compiler to analyze all the code together, enabling additional optimizations.

Still, calling large complex functions in hot loops can reduce the compiler's ability to optimize your code if the function doesn't get inlined.

### Floating point calculations can prevent optimizations

Some optimizations the compiler will do for integer math won't happen for floating point calculations.
This is covered in a later chapter.

