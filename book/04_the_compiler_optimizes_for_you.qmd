# The compiler can optimize for you (sometimes)

We'll start by going over the compiler's optimization capabilities.
The compiler is able to get rid of some redundant or inefficient code, but certainly not all of it.
So let's get ready to run some examples, and then consider what the compiler is already doing for you, and where it might need your manual intervention.

```{python}
# Disable SIMD so it doesn't distort or hide certain effects:
import os
os.environ["NUMBA_LOOP_VECTORIZE"] = "0"
from numba import njit
import numpy as np
```

```{python}
#| echo: false
%load_ext book_magics
```

Some of these optimizations you will have to do manually, but some can be automatically by the compiler.

## Compiler optimizations and their limitations

When your code gets compiled, the compiler runs it through a series of optimization passes, with each pass transforming the code in the some way.
The hope is that the cumulative series of transformations will result in code that is more efficient by:

1. Removing redundant code execution, our focus in this chapter.
2. Utilizing the CPU as efficiently as possible, our focus in later chapters.

A key requirement for the optimization passes is that your code continues behave the same way, even as it gets faster.
**To an outside observer, the results of running optimized code must be indistinguishable from running the original code.**

As a result, even if you had a perfect compiler that could identify every single optimization possibility, there would still be optimizations that couldn't be applied.
Only you, the author of the code, can know whether or not it is acceptable to make a change to the code's results and side-effects.
In practice, the compiler may only be looking at a single function: it doesn't have a full view of the whole program, especially when you're just writing a Python extension.

### Some examples of what the compiler can do

Even with this constraint mind, there is still plenty the compiler can do to speed up your code.

#### Precalculating mathematical expressions

If you have a fixed mathematical expression, the compiler can often just execute it at compilation time and replace the expression with the result.
For example, notice how the first function is slower, while the latter two take the same amount of time:

```{python}
@njit
def calc(a, b, c, d, e, f):
    return ((a + b + c) ** d / e) + f

@njit
def calc_fixed():
    return ((1 + 2 + 3) ** 7 / 6.0) + 5

@njit
def just_the_result():
    return 46661.0

# All three functions give the same result:
assert calc_fixed() == calc(1, 2, 3, 7, 6.0, 5)
assert calc_fixed() == just_the_result()
```

Here's how their speed compares:

```{python}
#| echo: false
%%compare_timing

calc(1, 2, 3, 7, 6.0, 5)
calc_fixed()
just_the_result()
```

#### Hoisting expressions out of inner loops

The compiler can also hoist repetitive expressions out of loops, so they're called once instead of many times.
For example, the expression `((n ** 2) * (1.5 / n) + n)` in the `compiler_will_hoist()` is repeated in every iteration, but it also always give the same result.
The compiler is therefore able to move it out of the loop and run it only once, as we do manually in `manually_hoisted()`.
Thus both functions take the same amount of time to run:

```{python}
@njit
def compiler_will_hoist(m, n):
    total = 0
    for i in range(m):
        total = m * ((n ** 2) * (1.5 / n) + n)
    return total

@njit
def manually_hoisted(m, n):
    total = 0
    constant = (n ** 2) * (1.5 / n) + n
    for i in range(m):
        total = m * constant
    return total

# The results are the same:
assert compiler_will_hoist(100, 17) == manually_hoisted(100, 17)
```

Notice they run at the same speed, because the compiler automatically optimizes the first version by hoisting the constant expression:

```{python}
#| echo: false
%%compare_timing

compiler_will_hoist(1_000_000, 3)
manually_hoisted(1_000_000, 3)
```

While both versions have the same speed, the manually hoisted version is arguably clearer about what is going on in the calculation, and the compiler won't always be able to hoist expressions, so doing it manually might still be worth it.

#### Inlining TODO

TODO

There are many other optimizations the compiler can do, with more added or tweaked with every compiler release.

## What the compiler won't do

Sometimes the compiler won't apply seemingly obvious optimizations because the optimization might change your code's behavior; when in doubt, the compiler will err on the side of conservatism.
Some reasons this can happen include:

* Floating point calculations, which we'll talk about in a later chapter.
* Access to memory that is visible outside the function.
* The potential for aliasing.
* Function calls.

By identifying these patterns, you can learn to write code that avoids them.

### Memory accesses can prevent optimizations

If your code is accessing a local variable that is present only in the function, that is clearly something the compiler can safely optimize and modify.
However, the compiler may consider removing writes to memory that is accessible outside your function to be more questionable, because in theory those writes might have side effects.

```{python}
@njit
def local_stack(n):
    acc = 0
    for i in range(n):
        acc += i
    return acc

@njit
def external_memory(n, arr):
    arr[0] = 0
    for i in range(n):
        arr[0] += i
    return arr[0]


# Both functions give the same result:
ZEROS = np.zeros((1, ), dtype=np.float64)
assert local_stack(5) == external_memory(5, ZEROS)
```

Here's how their speed compares:

```{python}
#| echo: false
%%compare_timing

local_stack(1_000_000)
external_memory(1_000_000, ZEROS)
```

TODO this should really be two optimizations with this section having its own example

Of the two versions, the first version is _vastly_ faster, surprisingly so.
If my i7-12700k CPU runs at a clock frequency of approximately 4GHZ, that means it does about 4 CPU cycles per nanosecond.
It should be able to do a few CPU instructions per cycle thanks to instruction-level parallelism, which we'll talk about the in the next chapter.
So let's say it can do 2000 instructions in 100 nanoseconds, just to be generous.
How can it do a million additions with just 2000 instructions?

The LLVM compiler used by Numba is smart enough to figure out that the sum of 0, 1, 2, ..., N is actually `NÃ—(N + 1)/2`, so it just substitutes the faster formula when it can.
So the runtime of the `local_stack()` function will actually be the same regardless of `N`.

Unfortunately, in the case of using an externally given array as the temporary accumulator, the presence of writes to the array likely prevents that particular optimization.
If the array we're using as an accumulator is local to the function, in this case at least the compiler is once again happy to apply the optimization:

```{python}
@njit
def internal_array(n):
    arr = np.zeros((1, ), dtype=np.uint64)
    for i in range(n):
        arr[0] += i
    return arr[0]

assert local_stack(5) == internal_array(5)
```

And if we time it:

```{python}
#| echo: false
%%compare_timing

internal_array(1_000_000)
```

### The possibility of aliasing can prevent optimizations


### Function calls can prevent optimizations

When your code calls another function, the compiler may be uncertain whether that other function has side-effects.
If it does have side-effects, removing calls to that function would change the program's behavior, so the compiler will err on the side of conservatism when optimizing.

In practice, the compiler will often "inline" small functions, essentially copy/pasting their code into the calling function.
This allows the compiler to analyze all the code together, enabling additional optimizations.

Still, calling large complex functions in hot loops can reduce the compiler's ability to optimize your code if the function doesn't get inlined.

### Floating point calculations can prevent optimizations

Some optimizations the compiler will do for integer math won't happen for floating point calculations.
This is covered in a later chapter.

