# Change your results to get more speed

In all the optimization examples so far the optimized code gave the exact same result as the original code.
Sometimes you can achieve additional speedups by returning different results; they might be almost indistinguishable, or they might be noticeably worse, depending how you achieved the speedup.
Whether that tradeoff is worth it depends very much on the actual costs, the actual benefits, and your specific situation.

```{python}
#| echo: false
%load_ext book_magics
```

## Reduce the accuracy of your results

One potential optimizationâ€”thought not the only oneâ€”is relaxing the precision or even accuracy of your code.
Having worse results may be acceptable in many situations:

* The results you're calculating may be more "precise" than the error caused by your input data.
  If that extra precision is slowing your code down, you can reduce your precision and still got results that are faster and still just as accurate.
  A generic library has no way of knowing what level of precision is acceptable, so it can't make this sort of compromise.
* In other cases, less accurate results may also be acceptable.
  For example, the monetary cost of inaccuracy may be much lower than the monetary cost savings from a faster run time.

For my example I'm using data from the realm of public transportation: bus "headways", the time between the arrival of the previous and the next bus.
Headways need to be as low as possible so that passengers can rely on the bus: if the bus arrives every 5 minutes that's a lot better than the bus arriving every 10 minutes.

My goal is to compute the median headway experienced by passengers on my local transit authority's buses:

```{python}
import pandas as pd
import numpy as np

BUS_HEADWAYS = pd.read_parquet(
    "../data/MBTA-bus-2022-05.parquet",
    columns=["headway"]
)["headway"].dropna().values

print("Size:", len(BUS_HEADWAYS))
print("dtype:", BUS_HEADWAYS.dtype)
```

The NumPy library has built-in support for calculating the median, so I could just use that:

```{python}
print("Median (in seconds):", np.median(BUS_HEADWAYS))
```

But I want faster results.
I'm going to implement a custom algorithm that is faster, by customizing it to this particular data.

### Customizing the algorithm

Here's a high-level view of the data:

```{python}
print("Min value (in seconds):", int(BUS_HEADWAYS.min()))
print("Max value (in seconds):", int(BUS_HEADWAYS.max()))
print(
    "99.9th percentile (in seconds):",
    int(np.quantile(BUS_HEADWAYS, 0.999))
)
```

Based on these results, one can build a custom median algorithm using two reasonable assumptions:

**Super-long headways don't have to be very accurate:** As you can see, 99.9% of headways are less than 68 minutes (4067 seconds).
From a passenger experience perspective, the impact of headways is much more meaningful for frequent buses.
For example, a change from a 5-minute to a 15-minute headway can have a major negative impact on the usefulness of a bus route.
On the other hand, a bus route that runs every 60 minutes isn't very different from a bus that runs every 70 minutes; they both suck.

**Reducing accuracy to 15 seconds is reasonable:** The data we have is at a 1-second resolution, but it's unlikely that a headway of 67 seconds really make a difference versus a headway of 66 seconds.
Plus, the arrival and departure of a bus at a stop takes a few seconds, and it's doubtful that the timing data is actually as accurate as it is precise.

I'm going to implement a histogram-based median.
Here's the basic algorithm:

* Create a histogram with one bucket for every 15 second range of headways.
* Iterate over each headway, and increment the count in the corresponding bucket in the histogram.
  For example, if a headway is 37 seconds, it will go in the 30-44 seconds bucket, which is the third bucket.
  Once all headways have been counted in the histogram, if for example the 3rd bucket had a count of 20, that means 20 buses took between 30 and 44 seconds to arrive.
* Then, we add up the size of each bucket in the histogram, from smallest to largest, until we hit 50% of the headways we inspected.
  That 50% point is the median, since 50% of the headways are smaller in value and 50% of the headways were larger in value.

To give a sense of what the histogram looks like, here's a plot:

```{python}
import matplotlib.pyplot as plt

# This isn't actually used in the implementation below, I'm just implementing
# this subset of the algorithm for educational purposes:
def plot_histogram(bus_headways):
    # Histogram by number of quarter-minutes (15 seconds), with a maximum of
    # 500 Ã— 15 = 7,500 seconds.
    by_quarter_minutes = [0] * 500
    for headway in bus_headways:
        number_quarter_minutes = int(headway / 15)
        # If the data is too large, just use the highest bucket. Since 7500 is
        # higher than the 99.9th percentile value, we're underestimating less
        # than 0.1% of the data.
        by_quarter_minutes[min(number_quarter_minutes, 499)] += 1

    # Draw a barchart of the histogram:
    plt.bar(range(500), by_quarter_minutes)
    plt.ylabel("Number of headways for the bucket")
    plt.xlabel("Length of headway, in quarter minutes (15 secs)")
    plt.show()

plot_histogram(BUS_HEADWAYS)
```

Looking at this histogram, I can estimate by eye that the point where half the headways are on one side and half on the other, which is to say the median, is around the 50th bucket, i.e. 50 * 15 seconds, around 750 seconds.

In the full algorithm I'm going to find the median value with code, rather than visually, but the same principle applies: it finds the bucket where half the counted headways are on each side.

Here's the actual implementation:

```{python}
from numba import jit

# numpy.median() is implemented in a compiled language, so in order to have a
# fair comparison I implemented my version in a compiled language. As a
# reminder, the @jit decorator will compile the function the first time it is
# called.
@jit
def histogram_median(bus_headways):
    by_quarter_minutes = [0] * 500
    for headway in bus_headways:
        number_quarter_minutes = int(headway / 15)
        by_quarter_minutes[min(number_quarter_minutes, 499)] += 1

    # Now that we know how many samples there are for every quarter minute
    # index, add up the cumulative counts until we hit 50% of values:
    median_location = len(bus_headways) / 2
    cumulative_samples = 0
    median = 0
    for i in range(len(by_quarter_minutes)):
        cumulative_samples += by_quarter_minutes[i]
        if cumulative_samples >= median_location:
            median = i
            break

    # Convert back to seconds:
    return median * 15.0
```

Comparing the new algorithm to the NumPy `np.median()`, you can see that the results are pretty close.
This is partially luck, but in general it shouldn't be too far off:

```{python}
print("Median:", np.median(BUS_HEADWAYS), "secs")
print("Approximate median:", histogram_median(BUS_HEADWAYS), "secs")
```

And comparing the speed of the two, the custom algorithm is much faster:

```{python}
#| echo: false
%%compare_timing
np.median(BUS_HEADWAYS)
histogram_median(BUS_HEADWAYS)
```

This custom algorithm wouldn't work with generic inputs: it makes many assumptions about the distribution and range of the data.
But for this _specific_ situation and data, it's good enoughâ€”and much faster than a general purpose algorithm.

## Change what your calculation returns

In addition to changing how accurate a function's result is, you can also change what it returns.
Sometimes the format of your function's results is restricting your ability to speed it up.

For my example I will do some simplistic natural language processing.
Jane Austen is the author of five classic novels, and of the novel _Mansfield Park_.
Do Austen's novels talk more about men, more about women, or more about groups of people?
I'm going to look at her first novel, _Northanger Abbey_:

```{python}
import string

with open("../data/northanger_abbey.txt") as f:
    WORDS = [
        word.strip(string.punctuation).lower()
        for word in f.read().split()
    ]
```

Here is a straightforward, though certainly not ideal, algorithm to answer the question:

```{python}
from collections import Counter

def gendered_words_count(words):
    men_words = {"him", "he"}
    women_words = {"her", "she"}
    # This could be groups of people, or people whose gender is unknown; see
    # https://pemberley.com/janeinfo/austheir.html for examples of Austen's use
    # of the singular "they".
    other_words = {"they", "them"}
    counts = {"women": 0, "men": 0, "other": 0}
    for word in words:
        if word in women_words:
            counts["women"] += 1
        elif word in men_words:
            counts["men"] += 1
        elif word in other_words:
            counts["other"] += 1
    return counts
```

One thing to notice about this algorithm is that it returns the exact count of the gendered words:

```{python}
gendered_words_count(WORDS)
```

Providing this information can make it more difficult to speed up the implementation.
So instead of exact counts, for this particular use case it's better to return the relative percentages of the three categories:

```{python}
def gendered_words(words):
    men_words = {"him", "he"}
    women_words = {"her", "she"}
    other_words = {"they", "them"}
    counts = {"women": 0, "men": 0, "other": 0}
    for word in words:
        if word in women_words:
            counts["women"] += 1
        elif word in men_words:
            counts["men"] += 1
        elif word in other_words:
            counts["other"] += 1

    # ðŸ˜Ž Instead of absolute counts, return relative fractions:
    total_count = sum(counts.values())
    return {
        gender: count / total_count
        for (gender, count) in counts.items()
    }

def print_percentage(gender_fractions):
    print({
        gender: f"{round(100 * fraction)}%"
        for (gender, fraction) in gender_fractions.items()
    })
```

Here's what the new results look like:

```{python}
print_percentage(gendered_words(WORDS))
```

Now, this doesn't actually make the function any faster.
But in the next section you'll see how this change will enable switching to a faster, albeit less accurate, implementation.

## Look at less data

How could you speed up `gendered_words()`?
There are some ways you could probably get rid of wasted work (all those dictionary writes will probably be faster if they were replaced with local variables).
But that's not the focus of this chapter.

Another way to speed up this calculation is to run the same algorithm, but on a random sample of the words in the novel, say 5,000 words chosen without replacement.

```{python}
import random

def gendered_sampled_words(words):
    sample = random.sample(words, 5000)
    return gendered_words(sample)
```

How does the sampled implementation compare to running the algorithm on the complete list of words?

```{python}
print("ACCURATE", end=" ")
print_percentage(gendered_words(WORDS))
print()
for i in range(1, 6):
    print("SAMPLE", i, end=" ")
    print_percentage(gendered_sampled_words(WORDS))
```

As you can see, the numbers the sampled version returns are not identical, but the big picture message doesn't change: based on this rather simplistic algorithm, Austen spends more time talking about individual women than about men, groups of people, or people of unknown gender.

And the sampled version is much faster:

```{python}
#| echo: false
%%compare_timing
gendered_words(WORDS)
gendered_sampled_words(WORDS)
```

Whether or not sampling works for your particular problem is something you will need to decide.
