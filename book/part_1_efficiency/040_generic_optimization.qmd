# Get rid of repetitive and wasteful code âžŠ

Once you've picked a good algorithm, you can make your implementation more efficient by removing repetitive, wasteful code.
The code's scalability won't change if you're still using the same algorithm, but it will still run faster compared to the original version.

```{python}
#| echo: false
%load_ext book_magics
```

## Focus your efforts on inner loops

As far as speed of computation goes, the three most important locations in your data processing code are loops, loops, and loops.
A basic assumption in this book is that the code you're writing is used to process relatively large amounts of data.
That means you will almost always end up writing or calling functions with the following basic structure:

```{python}
#| eval: false
# This only gets called once:
do_some_setup()

for item in large_amount_of_data:
    # This gets called a lot:
    do_something(item)

# This only gets called once:
do_some_cleanup()
```

Given large amounts of data, and data structures like NumPy `ndarray`s that are designed for fast access from low-level languages, the bulk of your processing will happen inside the loop.
Code that runs a lot is sometimes called "hot" code.
You can speed up code running in a loop by:

1. Reducing the number of loop iterations. Perhaps there's data you don't need to process at all?
2. Speeding up the work you repeatedly do inside the loop, the code you will be calling over and over again.

If you have nested loops, the innermost loop will have the most iterations, and therefore is the most impactful place to fix inefficient code.
For example:

```python
for i in range(N):
    # outer_loop_function() will run N times:
    outer_loop_function()

    for j in range(M):
        # inner_loop_function() will run NÂ·M times:
        inner_loop_function()
```

## Don't process data you don't need to

The fastest code is code you don't run at all.
So if you can avoid processing data altogether,  that can provide significant speed-ups.

As an example, let's implement the Sieve of Eratosthenes, a classic algorithm for finding prime numbers.
It works by iterating over all whole numbers in a range, crossing out multiples of 2 (4, 6, 8, ...), then multiples of 3 (6, 9, 12, ...), then multiples of 5 (10, 15, 20, ...), and so on.

```{python}
def find_primes_1(up_to_value):
    # Create a list of booleans, with default value being True:
    is_prime = [True] * up_to_value
    # 0 and 1 are not prime:
    is_prime[0] = False
    is_prime[1] = False

    # For every number starting with 2:
    for i in range(2, up_to_value):
        # If we're already checked this number, we're done:
        if not is_prime[i]:
            continue
        # Otherwise, mark all multiples of i as not prime, since they're
        # multiples, e.g. 4, 6, 8 are multiples of 2 so not prime.
        for j in range(i*2, up_to_value, i):
            is_prime[j] = False

    return is_prime

assert find_primes_1(6) == [
    False, False, True, True, False, True
]
```

Can we make this code any faster?
One thing to notice is that we're checking certain numbers twice.
When we rule out multiples of 5, we're ruling out `5 * 2 = 10` and `5 * 3 = 15` and `5 * 4 = 20`.
But we've already ruled them out in previous rounds, since for example `2 * 5 = 10` would have been previously ruled out as a multiple of 2.

That means we can tweak our `j` loop to start at a later point (`i * i` instead of `i * 2`) to prevent doing duplicate work:

```{python}
def find_primes_2(up_to_value):
    is_prime = [True] * up_to_value
    is_prime[0] = False
    is_prime[1] = False

    for i in range(2, up_to_value):
        if not is_prime[i]:
            continue
        # ðŸ˜Ž Anything smaller than i*i was already processed in an earlier
        # iteration, so no need to redo it.
        for j in range(i * i, up_to_value, i):
            is_prime[j] = False

    return is_prime

assert find_primes_1(100) == find_primes_2(100)
```

This small tweak makes our algorithm run noticeably faster:

```{python}
#| echo: false
%%compare_timing
find_primes_1(5_000)
find_primes_2(5_000)
```

## Move `if`s up and `for`s down

This principle is [due to Alex Kladov](https://matklad.github.io/2023/11/15/push-ifs-up-and-fors-down.html).
For this chapter's purposes you can just focus on one aspect of this principle: when you have an `if` inside a `for` loop, it's going to be run over and over again.
So if there's a way to move that out of the `for` loop, it's possible your code will be faster.

Continuing the same example, in our loop above we do an `if is_prime[i]: continue` check on all values of `i`.
That includes even numbers like 8 or 212, even though we know that any even number larger than 2 isn't prime.
Instead of doing this check in every single iteration, we can split processing into two loops, one for odd numbers and one for even numbers.
The total number of iterations doesn't change, it's still `up_to_value`, but each iteration does less work:

```{python}
def find_primes_3(up_to_value):
    is_prime = [True] * up_to_value
    is_prime[0] = False
    is_prime[1] = False

    # ðŸ˜Ž Unconditionally rule out all even numbers larger than 2:
    for i in range(4, up_to_value, 2):
        is_prime[i] = False

    # ðŸ˜Ž Now only check odd numbers:
    for i in range(3, up_to_value, 2):
        if not is_prime[i]:
            continue
        for j in range(i*i, up_to_value, i):
            is_prime[j] = False

    return is_prime

assert find_primes_1(100) == find_primes_3(100)
```

This version is even faster:

```{python}
#| echo: false
%%compare_timing
find_primes_2(5_000)
find_primes_3(5_000)
```

## Avoid duplicate calculations

Now that we're only iterating over odd numbers in the second loop, we can apply another optimization in the inner-most `for j` loop.
Let's say we're ruling out multiples of 3.
We rule out the sequence `9`, `12`, `15`, `18`, and so on up to `up_to_value`.
Notice that every second number is even, because we're adding up pairs of odd numbers.
And those even numbers have already been marked as non-prime.

So we can optimize the code even further by only checking every second multiple:

```{python}
def find_primes_4(up_to_value):
    is_prime = [True] * up_to_value
    is_prime[0] = False
    is_prime[1] = False

    for i in range(4, up_to_value, 2):
        is_prime[i] = False

    for i in range(3, up_to_value, 2):
        if not is_prime[i]:
            continue
        # ðŸ˜Ž Only mark multiples that are odd (skipping ahead 2 * i in each
        # iteration), since even numbers have already been handled:
        for j in range(i*i, up_to_value, 2 * i):
            is_prime[j] = False

    return is_prime

assert find_primes_1(100) == find_primes_4(100)
```

Once again, we've sped up the code:

```{python}
#| echo: false
%%compare_timing
find_primes_1(5_000)
find_primes_2(5_000)
find_primes_3(5_000)
find_primes_4(5_000)
```

## Handle edge-cases in advance

Some branches are there to handle a rare edge case.
If that branch is inside a loop, calling the same code over and over is a waste of resources.
If you can, handling edge cases separately can speed up your code.

Once again we'll look at the example of calculating a 7-day moving average.
But for the first 6 days, we won't have sufficient previous values.
In the last chapter we skipped calculating them at all, and just started from the 7th day.
But if we do want some values for those first 6 days.

There are various strategies to deal with this missing data, and in this case we'll repeat the value in the first day:

```{python}
def moving_average(timeseries):
    result = []
    first_day = timeseries[0]
    for i in range(len(timeseries)):
        total = 0
        if i < 6:
            # Fill in missing values for first few days:
            total += (6 - i) * first_day
        for j in range(max(i - 6, 0), i + 1):
            total += timeseries[j]
        result.append(total / 7)
    return result
```

A performance problem with this code is the `if i < 6`: it's being called on every single iteration of the loop, even though it's only needed for the first 6 iterations.

Instead we can handle the first 6 days separately, with their own code path.
That means the rest of the calculations can safely assume that 7 days of data are always available:

```{python}
def moving_average_2(timeseries):
    result = []

    # ðŸ˜Ž Handle the first 6 days on their own:
    first_day = timeseries[0]
    for i in range(6):
        total = first_day * (6 - i)
        # Fill in missing values for first few days:
        for j in range(i + 1):
            total += timeseries[j]
        result.append(total / 7)

    # ðŸ˜Ž Then handle the bulk of the data, without the extra branch we
    # previously had:
    for i in range(6, len(timeseries)):
        total = 0
        for j in range(i - 6, i + 1):
            total += timeseries[j]
        result.append(total / 7)
    return result

import random
DATA = [random.random() for _ in range(10_000)]

assert moving_average(DATA) == moving_average_2(DATA)
```

This version is faster, by removing work from the main loop:

```{python}
#| echo: false
%%compare_timing
moving_average(DATA)
moving_average_2(DATA)
```
