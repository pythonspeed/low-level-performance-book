# Why compilers have a hard time optimizing floating point math

If you add two integers, the order doesn't matter; likewise for multiplication.
As a result, the compiler can utilize this flexibility to rearrange calculations to help with a variety of optimizations:

* Hoisting a calculation out of a loop, so it's only done once.
* Instruction-level parallelism can benefit from reduced data dependencies.
* SIMD auto-vectorization may be easier with a different calculation order.

Unfortunately, floating-point calculations give different results if they are run in a different order.
For example:

```{python}
import numpy as np
from numba import njit

@njit
def add_in_order(start, middle, end):
    result = start
    for _ in range(100_000_000):
        result += middle
    result += end
    return result

LARGE = np.float32(100_000_000_000)
SMALL = np.float32(0.1)

assert add_in_order(0, SMALL, LARGE) != add_in_order(LARGE, SMALL, 0)
```

When we start from a large number, adding a small number gives you the large number again.
So if we do that in a loop we are still left with the original number:

```{python}
assert LARGE + SMALL == LARGE
assert add_in_order(LARGE, SMALL, 0) == LARGE
```

But when we start from zero, the accumulation of small numbers eventually becomes a big enough number that adding to the large number succeeds.

```{python}
assert add_in_order(0, SMALL, LARGE) > LARGE
```

Because of this lack of flexibility in re-ordering floating point operations, and more broadly the ways in which floating point is complex to deal with, the compiler can do less.
And that means fewer optimizations and slower code.
In the next few chapters, then, we'll discuss ways to speed up your floating point calculations.
