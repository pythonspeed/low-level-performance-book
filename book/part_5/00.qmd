# Why compilers have a hard time optimizing floating point math

If you add two integers, the order doesn't matter; likewise for multiplication.
As a result, the compiler can utilize this flexibility to rearrange calculations to help with a variety of optimizations:

* Hoisting a calculation out of a loop, so it's only done once.
* Instruction-level parallelism can benefit from reduced data dependencies.
* SIMD auto-vectorization may be easier with a different calculation order.

Unfortunately, floating-point calculations give different results if they are run in a different order.
For example:

```{python}
import numpy as np
from numba import njit

@njit
def add_in_order(start, middle, end):
    result = start
    for _ in range(100_000_000):
        result += middle
    result += end
    return result

LARGE = np.float32(100_000_000_000)
SMALL = np.float32(0.1)

assert (
    add_in_order(0, SMALL, LARGE) !=
    add_in_order(LARGE, SMALL, 0)
)
```

When we start from a large number, adding a small number gives you the large number again.
So if we do that in a loop we are still left with the original number:

```{python}
assert LARGE + SMALL == LARGE
assert add_in_order(LARGE, SMALL, 0) == LARGE
```

But when we start from zero, the accumulation of small numbers eventually becomes a big enough number that adding it to the large number gives us a different value.

```{python}
assert SMALL + SMALL > SMALL
assert add_in_order(0, SMALL, LARGE) > LARGE
```

Recall that the compiler will only apply optimizations if it knows the result will be _identical_ to the original code.
Clearly, re-ordering floating point calculations does not always give identical results!
As a result, the compiler can apply fewer optimizations to floating-point calculations than it would to equivalent integer calculations, resulting in slower code.

Keeping this problem in mind, in the next few chapters we'll discuss ways to speed up your floating point calculations.
