# Replace floats with higher-precision integer operations

Another approach to the compiler's difficulty with optimizing floating-point is to avoid floating-point altogether.
If your input data has a relatively small range, for example `uint8` or `int16`, you can use a larger range integer like `uint32` or `int64` to do the calculations.
In some cases the resulting code will be easier for the compiler to optimize.
The calculation may give slightly different results, but quite often the difference won't be meaningful, or will be worth it given the speedup.

Let's look at an example; we have an image of a cell:

```{python}
#| echo: false
%load_ext book_magics
```
```{python}
import numpy as np
from numba import njit
from skimage import io
from skimage.data import cell

IMAGE = cell()
assert IMAGE.dtype == np.uint8
```

```{python}
#| echo: false
%display_image IMAGE
```

We want to threshold the image, separating the image into two parts, the bright part and the dark part.
This is a useful operation to find the shapes of objects, in this the case the cell.
A naive algorithm for thresholding is comparing each pixel to the mean of the image.
Here's how we'd do it with NumPy:

```{python}
def mean_threshold_numpy(image):
    threshold = image.mean()
    result = (image > threshold).astype(np.uint8)
    result *= 255
    return result

THRESHOLD_NUMPY = mean_threshold_numpy(IMAGE)
```

And here's what the result looks like:

```{python}
#| echo: false
%display_image THRESHOLD_NUMPY
```

Our goal is to make this code faster.
As a first pass, we can write an implementation using Numba.
Because we're accumulating a large number of `uint8`s to create the total, we need to store it in a larger type, so we'll use a `float64`:

```{python}
from numba import float64

@njit
def mean_threshold_1(image):
    # Image is uint8, so we need an accumulator that can hold larger values:
    total = float64(0.0)
    for y in range(image.shape[0]):
        for x in range(image.shape[1]):
            total += image[y, x]
    threshold = total / image.size
    result = np.empty(image.shape, dtype=np.uint8)
    for y in range(image.shape[0]):
        for x in range(image.shape[1]):
            result[y, x] = (image[y, x] >= threshold) * 255
    return result

THRESHOLD_1 = mean_threshold_1(IMAGE)
# Make sure the result is basically the same, with less than 0.1% of pixels being
# different:
assert ((THRESHOLD_NUMPY ^ THRESHOLD_1) > 0).sum() < IMAGE.size / 1000
```

Unfortunately this new version isn't any faster:

```{python}
#| echo: false
%%compare_timing --measure=instructions
mean_threshold_numpy(IMAGE)
mean_threshold_1(IMAGE)
```

But what if instead of accumulating a `float64`, we accumulate a `uint64`?

```{python}
from numba import uint64, uint8

@njit
def mean_threshold_2(image):
    total = uint64(0)
    for y in range(image.shape[0]):
        for x in range(image.shape[1]):
            total += image[y, x]
    threshold = total / image.size
    result = np.empty(image.shape, dtype=np.uint8)
    for y in range(image.shape[0]):
        for x in range(image.shape[1]):
            result[y, x] = (image[y, x] >= threshold) * 255
    return result

THRESHOLD_2 = mean_threshold_2(IMAGE)
# Make sure the result is basically the same, with less than 0.1% of pixels being
# different:
assert ((THRESHOLD_NUMPY ^ THRESHOLD_2) > 0).sum() < IMAGE.size / 1000
```

This version is faster!

```{python}
#| echo: false
%%compare_timing --measure=instructions
mean_threshold_numpy(IMAGE)
mean_threshold_2(IMAGE)
```

But we can do better.
Notice above we're comparing a `float64` to the pixels, which are `uint8`.
Somewhere in there, then, the compiler is likely converting both values to the same type, but it's unclear how efficient that conversion is.
So let's do it ourselves:

```{python}
@njit
def mean_threshold_3(image):
    total = uint64(0)
    for y in range(image.shape[0]):
        for x in range(image.shape[1]):
            total += image[y, x]

    # Convert mean to uint8 before we do the comparison:
    threshold = uint8(np.round(total / image.size))
    result = np.empty(image.shape, dtype=np.uint8)
    for y in range(image.shape[0]):
        for x in range(image.shape[1]):
            result[y, x] = (image[y, x] >= threshold) * 255
    return result

THRESHOLD_3 = mean_threshold_3(IMAGE)
# Make sure the result is basically the same, with less than 0.1% of pixels being
# different:
assert ((THRESHOLD_NUMPY ^ THRESHOLD_3) > 0).sum() < IMAGE.size / 1000
```

This new version is significantly faster:

```{python}
#| echo: false
%%compare_timing --measure=instructions
mean_threshold_numpy(IMAGE)
mean_threshold_3(IMAGE)
```

And the result is basically identical:

```{python}
#| echo: false
%display_image THRESHOLD_3
```
