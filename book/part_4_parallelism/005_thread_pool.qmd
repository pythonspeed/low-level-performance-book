# Use a thread pool

To run code on multiple CPU cores, you need to either use multiple threads or multiple processes.
Threads, running within the same process, have a much easier time sharing data than different processes would, so they're what I'll be covering in this book.

```{python}
#| echo: false
%load_ext book_magics
```

## Use a thread pool

You could launch a new thread for each task you need to do.
For example, here we have a thread per chunk; if there are 1000 chunks, this code will launch 1000 threads:

```{python}
from threading import Thread
import numpy as np
from numba import jit

# ðŸ˜Ž `nogil=True` enables multiple threads to call this in parallel; I'll cover
# the GIL in a later chapter.
@jit(nogil=True)
def sum_of_squares(arr):
    result = 0
    for i in range(len(arr)):
        result += arr[i] ** 2
    return result


def parallel_launch(arr, num_chunks):
    results = []
    threads = []
    # Split the array into a number of chunks; this API won't copy the
    # data, so it's very fast:
    chunks = np.array_split(arr, num_chunks)

    # Launch a thread for each chunk:
    for chunk in chunks:
        thread = Thread(target=lambda: results.append(sum_of_squares(chunk)))
        thread.start()
        threads.append(thread)

    # Wait for all threads to exit:
    for thread in threads:
        thread.join()
    assert len(results) == len(chunks)
    return sum(results)

DATA = np.random.randint(0, 10, 1_000_000, dtype=np.int64)
assert sum_of_squares(DATA) == parallel_launch(DATA, 4)
```

There are multiple problems with this approach.
First, launching a new thread takes time, so it adds extra overhead to every single task you want to run.

Second, you may end up with too many threads.
If your CPU has 4 cores, that means on the _hardware_ level you can only run 4 threads at once across all processes.
So that might be two threads in process A and single-threaded processes B and C.
If you have more than 4 threads in total running across all processes, the operating system's scheduler will run a thread for a short time, then switch to a different thread.

Switching between threads has overhead, for example they might read different memory and so need to fetch that data into the local cache.
If you have 4 CPU cores and 100 threads, you're only going to have 4 of them running at any given time, so all those extra threads don't speed things up... and the switching overhead will actually slow you down.

Here's how fast our parallel implementation is:

```{python}
#| echo: false
%%compare_timing
sum_of_squares(DATA)
parallel_launch(DATA, 4)
```

Instead of launching a thread per task, a better solution is to use a pool of threads with a fixed size.
You don't have the overhead of launching a thread per task, and you can limit the number of threads so they're not competing for a limited number of cores.

The Python standard library has a couple of thread pool implementations, with the recommended one being `concurrent.futures.ThreadPoolExecutor`.
Here's a version of parallel summing that uses this thread pool:

```{python}
from concurrent.futures import ThreadPoolExecutor

# I'm going to reuse the thread pool so the cost of starting it up doesn't
# impact the benchmarks. This particular number of workers assumes your CPU has
# at least 4 cores, otherwise you'll want a smaller value.
THREAD_POOL = ThreadPoolExecutor(max_workers=4)

def parallel_pool(arr, num_chunks):
    chunks = np.array_split(arr, num_chunks)
    # This calls sum_of_square(chunk) for every chunk in chunks; the calls will
    # be distributed across the threads of the thread pool, with only 4 running
    # in parallel. It returns an iterable of results; the order may not match
    # the order of the chunks!
    results = THREAD_POOL.map(sum_of_squares, chunks)
    return sum(results)

assert sum_of_squares(DATA) == parallel_pool(DATA, 4)
```

This implementation of parallelism has less overhead:

```{python}
#| echo: false
%%compare_timing
sum_of_squares(DATA)
parallel_launch(DATA, 4)
parallel_pool(DATA, 4)
```
