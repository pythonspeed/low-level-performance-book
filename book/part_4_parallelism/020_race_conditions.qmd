# Avoid race conditions

If parallelism means you get wrong results, that extra speed is worse than useless.
And unfortunately, parallelism significantly increases the opportunities for buggy code.
In particular, as soon as you have more than one thread writing to the same address in memory, you will start having to worry about data races that can corrupt your data and cause undefined behavior.

```{python}
#| echo: false
%load_ext book_magics
```

## An example

I have a large NumPy array, and I want to calculate the sum of squares and sum of cubes:

```{python}
import numpy as np
from numba import jit

DATA = np.random.randint(0, 10, 1_000_000, dtype=np.int64)

# ðŸ˜Ž `nogil=True` enables multiple threads to call this in parallel; I'll cover
# the GIL in a later chapter.
@jit(nogil=True)
def squares_and_cubes(arr):
    square = 0
    cube = 0
    for i in range(len(arr)):
        val = arr[i]
        square += val ** 2
        cube += val ** 3
    return np.array([square, cube], dtype=arr.dtype)

assert np.array_equal(
    squares_and_cubes(np.array([1, 2, 3], dtype=np.int64)),
    np.array([14, 36], dtype=np.int64)
)
```

To speed this up, I am going to use a thread pool to sum different chunks in parallel.
You can submit tasks to a thread pool to be run in one of the threads, and the operating system will (hopefully) spread those threads out so they run on different CPU cores.
Hopefully this will return results faster.

```{python}
import os
from concurrent.futures import ThreadPoolExecutor

# I set a number of threads equal to the number of CPU cores.
THREAD_POOL = ThreadPoolExecutor(max_workers=os.cpu_count())

def parallel(arr, num_chunks=10):
    result = np.array([0, 0], dtype=arr.dtype)

    # Split the array into a number of chunks; this API won't copy the
    # data, so it's very fast:
    chunks = np.array_split(arr, num_chunks)

    # ðŸ˜Ž For each chunk, process it in a thread in the thread pool by calling
    # _bad_process_chunk(result, chunk).
    for _ in THREAD_POOL.map(
        _bad_process_chunk, [result] * num_chunks, chunks
    ):
        # We ignore the results, foolishly relying on _bad_process_chunk to
        # mutate shared global state, the `result` array.
        pass

    return result

@jit(nogil=True)
def _bad_process_chunk(result, chunk):
    for i in range(len(chunk)):
        val = chunk[i]
        # ðŸ˜± This is a bug, leading to incorrect results:
        result[0] = result[0] + val ** 2
        result[1] = result[1] + val ** 3
```

Surely this code will work!

```{python}
print("Single-threaded:", list(squares_and_cubes(DATA)))
print("Parallel:       ", list(parallel((DATA))))
```

This code has a bug.
Since I rerun the code every time I regenerate the book, it might or might not be visible in the output above; the bug is inconsistent.
This is common with concurrency bugs, making them hard to reproduce.

If I run the calculation multiple times, eventually I'll get an inconsistent result:

```{python}
unique_results = set()
while len(unique_results) < 2:
    unique_results.add(tuple(parallel(DATA)))
print("Different results found:", unique_results)
```

There are two different results for the same calculation.
This is bad.

## Avoid data races

The problem with the parallel code above is that it writes to a single location in memory, `result`, from multiple threads.
Consider:

```python
result[0] = result[0] + val ** 2
```

Here's what may happen:

1. Thread 1 reads `result[0]`, stores it in what I'll call `t1_result`.
2. Thread 2 reads `result[0]`, stores it in what I'll call `t2_result`.
3. Thread 1 sets `result[0]` to `t1_result` plus its local `val ** 2`.
4. Thread 2 sets `result[0]` to `t3_result` plus its local `val ** 2`.

The end result is that all the work from thread 1 gets wiped out and never recorded.

This is a data race: you get inconsistent results depending which thread wins.

### Solution #1: Protect access to shared variables with a lock

A lock lets you ensure only one thread at a time is calling some part of the code:

```python
import threading

lock = threading.Lock()
with lock:
    # ... only one thread can call this code at a time ...
```
Since the problem of data races is caused by multiple threads accessing the same data, limiting access to one thread at a time is a potential solution.
But you do have to make sure you're protecting the right part of the code.

Protecting just the writes with a lock is insufficient.
You can still have the exact same data race described above if you do:

```python
# ...
# ðŸ˜± Race condition still exists:
old_result = result[0]
with lock:
    result[0] = old_result + val ** 2
# ...
```

Nor does it help to protect the read and write with separate locking:

```python
# ...
# ðŸ˜± Race condition still exists:
with lock:
    old_result = result[0]
with lock:
    result[0] = old_result + val ** 2
# ...
```

Rather, you need to figure out which part of the change to the shared data needs to be atomic, i.e. happening at once, from the perspective of other threads.
All of that needs to be protected by a lock.

From a performance perspective:

* To maximize parallelism, the amount of work done while the lock is held should be minimized.
* To minimize overhead, you should avoid acquiring the lock frequently.

Here is how you can use a lock to implement a correct and fast parallel algorithm:

```{python}
from threading import Lock

def parallel_locking(arr, num_chunks=10):
    result = np.array([0, 0], dtype=arr.dtype)
    lock = Lock()

    def process_chunk(result, chunk):
        # Rather than acquiring the lock for every value, I first create a
        # partial sum and then only hold the lock once, at the end:
        square, cube = squares_and_cubes(chunk)
        # Acquire the lock only once per chunk, and covering both the reads and
        # the writes so there is only one atomic operation from the perspective
        # of other threads:
        with lock:
            result[0] += square
            result[1] += cube

    chunks = np.array_split(arr, num_chunks)

    for _ in THREAD_POOL.map(process_chunk, [result] * num_chunks, chunks):
        pass

    return result

assert np.array_equal(
    squares_and_cubes(DATA),
    parallel_locking(DATA)
)
```

Now that the implementation is correct, it's also worth measuring its speed:

```{python}
#| echo: false
%%compare_throughput --unit=Numbers:len(DATA)
squares_and_cubes(DATA)
parallel_locking(DATA)
```

The speedup depends, among other things, on how many cores the CPU has.

### Solution #2: Avoid writing to shared data from multiple threads

The only shared data in this code is the final `result`.
Instead of adding the partial sums to `result` in the pool of threads, another option is to do so in the main thread.
Since the number of partial results is small, doing this is fast, and you don't have to worry about locks since only one thread is doing the reading and writing.

This is a variation of the map/reduce pattern: the threads map a function over chunks of data without mutating (modifying) any shared state, and then later the partial results are reduced (combined) into a final result.
Reducing can happen in a single thread, as it is in this example, or it too can be run in a thread pool.

```{python}
def parallel_mapreduce(arr, num_chunks=10):
    chunks = np.array_split(arr, num_chunks)
    # All modifications to `result` only happen in a single thread:
    result = np.array([0, 0], dtype=arr.dtype)

    # 1. Map over the chunks:
    partial_results = THREAD_POOL.map(squares_and_cubes, chunks)

    # 2. Reduce into the final result:
    for (part_square, part_cube) in partial_results:
        result[0] += part_square
        result[1] += part_cube

    return result

assert np.array_equal(
    squares_and_cubes(DATA),
    parallel_mapreduce(DATA)
)
```

And again, the parallel version is faster:

```{python}
#| echo: false
%%compare_throughput --unit=Numbers:len(DATA)
squares_and_cubes(DATA)
parallel_mapreduce(DATA)
```

### Solution #3: Atomic variables

Sometimes you can use atomic variables, which can be safely used across thread.
This is beyond the scope of this book; I recommend reading [_Rust Atomics and Locks_ by Mara Bos](https://marabos.nl/atomics/).
Both Rust and C have borrowed the C++ atomics model, so the underlying concepts will translate across these languages.

## Use Rust

If you use the Rust programming language, the compiler will enforce a rule that for each piece of data, you can have just one of the following:

1. A single writable (mutable) reference.
2. One or more read-only references.

The result is that if you were to try to write the original data race example code in Rust, the compiler will refuse to compile your code.
You will be forced to use one of the two strategies needed to solve the problem.

Rust doesn't solve all such problems, so for example the bad lock usages above might compile depending on the kind of data[^copy].
But it does catch the worst kind of problems, the ones that cause undefined behavior, and it forces you to at least try to fix these problems.

[^copy]: Anything with the `Copy` trait, or if you explicitly do a `clone()` anything with the `Clone` trait.

## Avoid global side-effects

TODO
