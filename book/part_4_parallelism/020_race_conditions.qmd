# Avoid race conditions

If parallelism means you get wrong results, that extra speed is worse than useless.
And unfortunately, parallelism significantly increases the opportunities for buggy code.
In particular, as soon as you have more than one thread writing to the same address in memory, you will start having to worry about nondeterministic behavior that can corrupt your data or even cause undefined behavior.

```{python}
#| echo: false
%load_ext book_magics
```

## An example

I have a large NumPy array, and I want to calculate the sum of squares and sum of cubes:

```{python}
import numpy as np
from numba import jit

DATA = np.random.randint(0, 10, 1_000_000, dtype=np.int64)

# ðŸ˜Ž `nogil=True` enables multiple threads to call this in parallel; I'll cover
# the GIL in a later chapter.
@jit(nogil=True)
def squares_and_cubes(arr):
    square = 0
    cube = 0
    for i in range(len(arr)):
        val = arr[i]
        square += val ** 2
        cube += val ** 3
    return np.array([square, cube], dtype=arr.dtype)

assert np.array_equal(
    squares_and_cubes(np.array([1, 2, 3], dtype=np.int64)),
    np.array([14, 36], dtype=np.int64)
)
```

To speed this up, I am going to use a thread pool to sum different chunks in parallel.
You can submit tasks to a thread pool to be run in one of the threads, and the operating system will (hopefully) spread those threads out so they run on different CPU cores.
Hopefully this will return results faster.

```{python}
import os
from concurrent.futures import ThreadPoolExecutor

# I set a number of threads equal to the number of CPU cores.
THREAD_POOL = ThreadPoolExecutor(max_workers=os.cpu_count())

def parallel(arr, num_chunks=10):
    result = np.array([0, 0], dtype=arr.dtype)

    # Split the array into a number of chunks; this API won't copy the
    # data, so it's very fast:
    chunks = np.array_split(arr, num_chunks)

    # ðŸ˜Ž For each chunk, process it in a thread in the thread pool by calling
    # _bad_process_chunk(result, chunk).
    for _ in THREAD_POOL.map(
        _bad_process_chunk, [result] * num_chunks, chunks
    ):
        # We ignore the results, foolishly relying on _bad_process_chunk to
        # mutate shared global state, the `result` array.
        pass

    return result

@jit(nogil=True)
def _bad_process_chunk(result, chunk):
    for i in range(len(chunk)):
        val = chunk[i]
        # ðŸ˜± This is a bug, leading to incorrect results:
        result[0] = result[0] + val ** 2
        result[1] = result[1] + val ** 3
```

Surely this code will work!

```{python}
print("Single-threaded:", list(squares_and_cubes(DATA)))
print("Parallel:       ", list(parallel((DATA))))
```

This code has a bug.
Since I rerun the code every time I regenerate the book, it might or might not be visible in the output above; the bug is inconsistent.
This is common with concurrency bugs, making them hard to reproduce.

If I run the calculation multiple times, eventually I'll get an inconsistent result:

```{python}
unique_results = set()
while len(unique_results) < 2:
    unique_results.add(tuple(parallel(DATA)))
print("Different results found:", unique_results)
```

There are two different results for the same calculation.
This is bad.

## Avoid data races

The problem with the parallel code above is that it writes to a single location in memory, `result`, from multiple threads.
Consider:

```python
result[0] = result[0] + val ** 2
```

Here's what may happen:

1. Thread 1 reads `result[0]`, stores it in what I'll call `t1_result`.
2. Thread 2 reads `result[0]`, stores it in what I'll call `t2_result`.
3. Thread 1 sets `result[0]` to `t1_result` plus its local `val ** 2`.
4. Thread 2 sets `result[0]` to `t3_result` plus its local `val ** 2`.

The end result is that all the work from thread 1 gets wiped out and never recorded.

This is a data race: you get inconsistent results depending which thread wins.

### Solution #1: Protect access to shared variables with a lock

A lock lets you ensure only one thread at a time is calling some part of the code:

```python
import threading

lock = threading.Lock()
with lock:
    # ... only one thread can call this code at a time ...
```
Since the problem of data races is caused by multiple threads accessing the same data, limiting access to one thread at a time is a potential solution.
But you do have to make sure you're protecting the right part of the code.

Protecting just the writes with a lock is insufficient.
You can still have the exact same data race described above if you do:

```python
# ...
# ðŸ˜± Race condition still exists:
old_result = result[0]
with lock:
    result[0] = old_result + val ** 2
# ...
```

Nor does it help to protect the read and write with separate locking:

```python
# ...
# ðŸ˜± Race condition still exists:
with lock:
    old_result = result[0]
with lock:
    result[0] = old_result + val ** 2
# ...
```

Rather, you need to figure out which part of the change to the shared data needs to be atomic, i.e. happening at once, from the perspective of other threads.
All of that needs to be protected by a lock.

From a performance perspective:

* To maximize parallelism, the amount of work done while the lock is held should be minimized.
* To minimize overhead, you should avoid acquiring the lock frequently.

Here is how you can use a lock to implement a correct and fast parallel algorithm:

```{python}
from threading import Lock

def parallel_locking(arr, num_chunks=10):
    result = np.array([0, 0], dtype=arr.dtype)
    lock = Lock()

    def process_chunk(result, chunk):
        # Rather than acquiring the lock for every value, I first create a
        # partial sum and then only hold the lock once, at the end:
        square, cube = squares_and_cubes(chunk)
        # Acquire the lock only once per chunk, and covering both the reads and
        # the writes so there is only one atomic operation from the perspective
        # of other threads:
        with lock:
            result[0] += square
            result[1] += cube

    chunks = np.array_split(arr, num_chunks)

    for _ in THREAD_POOL.map(process_chunk, [result] * num_chunks, chunks):
        pass

    return result

assert np.array_equal(
    squares_and_cubes(DATA),
    parallel_locking(DATA)
)
```

Now that the implementation is correct, it's also worth measuring its speed:

```{python}
#| echo: false
%%compare_throughput --unit=Numbers:len(DATA)
squares_and_cubes(DATA)
parallel_locking(DATA)
```

The speedup depends, among other things, on how many cores the CPU has.

### Solution #2: Avoid writing to shared data from multiple threads

The only shared data in this code is the final `result`.
Instead of adding the partial sums to `result` in the pool of threads, another option is to do so in the main thread.
Since the number of partial results is small, doing this is fast, and you don't have to worry about locks since only one thread is doing the reading and writing.

This is a variation of the map/reduce pattern: the threads map a function over chunks of data without mutating (modifying) any shared state, and then later the partial results are reduced (combined) into a final result.
Reducing can happen in a single thread, as it is in this example, or it too can be run in a thread pool.

```{python}
def parallel_mapreduce(arr, num_chunks=10):
    chunks = np.array_split(arr, num_chunks)
    # All modifications to `result` only happen in a single thread:
    result = np.array([0, 0], dtype=arr.dtype)

    # 1. Map over the chunks:
    partial_results = THREAD_POOL.map(squares_and_cubes, chunks)

    # 2. Reduce into the final result:
    for (part_square, part_cube) in partial_results:
        result[0] += part_square
        result[1] += part_cube

    return result

assert np.array_equal(
    squares_and_cubes(DATA),
    parallel_mapreduce(DATA)
)
```

And again, the parallel version is faster:

```{python}
#| echo: false
%%compare_throughput --unit=Numbers:len(DATA)
squares_and_cubes(DATA)
parallel_mapreduce(DATA)
```

### Solution #3: Atomic variables

Sometimes you can use atomic variables, which can be safely used across thread.
This is beyond the scope of this book; I recommend reading [_Rust Atomics and Locks_ by Mara Bos](https://marabos.nl/atomics/).
Both Rust and C have borrowed the C++ atomics model, so the underlying concepts will translate across these languages.

## Use Rust to reduce the risk of race conditions

If you use the Rust programming language, the compiler will enforce a rule that for each piece of data, you can have just one of the following:

1. A single writable (mutable) reference.
2. One or more read-only references.

The result is that if you were to try to write the original data race example code in Rust, the compiler would refuse to compile your code.
That is, the compiler would force you use one of the strategies needed to solve the problem.

Rust doesn't solve all such problems, so for example the bad lock usages above might compile depending on the kind of data you're accessing[^copy].
But it does catch the worst kind of problems, the ones that cause undefined behavior.

[^copy]: Anything with the `Copy` trait, or, if you explicitly do a `clone()`, anything with the `Clone` trait.

## Avoid race conditions outside of shared memory

Any time you may be writing to the same location from multiple threads or processes, you are asking for trouble.
Sometimes you may cause race conditions accidentally, by writing to shared variables like module globals:

```python
COUNTER = 0

def do_work():
    # ...
    global COUNTER
    # ðŸ˜± It's a data race!
    COUNTER += 1
    # ...
```

But these problems can happen in other places as well.
For example:

* If you have multiple processes writing files to disk, you can lose data if multiple processes choose the same output file path and stomp on each other's ouput.
* In relational databases like PostgreSQL, in [certain isolation levels](https://www.postgresql.org/docs/current/transaction-iso.html) you can have data change out from under you in the middle of a transaction, which can lead to incorrect results.

Because these problems are universal, avoid writing to shared locations if you can, and otherwise use other solutions like locking, or (where relevant) stronger isolation levels for database transactions.

## "Works for me" is not sufficient guarantee of correctness

Because race conditions rely on specific timing of operations, they can sometimes be very difficult to reproduce, and may only occur intermittently or under very specific conditions.
The fact your code seemingly runs correctly on your computer does not mean it is free of race conditions.

Some approaches to ensuring your parallel logic is actually correct include:

* Using tools and architectures that reduce the risks; Rust is one example metnioned above.
  This won't solve everything, but it can help.
* Systemically testing all thread interleaving options; see for example the [Loom crate](https://docs.rs/loom/latest/loom/) for Rust.
  This can be slow.
* Modeling with formal methods, for example using [TLA+](https://learntla.com/index.html).
  You still need to worry about whether your implementation matches the formal model.
* Runtime correctness checks like [ThreadSanitizer](https://github.com/google/sanitizers/wiki/threadsanitizercppmanual), available in gcc and clang.
  Because this only does checks at runtime, and given that race conditions can be hard to reproduce, this won't catch everything.
