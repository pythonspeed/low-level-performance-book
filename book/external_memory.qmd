# Avoid unnecessary writes to non-local memory

If your code is accessing a local variable that is present only in the function, that is clearly something the compiler can safely optimize and modify.
However, the compiler may consider removing writes to memory that is accessible outside your function to be more questionable, because in theory those writes might have side effects.

## Know the difference between the stack and the heap

When you store some data in a variable, the code generated by the compiler might store data in two different places: the stack and the heap.

* **The heap (external to the function call):** This is memory that is retrieved using operating system facilities like `malloc()` or `mmap()` or Windows equivalents; it can potentially be very large.
  When passed to another function, the memory is _not_ copied; instead a pointer or reference to the underlying memory is copied.
* **The stack (internal to the function call):** This is memory that is completely local to a function call in progress, and typically fairly small.
  Because it's tied to a specific function call, it can't be passed between functions.
  If you pass a number on the stack to another function, the value gets copied.
  This is fine semantically, since numbers are immutable, and fast because numbers only use a few bytes of memory.

For our purposes, focusing on the subset of Numba we're using:

* Variables that store numbers are on the stack, completely internal to the function.
* NumPy arrays are on the heap, i.e. obtained with operating system memory allocation APIs.

## Avoid unnecessary writes to the heap

Why should you care about this difference?
For one thing, allocating memory on the heap involves calling operating system facilities, which adds overhead.
So don't allocate memory in an inner loop.

More significantly, remember that the compiler will try to generate code that behaves exactly _as if_ it was the original code you wrote _to an external observer_.
If the original code you wrote writes to memory on the heap, the compiler will usually assume that was your intention.
Writes to the stack are easy to optimize away, because they're local to the function call; an external observer can't see them at all.
But memory on the heap comes from the operating system, and if passed in as an argument or returned from a function may well have an external observer.
And that can prevent the compiler from applying optimizations.

```{python}
#| echo: false
%load_ext book_magics
```

```{python}
import numpy as np
from numba import njit

@njit
def local_stack(n):
    acc = 0
    for i in range(n):
        acc += i
    return acc

@njit
def external_memory_input(n, temp_arr):
    temp_arr[0] = 0
    for i in range(n):
        temp_arr[0] += i
    return temp_arr[0]

@njit
def external_memory_output(n):
    temp_arr = np.zeros((1,), dtype=np.uint64)
    for i in range(n):
        temp_arr[0] += i
    return temp_arr


# Both functions give the same result:
ZEROS = np.zeros((1, ), dtype=np.uint64)
assert local_stack(5) == external_memory_input(5, ZEROS)
assert local_stack(5) == external_memory_output(5)[0]
```

Here's how their speed compares:

```{python}
#| echo: false
%%compare_timing

local_stack(1_000_000)
external_memory_input(1_000_000, ZEROS)
external_memory_output(1_000_000)
```

Of the two versions, the first version is _vastly_ faster, surprisingly so.
If my i7-12700k CPU runs at a clock frequency of approximately 4GHZ, that means it does about 4 CPU cycles per nanosecond.
It should be able to do a few CPU instructions per cycle thanks to instruction-level parallelism, which we'll talk about the in the next chapter.
So let's say it can do 2000 instructions in 100 nanoseconds, just to be generous.
How can it do a million additions with just 2000 instructions?

The LLVM compiler used by Numba is smart enough to figure out that the sum of 0, 1, 2, ..., N is actually `NÃ—(N + 1)/2`, so it just substitutes the faster formula when it can.
So the runtime of the `local_stack()` function will actually be the same regardless of `N`.

Unfortunately, in the case of using an externally given array as the temporary accumulator, or an array that gets returned, the presence of writes to the array likely prevents that particular optimization.
The arrays are visible outside the function calls, and therefore the writes to the function are something that doesn't get optimized away.

## The compiler might optimize away heap allocations, but don't rely on this

If a heap allocation, in this case an array, is completely local to the function, the compiler may figure that out and optimize it away:

```{python}
@njit
def internal_array(n):
    arr = np.zeros((1, ), dtype=np.uint64)
    for i in range(n):
        arr[0] += i
    return arr[0]

assert local_stack(5) == internal_array(5)
```

And if we time it:

```{python}
#| echo: false
%%compare_timing

local_stack(1_000_000)
internal_array(1_000_000)
```

In practice, it's best not to rely on this optimization, since it is likely to be fragile.
