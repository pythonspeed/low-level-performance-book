# Example: Low-level code can help you save memory

Because Python is slow, libraries like NumPy rely on "vectorized" operations, implemented in a fast low-level language and operating on the whole array.
Unfortunately, operating on full arrays means temporary values also have to be full arrays, which can lead to high memory usage.
The memory usage can be solved by switching to a Python `for` loop, but iterating over individual array items in Python is slow.
Very slow.

So if memory usage due to temporary arrays is a problem, rewriting your code in a low-level programming language is the solution.
You will be able _quickly_ iterate over individual array items, running quickly and saving memory.

To see how this works in practice, it's best to consider an example.

## Rescaling intensity of images

Imagine we have an 8-bit greyscale image.
Oftentimes, low values and high values aren't used much, resulting in an image whose contrast is too low:

```{python}
from numba import njit
import numpy as np
from skimage import io

image = io.imread("../images/lowcontrast.jpg")
```

The original image looks like this:

```{python}
#| echo: false
%load_ext book_magics
%display_image image
```

We want to stretch the values between 30 and 190 to be between 0 and 255, thus increasing the contrast.
One way to do this is with normal NumPy APIs.
In order to minimize memory usage, we try very hard to minimize the creation of temporary arrays:

```{python}
def numpy_rescale_intensity(img, min_value, max_value):
    """Stretch (min_value, max_value) to (0, 255)."""
    shifted = img.clip(min_value, max_value)
    shifted -= min_value
    # This creates a temporary float64 array:
    scaled = shifted / (max_value - min_value)
    scaled *= 255
    np.round(scaled, out=scaled)
    shifted[:] = scaled
    return shifted

original_rescaled = numpy_rescale_intensity(image, 30, 190)
```

The result looks like this:

```{python}
#| echo: false
%display_image original_rescaled
```

How much memory does it use?
Assuming the input image takes N megabytes and is a `uint8`, we create both a result array and a temporary `float64` array.
The latter takes 8× as much memory since a `float64` uses 8× as many bytes as a `uint8`.
So the total peak memory is 900% higher than the original image!

Thing is, we don't need a whole temporary array.
Each pixel in the image is calculated independently, so if we did the calculation one pixel at a time, we'd use minimal amounts of memory.
But do that without slowing down our implementation, we need to switch to fast, low-level code.
Let's try that!

## A lower-memory implementation with low-level code

Here's a first pass implementation of the same algorithm, done with Numba:

```{python}
@njit
def lowlevel_rescale_intensity(img, min_value, max_value):
    shifted_max = max_value - min_value
    result = np.empty(img.shape, dtype=np.uint8)
    for y in range(img.shape[0]):
        for x in range(img.shape[1]):
            old_value = img[y, x]
            shifted = 0 if old_value < min_value else old_value - min_value
            shifted = shifted_max if shifted > shifted_max else shifted
            scaled = (shifted / shifted_max) * 255
            result[y, x] = np.uint8(np.round(scaled))
    return result

assert np.array_equal(
    original_rescaled,
    lowlevel_rescale_intensity(image, 30, 190)
)
```

Notice that instead of having a whole temporary `float64` array, we just have temporary `float64` values, massively reducing our memory usage.
As it turns out, this also runs faster:

```{python}
#| echo: false
%%compare_timing
numpy_rescale_intensity(image, 30, 190)
lowlevel_rescale_intensity(image, 30, 190)
```

## Making it run even faster by precalculating results

Can we speed up this code?
There's a number of steps used to calculate the new value, perhaps we could optimize that calculation?

In fact, optimizing the calculation is a distraction.
The key to faster code in this case is remembering that the fastest code is code that doesn't run at all.

Notice that our function takes a value between 0 and 255 and turns it into another value between 0 and 255.
For a given set of inputs, _that relationship is fixed_.
If you encounter a pixel with value 17, the rescaled value will be exactly the same as the last time you encountered 17.
And yet, in our existing implementation, we will do the same exact calculation with the same exact results, over and over and over again.

Instead of repeatedly doing the same calculation, we can create a lookup table once.
The lookup table only needs to hold 256 `uint8`s, one for each potential pixel value, so it'll be small.
And because we're accessing it in a tight loop, it will stay in the CPU's L1 cache, so reading from it should be fast.
Here's what this looks like:

```{python}
@njit
def lut_rescale_intensity(img, min_value, max_value):
    shifted_max = max_value - min_value

    # Create a lookup table mapping from original value to rescaled value:
    rescaled = np.empty((256,), dtype=np.uint8)
    for i in range(256):
        shifted = 0 if i < min_value else i - min_value
        shifted = shifted_max if shifted > shifted_max else shifted
        scaled = (shifted / shifted_max) * 255
        rescaled[i] = np.uint8(np.round(scaled))

    # For each pixel in the image, update it using the corresponding value in
    # the lookup table:
    result = np.empty(img.shape, dtype=np.uint8)
    for y in range(img.shape[0]):
        for x in range(img.shape[1]):
            result[y, x] = rescaled[img[y, x]]
    return result

assert np.array_equal(
    original_rescaled,
    lut_rescale_intensity(image, 30, 190)
)
```

Here's the speed of our new version:

```{python}
#| echo: false
%%compare_timing
numpy_rescale_intensity(image, 30, 190)
lowlevel_rescale_intensity(image, 30, 190)
lut_rescale_intensity(image, 30, 190)
```

Our new version is 10× faster and uses a tenth of the memory usage of the original version.
Not bad!
