# Example: Save memory with low-level code

Because Python is slow, libraries like NumPy rely on batch operations that are implemented in a fast low-level language, and operating on the whole array[^vectorized].
Unfortunately, that means temporary values also have to be whole arrays, which can lead to high memory usage.
The memory usage can be solved by switching to a Python `for` loop, but iterating over individual array items in Python is very slow.

[^vectorized]: These are sometimes known to Python programmers as "vectorized" operations. C/C++/Rust programmers have a different meaning for the word, using "vectorized" to mean SIMD.

If memory usage due to temporary arrays is a problem, rewriting your code in a low-level programming language is the solution.
Your code will be able to iterate over individual array items, saving memory, while still running quickly.

To see how this works in practice, it's best to consider an example.

## Example: Rescaling the intensity of images

Imagine we have an 8-bit greyscale image.
Oftentimes, low values and high values aren't used much, resulting in an image whose contrast is too low:

```{python}
from numba import jit
import numpy as np
from skimage import io

image = io.imread("../images/lowcontrast.jpg")
```

The original image looks like this:

```{python}
#| echo: false
%load_ext book_magics
%display_image image
```

We want to stretch the values between 30 and 190 to be between 0 and 255, thus increasing the contrast.
One way to do this is with normal NumPy APIs.
In order to minimize memory usage, we try very hard to minimize the creation of temporary arrays:

```{python}
def numpy_rescale_intensity(img, min_value, max_value):
    """Stretch (min_value, max_value) to (0, 255)."""
    shifted = img.clip(min_value, max_value)
    shifted -= min_value
    # This creates a temporary float64 array:
    scaled = shifted / (max_value - min_value)
    scaled *= 255
    np.round(scaled, out=scaled)
    shifted[:] = scaled
    return shifted

original_rescaled = numpy_rescale_intensity(image, 30, 190)
```

The result looks like this:

```{python}
#| echo: false
%display_image original_rescaled
```

How much memory does it use?
Assuming the input image takes N megabytes and is a `uint8`, we create both a result array and a temporary `float64` array.
The latter takes 8Ã— as much memory since a `float64` uses 8Ã— as many bytes as a `uint8`.
So the total peak memory is 900% higher than the original image!

Thing is, we don't need a whole temporary array.
Each pixel in the image is calculated independently, so if we did the calculation one pixel at a time, we'd use minimal amounts of memory.
But do that without slowing down our implementation, we need to switch to fast, low-level code.
Let's try that!

## Avoid temporary arrays by switching to a `for` loop

Here's a first pass implementation of the same algorithm, done with Numba:

```{python}
@jit
def lowlevel_rescale_intensity(img, min_value, max_value):
    shifted_max = max_value - min_value
    result = np.empty(img.shape, dtype=np.uint8)
    for y in range(img.shape[0]):
        for x in range(img.shape[1]):
            old_value = img[y, x]
            shifted = 0 if old_value < min_value else old_value - min_value
            shifted = shifted_max if shifted > shifted_max else shifted
            scaled = (shifted / shifted_max) * 255
            result[y, x] = np.uint8(np.round(scaled))
    return result

assert np.array_equal(
    original_rescaled,
    lowlevel_rescale_intensity(image, 30, 190)
)
```

Notice that instead of having a whole temporary `float64` array the size of the original image, we now have temporary `float64` values for only one pixel at a time, massively reducing our memory usage.

As it turns out, not only do we save memory, but our code also runs faster:

```{python}
#| echo: false
%%compare_timing --measure=peak_memory
numpy_rescale_intensity(image, 30, 190)
lowlevel_rescale_intensity(image, 30, 190)
```

## Replace repetitive calculations with a lookup table

Can we speed up this code?
There's a number of steps used to calculate the new value, perhaps we could optimize that calculation?

In fact, optimizing the calculation is a distraction.
The key to faster code in this case is remembering that the fastest code is code that doesn't run at all.

Notice that our function takes a value between 0 and 255 and turns it into another value between 0 and 255.
For a given set of inputs, _that relationship is fixed_.
If you encounter a pixel with value 17, the rescaled value will be exactly the same as the last time you encountered 17.
And yet, in our existing implementation, we will do the same exact calculation with the same exact results, over and over and over again.

Instead of repeatedly doing the same calculation, we can create a lookup table once.
The lookup table only needs to hold 256 `uint8`s, one for each potential pixel value, so it'll be small.
And because we're accessing it in a tight loop, it will stay in the CPU's L1 cache, so reading from it should be fast.
Here's what this looks like:

```{python}
@jit
def lut_rescale_intensity(img, min_value, max_value):
    shifted_max = max_value - min_value

    # ðŸ˜Ž Create a lookup table mapping from original value to rescaled value.
    # We only need to do 256 calculations:
    rescaled = np.empty((256,), dtype=np.uint8)
    for i in range(256):
        shifted = 0 if i < min_value else i - min_value
        shifted = shifted_max if shifted > shifted_max else shifted
        scaled = (shifted / shifted_max) * 255
        rescaled[i] = np.uint8(np.round(scaled))

    # ðŸ˜Ž For each pixel in the image, update it using the corresponding value
    # in the lookup table, a very cheap operation:
    result = np.empty(img.shape, dtype=np.uint8)
    for y in range(img.shape[0]):
        for x in range(img.shape[1]):
            result[y, x] = rescaled[img[y, x]]
    return result

assert np.array_equal(
    original_rescaled,
    lut_rescale_intensity(image, 30, 190)
)
```

Here's the speed of our new version:

```{python}
#| echo: false
%%compare_timing --measure=peak_memory
numpy_rescale_intensity(image, 30, 190)
lowlevel_rescale_intensity(image, 30, 190)
lut_rescale_intensity(image, 30, 190)
```

Our new version is 8Ã— faster and uses a tenth of the memory usage of the original version.
Not bad!
