# Roadmap: Use this book to speed up your code {.unnumbered}

How you read this book will depend on whether it's relevant to your immediate goals, and on how much time you have.

## Make sure you're the book's audience

When writing this book I assumed that:

* You're a scientist, data scientist, or software developer who uses Python to do numeric computing or other computationally intensive data processing.
* You need to speed up the time it takes to process bulk data: throughput rather than latency.
  For example, you're processing a giant CSV and only care about the time it takes to finish processing all the data.
  On the other hand, if you're working on a REST API that needs to be consistently responsive to millions of tiny parallel requests, this book will be less helpful.
* You know Python, but don't necessarily know a low-level compiled language.

This book can of course be useful more broadly, but the further you are from the intended audience the less helpful it will be.

## Validate you actually have a computational bottleneck

Most of this book focuses on speeding up computation.
But taking a step back, there are many other reasons your code could be slow: if the bottleneck is reading from the network, there is no point in making your calculations faster.
So before you start optimizing your code, it's important to understand that other bottlenecks exist, and to do the work to identify which specific bottleneck is impacting your code.

```{python}
#| echo: false
%load_ext book_magics
```

One way to do this is with one of the  many performance profilers for Python—[line_profiler](https://pypi.org/project/line-profiler/), [Scalene](https://github.com/plasma-umass/scalene), [Py-Spy](https://github.com/benfred/py-spy), [PyInstrument](https://pyinstrument.readthedocs.io/en/latest/), [VizTracer](https://viztracer.readthedocs.io/en/stable/), and others.
By measuring yoru code, you can identify which parts are taking the most time.

The following example code matches the basic high-level structure of many data processing programs.
If you are not familiar with Pandas, that's fine, it's the high-level part that matters:

```{python}
import pandas as pd

def read_data(file_or_url):
    return pd.read_csv(file_or_url)

def process_data(input_df):
    # Return the distribution of the first letter of the first name; if you're
    # unfamiliar with Pandas that's fine, you don't need to understand this
    # particular code.
    first_letter = input_df["FIRST NAME"].dropna().apply(
        lambda s: s[0]
    )
    return first_letter.value_counts()

def generate_report(output):
    output.to_csv("result.csv")

def full_pipeline(file_or_url):
    input_df = read_data(file_or_url)
    result = process_data(input_df)
    generate_report(result)
```

Next I'll profile this code to see where it's spending the most time.
I'll use `line_profiler` to profile the code, as it's probably the simplest profiler you can use.
You can use `line_profiler` on the command-line, or inside a Jupyter notebook or equivalent:

```{python}
# Load the line_profiler extension into a Jupyter notebook; this book is
# written using Jupyter notebooks, one per chapter.
%load_ext line_profiler

# Birth name and date for people born in Missouri between 1980 and 1989:
url = "https://archive.org/download/Missouri_Birth_Index_-_1980-1989/Reclaim_The_Records_-_Missouri_Birth_Index_-_1980-1989.csv"

# Run the expression full_pipeline(url), and show profiled time for the
# full_pipeline and process_data functions:
%lprun -f full_pipeline -f process_data full_pipeline(url)
```

As you can see from looking at the output for `full_pipeline()`, reading in the CSV is the main bottleneck, using up 99% of the time.
This presumably is some combination of bandwidth (downloading the file) and computation (parsing the CSV).
Some profilers—like the [Sciagraph profiler](https://sciagraph.com) I created—can tell you whether code was spending it's time waiting or doing computation.
Lacking that information, you'll need to figure this out from some combination of context, detailed profiling, and experiments.

The profiling also suggests which parts of `process_data()` are slower.
However, since loading the data uses the vast majority of the time there is no point in trying to optimize `process_data()`, at least initially.
If I wanted faster results, I'd need to figure out a faster way to load the data.

Keep in mind that profilers can distort the results somewhat, so low overhead is an important feature in a profiler.
If you want this style of line-by-line profiling, a more sophisticated and lower-overhead profiler is [Scalene](https://github.com/plasma-umass/scalene).

If you discover your bottleneck is not computation, this book won't help you, and you will need to find other resources.

## How to read this book to improve your skills

Reading this book from start to finish can help you build a mental map of performance bottlenecks, and how you can solve them.
You can then try to write faster code from the start, and you will have an easier time when you do hit a specific performance problem.

Depending how much time and interest you have, consider skipping optional chapters on your first read through.
Optional chapters are marked with "⋯" symbols, so they will have titles that look like this: *⋯ An optional chapter ⋯*.

## How to read this book to fix a specific performance bottleneck

### Step 1. Identify the general bottleneck with a profiler

### Step 2. Figure out how you will measure speed

To know whether your code is getting faster or not, you need some way to measure its speed.
That means you need to:

1. Choose an appropriate metric, for example if you're processing a large CSV you might choose a throughput measure of rows/second (see chapter XXX).
2. Implement a benchmarking setup to measure that metric, while minimizing noise and distortion (see chapter XXX).

### Step 3. Apply the Practice of Algorithmic Efficiency

Algorithmic efficiency won't always give you the biggest boost to speed.
But beyond the speed benefits you will get, it can also help you _understand_ your code better.
This is very helpful in applying other practices.
If you haven't done so recently, I would recommend at least skimming all the chapters in that part of the book, and then seeing how these general principles apply to your code.

If your function is now fast enough, skip to step 5.

### Step 4. Think about the Practice of Parallelism

Now that you understand your code a bit better, and if your code is single-threaded, it's now worth thinking about parallelism: where exactly will you apply it, if you do?
I say "think" because you may or may not want to _implement_ parallelism at this point.
For example, if you end up porting to a compiled language that might enable parallelism within a function, but you'd have to do that first.
But you want to at least have a plan for parallelism in mind, as that might impact how you implement other changes.

To inform your decision, I would recommend reading chapters XXX and YYY.

### Step 4. Apply additional practices as needed

#### Pure Python code

#### Using an existing compiled extension library like NumPy or Pandas

#### You're using 

### Step 5. Apply parallelism, if you haven't already

### Step 6. Consider why your code ended up being slow

