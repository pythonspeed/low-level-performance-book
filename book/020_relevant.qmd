#  Is this book for you? {.unnumbered}

Before reading this book, you should make sure it's relevant to your needs.

## First, do no harm

Imagine you are working on an application that makes someone sick every time you run it.
This is bad.

Then, you make it 10× faster: now you can make ten people sick in the same amount of time!
Your software has gone from bad to worse.

**If your software harms people or harms the environment, making it faster will amplify the harm.**
So if your work does cause harm, you shouldn't be reading this book, you should be looking for a new project or a new job.

## A focus on computation

When writing this book I assumed that:

* **You're writing computationally-intensive data processing.**
  That is, you're a scientist, data scientist, or software developer who uses Python to do numeric computing or other data processing.
* **You need to speed up the time it takes to process bulk data.**
  For example, you're processing a giant CSV and only care about the time it takes to finish processing all the data.
  On the other hand, if you're working on a server with a REST API that needs to be consistently responsive to millions of tiny parallel requests, this book will be less helpful.
  Put another way, this book focuses on optimizing throughput rather than latency.
* **Your code is slow because of computation.**
  This may or may not be the case, and it's something you can check empirically, as I'll discuss next.

## Validate you actually have a computational bottleneck

There are other bottlenecks besides computation that could cause your code to be slow.
For example, perhaps the bottleneck is reading data from the network, in which case there is no point in making your calculations faster.
So before you start optimizing your code, it's important to identify which specific bottleneck is impacting your code.

```{python}
#| echo: false
%load_ext book_magics
```

One way to do this is with one of the  many performance profilers for Python—[line_profiler](https://pypi.org/project/line-profiler/), [Scalene](https://github.com/plasma-umass/scalene), [Py-Spy](https://github.com/benfred/py-spy), [PyInstrument](https://pyinstrument.readthedocs.io/en/latest/), [VizTracer](https://viztracer.readthedocs.io/en/stable/), and others.
By measuring yoru code, you can identify which parts are taking the most time.

The following example code matches the basic high-level structure of many data processing programs.
If you are not familiar with Pandas, that's fine, it's the high-level part that matters:

```{python}
import pandas as pd

def full_pipeline(file_or_url):
    """
    Read a file or URL, process it, and then generate a report.
    """
    # Step 1: Read the data.
    input_df = pd.read_csv(file_or_url)

    # Step 2: Process the data.
    #
    # In particular, return the distribution of the first letter of the first
    # name; if you're unfamiliar with Pandas that's fine, you don't need to
    # understand this particular code.
    first_letter = input_df["FIRST NAME"].dropna().apply(
        lambda s: s[0]
    )
    result = first_letter.value_counts()

    # Step 3: Generate a report.
    result.to_csv("result.csv")
```

Next I'll profile this code to see where it's spending the most time.
I'll use `line_profiler` to profile the code, as it's probably the simplest profiler you can use.

You can use `line_profiler` on the command-line, or inside a Jupyter notebook or equivalent:

```{python}
# Load the line_profiler extension into a Jupyter notebook; this book is
# written using Jupyter notebooks, one per chapter.
%load_ext line_profiler

# Birth name and date for people born in Missouri between 1980 and 1989:
url = "https://archive.org/download/Missouri_Birth_Index_-_1980-1989/Reclaim_The_Records_-_Missouri_Birth_Index_-_1980-1989.csv"

# Run the expression full_pipeline(url), and show profiled time for the
# full_pipeline() function:
%lprun -f full_pipeline full_pipeline(url)
```

> **Note:** Keep in mind that profilers can distort the results somewhat, so low overhead is an important feature in a profiler.
> If you want this style of line-by-line profiling, a more sophisticated and lower-overhead profiler is [Scalene](https://github.com/plasma-umass/scalene).

As you can see from looking at the output for `full_pipeline()`, reading in the CSV is the main bottleneck, using up more than 90% of the time.
This presumably is some combination of bandwidth (downloading the file) and computation (parsing the CSV).
Some profilers—like the [Sciagraph profiler](https://sciagraph.com) I created—can tell you whether code was spending it's time waiting or doing computation.
Lacking that information, you'll need to figure this out from some combination of context, detailed profiling, and experiments.

The profiling also suggests which parts of step 2—the computational data processing—are slower.
However, since loading the data (step 1) uses the vast majority of the time, trying to optimize data processing (step 2) will not meaningfully improve run time.
If I wanted faster results, I'd need to figure out a faster way to load the data.

Since this book focuses on computation, if you discover your bottleneck is _not_ computation, this book won't help you, and you will need to find other resources.
