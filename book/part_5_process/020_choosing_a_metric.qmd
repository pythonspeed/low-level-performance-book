---
title: "Choose an appropriate speed metric for your software"
essential: true
---

If you're going to make your software faster, you need some way to measure it.
There are different metrics you can use, depending on your use case, which can lead you in different directions.

## An example: Geolocating

Imagine you're working on a geocoding service, that turns addresses into latitude/longitude pairs.
There are two ways to use the service:

* An individual API where a customer sends a single address, and gets back its location.
* A bulk API where a customer uploads a CSV with many addresses, and back a CSV with all the locations.

To measure these two APIs, you could send a few million requests to the API server as fast as possible, and also create an equivalent CSV file for the batch API and measure its processing time.
But what metric should you use to summarize the result?

## Metrics you can use

### Elapsed time

You could measure elapsed time.
And that's probably fine for individual optimization session, where you repeatedly measure the speed of the same input.

However, to be thorough, you will want to measure multiple different inputs, and elapsed time can be problematic in this case.
Imagine you get the following result:

* 7,000-row CSV: 130 milliseconds.
* 13,000-row CSV: 237 milliseconds.

Is the speed for both files the same?
You can do some math in your head, but that is error-prone and cumbersome.

### Throughput

A better metric for batch processing is throughput, in this case addresses/second.
If I translate the elapsed time in the two files above to throughput, I get:

* 7,000-row CSV: 53,846 addresses/second.
* 13,000-row CSV: 54,852 addresses/second.

It's much easier to see that both files were processed with the same speed.

For a server handling individual API requests, throughput is useful but insufficient.
Imagine that for 1% of addresses, processing is 100Ã— slower than the other 99% of requests.
For the batch API, this doesn't matter.
But for the server API, this means 1% of user requests will have super-slow response times, and users might not like that!
If you just measure addresses/second, these outliers will be invisible.

### Latency

For an API where you are doing many separate requests each of which needs to be fast, you often want additional metrics, the latency at different percentiles.
Latency is the time to respond to a request.
For example, you can calculate 50th, 95th, and 99th percentile latencies.

For a batch API, the latency of an individual address is irrelevant.
A user won't care how long it took to process any individual address, they just care about how long it takes until the full batch of results comes back.
