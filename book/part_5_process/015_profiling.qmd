---
title: "Use a profiler to find bottlenecks in your code"
---

Most of the examples in this book are small functions; it's clear where the bottleneck is.
Real-world code is typically far more complex, with many functions, classes, modules, and dependencies.
That means you need some way to identify which part or parts of all this code are the bottlenecks.

One common way to identify bottlenecks is using profilers: tools that measure software performance.
I gave a short example of using a profiler [at the start of the book](../020_relevant.qmd), used for exactly this purpose.

Profilers are immensely useful, but suffer from the difficulty of measuring and summarizing performance data.
As a result, there many different profilers, each with their own benefits and drawbacks, and it's important to understand the limits of whichever one you choose to use.

## Understand what your profiler is measuring

If you don't understand what and how your profiler measures data, you may be misled by the results.

### What gets included?

Different profilers might choose to omit certain measurements by default.
Thus by default the `py-spy` profiler omits threads that are idle, like those waiting for a lock.
Sometimes that's what you want; sometimes, if you're not aware of this default, the result will massively distort your understanding of bottlenecks.

Before using a profiler, you should therefore read its documentation, or lacking that the command-line options, just to get a better idea of what exactly it is choosing to measure.

### Deterministic vs. sampling

Beyond _what_ a profiler measures, it is also important _how_ a profiler measures.
There are two basic ways a profiler can measure elapsed time: either deterministically, or with sampling.
Consider for example a program with the following time spent per line:

```python
for i in range(1_000_000): # ~10% of total elapsed time
    do_something_quick(i)  # ~20% of total elapsed time
do_something_slow()        # ~70% of total elapsed time
do_something_very_quick()  # ~0.01% of total elapsed time
```

These numbers can't be observed directly, you need to measure them somehow.

A _deterministic_ profiler would measure the start and stop time of every line of code.
This adds overhead!
And what's more the mechanism it's likely to use ([`PyEval_SetTrace()`](https://docs.python.org/3/c-api/init.html#c.PyEval_SetTrace)) adds additional overhead.
This means your program might take a lot longer to run in some cases.

The resulting measurements will also be somewhat distorted, as the profiler will overweight those lines of code that get called more often.
For example, a 20% overhead per line would result in the following measurements:

```python
for i in range(1_000_000): # ~17% of total measured time
    do_something_quick(i)  # ~25% of total measured time
do_something_slow()        # ~58% of total measured time
do_something_very_quick()  # ~0% of total measured time
```

Besides distorting the results, a deterministic profiler can also slow down runtime significantly.

An alternative approach is a _sampling_ profiler.
At intervals, it checks what the program is doing, and then uses the ratio of samples to calculate the profiling result.
This reduces the overhead significantly, because instead of doing extra work on every line of code (or even more frequently), the measurement can be limited to a certain number of times a second.

For the example above, the output might look like this:

```python
for i in range(1_000_000): # ~11% of measured time
    do_something_quick(i)  # ~19% of measured time
do_something_slow()        # ~70% of measured time
do_something_very_quick()  # ~0% of measured time
```

A second run might give slightly different results, since sampling is being used, but the result will be fairly accurate for those parts of the code that take up the bulk of the run time.
Since these are usually the ones you want to speed up, inaccuracy for rarer lines of codes and functions is acceptable.

This book assumes you care about throughput, rather than latency.
For these use cases, sampling profilers are usually the better choice.

## Choose the right visualization (line-based, flamegraph, timeline)

```{python}
#| echo: false
%load_ext line_profiler
%load_ext pyinstrument
```

Profilers differ not just in what and how they measure, but also in the output they provide.
Consider the following code:

```{python}
import numpy as np

def create_ones_array():
    return np.ones((1_000_000,), dtype=np.int64)

def do_some_math():
    array = create_ones_array()
    for i in range(1000):
        array2 = create_ones_array()
        array += array2 * array.sum()
    return array
```

### Line-by-line profiling

One way to visualize it is by profiling it line by line.
For real world usage I'd suggest [using the Scalene profiler](https://github.com/plasma-umass/scalene), but for our purposes the simpler `line_profiler`[^lineprofiler] will suffice.
Here is its output:

```{python}
%lprun -f do_some_math -f create_ones_array do_some_math()
```

This visualization focuses on one module at a time.
As a result, it's harder to see the big picture of how much time each line spends waiting on calls to other functions.
Scalene gives you more information, but the line-by-line visualization is inherently limiting in getting an overall view of the code.

### Flamegraphs and other callgraph visualizations

Another approach is to show a tree of function calls, or a "callgraph".
To see what this might look like, here is the output from the PyInstrument profiler:

```{python}
#| echo: false
%%pyinstrument --render-option=time=percent_of_total
_ = do_some_math()
```

A common variant for this sort of tree is a "flamegraph"; the visualization above isn't quite the same as a flamegraph, but it should suffice to show the basic idea.
Some things to notice:

* There are multiple calls to `create_ones_array()`, but they are merged into one child of `do_some_math()` in the visualized tree.
* You can see the calls into NumPy and how long they took, you're not just limited to lines of code within the module being profiled.
* The tree doesn't preserve order of execution; you can't tell which functions were called first.

As a result, these sorts of visualizations are better than line-by-line execution at giving the big picture of where time was spent.
The visualization also has no concept of the passage of a time.

### Timelines

As mentioned above, tree views (including flamegraphs) merge multiple function calls: there's only one `create_ones_array()` entry in the tree even though it's called multiple times.
Now, consider what happens if this merging happens across multiple threads.
For a web server running lots of small, uniform queries, this is fine.

But for data processing applications, with code that does very different things at different times—loading data, processing data, writing data—flamegraphs can hide quite a lot of critical information.
Even if you are visualizing each thread separately with its own tree, since you can't tell when threads start or finish, you can't tell if your program is slow because threads are idle, or because your thread pool is too small.

If this information is important, another useful visualization is a timeline, that can show you multiple threads and what they were doing.
PyInstrument, at least in the version used in this chapter, unfortunately doesn't support a timeline for multiple threads, but you can use the [deterministic profiler VizTracer](https://github.com/gaogaotiantian/viztracer) to generate timelines for threaded applications.
I have created a sampling profiler specifically for data processing applications, [Sciagraph](https://sciagraph.com), which produces both flamegraphs and timelines.

## Understand the limitations of profiling for compiled code

For compiled code, profilers have another barrier to finding bottlenecks.

the code you are profiling is not exactly the same

TODO
