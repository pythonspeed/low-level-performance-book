# Microbenchmark your code to measure performance impacts

In almost every chapter of this book we compared the timing of different implementations of the same algorithm to see if our changes were faster.
However, running a function once can give inconsistent timing results.

For example, you can use the `%time` magic in Jupyter or IPython to measure the time of running a function once.

> If you've never used Jupyter: it's a system for creating computational notebooks, combining code and text.
> This book is in part built using Jupyter notebooks!
> It also supports special commands like `%time`, known as magics, that transform or control the code in some way, in this case by measuring how long it takes to run.

```{python}
%time _ = sum(range(1_000))
%time _ = sum(range(1_000))
%time _ = sum(range(1_000))
```

Given an individual run can be inconsistent for a variety of reasons, it's better to run the code a large number of times, and report the average run time.
There are pre-existing utilities and libraries for doing this:

* Python has a [`timeit`](https://docs.python.org/3/library/timeit.html) module you can use for command-line or API-based microbenchmarking.
* In Jupyter or IPython you can use the `%timeit` magic.
* The [`pytest-benchmark`](https://pypi.org/project/pytest-benchmark/) package lets you write benchmark suites to be run by `pytest`.
* This book uses a microbenchmarking library I wrote myself, wrapping Python's `timeit` module.

Here's an example of the Jupyter `%timeit` magic:

```{python}
%timeit sum(range(1_000))
```

## Benchmark the right thing

Microbenchmarking is an important tool in measuring the impact of performance optimizations.
But while it's seemingly quite simple, it also has the potential for false or misleading results.
So next I'll go over some potential problems and how to avoid them.

To begin with, no matter how accurate the result, if you measure the wrong thing you won't get useful information.

### Focus on optimizing the main bottleneck

If the code you are looking at only uses 2% of your program's run time, making it twice as fast will make your program 1% faster.
That's not very helpful.
So before you spend any time trying to optimize some code, you should make sure it's actually something worth optimizing.

**What to do:** As discussed in the previous chapter, before optimizing your code, use a profiler to figure out where the actual bottlenecks are.

### Use realistic data

Both the size of your data and its contents should be as realistic as possible:

* If your data is usually a 100MB CSV, benchmarking with a 10-row CSV file will likely give you misleading results.
* If you benchmark with a blank white image, you may get different speed results with a real image from a microscope.

**What to do:** If possible, use real data.
If you can't use real data, at least generate something that is similar to real data.

### Make sure to include Python overhead

The presumption in this book is that you will eventually call any compiled low-level code from Python.
Assuming that is true, some of the time spent by your code will involve:

* The overhead of calling a compiled extension from Python.
* The overhead of converting arguments and result between the Python representation and the low-level language representation.
  For example, a Python list of strings (`list[str]`) might be converted into a Rust `Vec<String>`, and vice-versa.
  Passing a NumPy array will typically be so fast so as to not matter; converting between Python and Rust strings is much more expensive.

If your underlying function is fast enough, the overhead of calling from Python might drown out the run time of the function.
Optimizing your function is pointless at that point.

**What to do:** If the arguments and result of your function are expensive to convert to/from Python, this is something you need to include in your measurements, because it will impact real-world performance.
As is the case for examples in this book, consider measuring the performance of your code by calling it from Python, rather than purely measuring the speed of the low-level code without the Python interface.

### Avoid benchmarking code compiled for development

Many compilers support different compilation modes or profiles:

* **Development or debug mode:** Lots more assertions and checks are added, no optimizations are applied by the compiler.
  The code is much slower, but easier to debug.
* **Release mode:** The code is compiled with optimizations, so it runs as fast as possible.
  This is what you'll use in production or release to users.

If you benchmark code compiled in debug mode, you might vastly understate your code's performance.
This is a common problem with Rust; `cargo build`, `maturin develop`, or `pip install -e .` will result in your code being compiled in Rust's much slower debug profile.

**What to do:** Make sure to compile your code in release mode, for example `cargo build --release` or `pip install .` for Rust projects.

### Be aware of platform-specific issues

These are by their nature quite varied, but Numba is a good example since it used in this book.
The first time you run a function decorated with `@numba.jit` with a new set of types, the code will be compiled.
This is slow, and it's a one time cost, so you usually want to ignore this compilation time when benchmarking: after all, it doesn't reflect the speed of running the function the second, third, or millionth time.

Let's see this compilation time in action:

```{python}
from time import time
import numpy as np
from numba import jit

@jit
def arr_sum(arr):
    result = 0
    for i in range(len(arr)):
        result += arr[i]
    return result
```

The first time we call `arr_sum()` with an array of `np.int64`, the elapsed time will be much higher than subsequently, due to compilation:

```{python}
# One million int64s:
ARR_64 = np.ones((1_000_000,), dtype=np.int64)

print("The first time we run the function, it needs to be compiled:")
%time _ = arr_sum(ARR_64)

print("\nThe second and third run, compilation doesn't happen:")
%time _ = arr_sum(ARR_64)
%time _ = arr_sum(ARR_64)
```

If we call `arr_sum()` with an array of `np.uint32`, this will require Numba to compile a new version of the function, so once again the first run will be much slower:

```{python}
# One million uint32s:
ARR_32 = np.ones((1_000_000,), dtype=np.uint32)

print("A new type, so the function it needs to be recompiled:")
%time _ = arr_sum(ARR_32)

print("\nThe second and third run, compilation doesn't happen:")
%time _ = arr_sum(ARR_32)
%time _ = arr_sum(ARR_32)
```

**Make sure to run Numba functions at least once, with matching types, before doing any benchmarking.**
You'll notice every code example in the book does this.
This ensures the compilation time doesn't distort the benchmarks.

## Minimize noise from the CPU

Running code on the CPU can take different amounts of time depending on a variety of factors.

### Minimize noise from other processes

If your computer has 4 cores and you're saturating all of them by compiling some code in the background, your microbenchmark will give different results than if your CPU is idle.
Other programs may also use enough memory or cache to impact your code's runtime performance.

**What to do:** Make sure you're not running CPU intensive operations when benchmarking, so you have some idle CPU cores that can be used for performance measurements.
If you're measuring multi-core performance, shut down as many other programs as possible.
If you're sensitive to memory availability, make sure other programs have left enough free memory for your benchmark.

But also consider what your production run time environment will look like; perhaps there will be other processes running!

### Disable "turboboost" on your CPU

Many CPUs will temporarily run faster for a short period of time, at a speed they can't sustain over longer periods of time due to overheating.
Intel calls this "turboboost".
These differences in speed can distort results, especially if you're running two benchmarks in close succession.

**What to do:** Run benchmarks with turboboost disabled.
On Intel CPUs on Linux you can temporarily disable turboboost like so:

```{shell-session}
$ echo "1" | sudo tee /sys/devices/system/cpu/intel_pstate/no_turbo
```

On AMD CPUs you can do:

```{shell-session}
$ echo "0" | sudo tee /sys/devices/system/cpu/cpufreq/boost
```

### Make sure your CPU isn't throttled due to power usage

If you unplug your laptop from power, it may reduce its maximum speed to conserve battery power.
This is particularly true of x86-64 CPUs, which are much less power efficient than Apple's new chips.

**What to do:** If you're using a laptop, always run benchmarks when plugged in.
Or, use a desktop computer instead.

### Make sure your CPU isn't throttled due to overheating

A CPU may also be throttled to keep your computer from overheating.
This tends to be more of a problem with laptops, since the form factor makes them harder to keep cool.

**What to do:** If you're using a laptop, make sure it's set up with good airflow.
Or better yet, use a desktop computer instead.

## Be aware of how CPU state can distort benchmarks

Your CPUs behavior changes based on what code it previously ran!

### Avoid misleading results due to cached data

Recall that data fetched from RAM is cached in a series of faster caches (L1, L2, L3).
When microbenchmarking, that means data retrieved in the first run may well be in the cache in future runs.
If memory is a bottleneck, that means future runs will be faster, and if you don't expect data to be in the cache, that means the speed measurement will be wrong.

**What to do:** Assuming your expected use case is larger-than-cache data, run microbenchmarks with sufficiently large data such that it doesn't fit in cache easily.

### Avoid misleading results due to the branch predictor

The CPU records the code you run in order to predict what branches will be taken.
Consider a situation where realistic data will result in hard-to-predict branching.
There are two failure modes:

* If you benchmark with small amounts of data, the CPU might be able to learn to "predict" outcomes simply because [you're running with the same data over and over again](https://lemire.me/blog/2019/10/16/benchmarking-is-hard-processors-learn-to-predict-branches/).
  In the real world, with more data, that won't be possible.
* If you benchmark with unrealistic, predictable data, your microbenchmarking won't record the cost of mispredicton.

**What to do:** Test with a large amount of data, and make sure the benchmark data's branch predictability matches that of real-world data.

## Use a realistic measurement environment

Your measurements may be correct for your current hardware, but not match the behavior you'll get in production.

### Match the hardware setup used in production

Your development computer can differ in production in a large number of ways: CPU speed, supported SIMD instructions, available RAM, cache sizes, and so on.
If you're developing on a modern ARM Mac and deploying to an x86-64 server, you're even using a completely CPU different architecture.
While some optimizations will work decently on all modern CPUs, inevitably there will be differences.

**What to do:** Try to match your development environment to production as much as possible.

### Match the hyperthreading / SMT setup used in production

"Hyperthreading" is Intel's name for symmetric multi-threading.
The basic idea is that since your CPU core can already do many things in parallel (instruction-level parallelism), you can just have a core pretend to be two virtual cores, or four cores.
Then, when one virtual core is not utilizing ILP as well as goodâ€”perhaps it's waiting for memory readsâ€”the other virtual core can continue to use the core's unused resources.
The better you optimize your code, the less this will help you, and it might actually make things worse since each virtual core will have half as much L1 and L2 caches.

This can impact you in a variety of ways:

* Your local computer may lack hyperthreading, but your cloud server almost certainly is using it.
* Your microbenchmark might indicate good speed because it gets to saturate that whole physical core.
  If you run a thread pool with a thread per virtual core, the resulting running code will run with half as much L1 and L2 caches for data, because it gets used by two virtual cores presumably processing different data.

**What to do:** Try to make your development machine match production, including the lack or presence of SMT.
On PCs you can disable hyperthreading/SMT in the BIOS, which you can typically configure by pressing the right key during boot.

### Use the same software versions as production

Different versions of Python, Numba, NumPy all behave differently in terms of performance.

**What to do:** As much as possible, try to ensure you're measuring with software versions that match real-world usage.

## Don't get outsmarted by the compiler

The compiler is smartâ€”and that means you might not be measuring what you think you're measuring.

Recall that if use constant calculations the compiler might notice, do the computation once, and just output the result.
This can happen in less obvious way with function inlining, where a function takes parameters but the compiler notices that it's only being called with constants:

**What to do:** Always benchmark with inputs that are passed in such that the compiler won't decide they're constants and optimize everything away.
When microbenchmarking Python extensions, the obvious solution is to pass these arguments in from Python.
Some benchmarking frameworks also have ways to mark arguments as non-optimizable, like [the `black_box()` function in Rust's Criterion benchmarking tool](https://bheisler.github.io/criterion.rs/book/getting_started.html).

Here's an example:

```{python}
#| echo: false
%load_ext book_magics
```

```{python}
from numba import jit

# This function will be benchmarked in a useful way if called directly:
@jit
def calc(a, b, c, d, e, f):
    return ((a + b + c) ** d / e) + f

# ðŸ˜¢ This function will be optimized by the compiler into just returning a
# constant, so benchmarking it won't give us any information about the
# underlying calc()'s speed:
@jit
def calc_fixed():
    return calc(1, 2, 3, 7, 6.0, 5)

# All both functions give the same result:
assert calc_fixed() == calc(1, 2, 3, 7, 6.0, 5)
```

Here's how their speed compares:

```{python}
#| echo: false
%%compare_timing

calc(1, 2, 3, 7, 6.0, 5)
calc_fixed()
```

`calc_fixed()` is giving us misleading results if our goal is to benchmark the wrapped `calc()`.
Because of compiler optimizations, it's no different than a function that just returns a constant:

```{python}
@jit
def constant():
    return 1

assert constant() == 1
```

```{python}
#| echo: false
%%compare_timing

calc_fixed()
constant()
```

## Reduce noise caused by randomness in software

Multiple runs of the same program can have different performance behavior due to built-in (or deliberate) randomness.
For example, every run of Python has a randomized seed for hash functions, which can make dictionaries and sets access data differently for certain operations.
Since dictionaries are pervasive in Python, different runs of the same code can then have different performance.

Importantly, running a function multiple times within the same process wouldn't overcome this source of noise.
You'll see the performance differences only across different processes.

**What to do:** You have two approaches to overcoming this sort of noise:

1. Use a fixed seed for randomness.
   You can set Python's hash seed using the [`PYTHONHASHSEED`](https://docs.python.org/3/using/cmdline.html#envvar-PYTHONHASHSEED) environment variable; just set it to a fixed number.
   Similarly, you can use [`random.seed()`](https://docs.python.org/3/library/random.html#random.seed) to set the seed for Python's random number generator, and [`numpy.random.default_rng()`](https://numpy.org/doc/2.0/reference/random/generator.html#numpy.random.default_rng) accepts a seed argument.
   In practice, this wont solve all sources of randomness, and it is also less realistic in that real programs will have different seeds on every run.
2. Run benchmarks multiple times, and average the results.
   You can use a program like [`hyperfine`](https://github.com/sharkdp/hyperfine) to do this, but that only measures the full process runtime, which is likely not what you want.
   Or, you can write your own framework.

## Unknown sources of noise

Beyond all the above identifiable reasons you might get different results, there are other reasons your code might take different amount of time to run.

**What to do:** This is why microbenchmarking usually involves running the code multiple times, to help average out any uncontrolled noise.
If the noise is tied to specific process runs, you'll need to run the benchmark multiple times.
