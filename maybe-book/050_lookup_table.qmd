# Use a lookup table

A lookup table is an extreme case of getting rid of repetitive code: you calculate a value _once_, and that's it.
To demonstrate how this works, we'll return to an example from a previous chapter, rescaling an image for better contrast.

```{python}
#| echo: false
%load_ext book_magics
```

Here's our previous Numba-based solution:

```{python}
from numba import jit
import numpy as np
from skimage import io

IMAGE = io.imread("../images/lowcontrast.jpg")

@jit
def compiled_rescale_intensity(img, min_value, max_value):
    shifted_max = max_value - min_value
    result = np.empty(img.shape, dtype=np.uint8)
    height, width = img.shape
    for y in range(height):
        for x in range(width):
            old_value = img[y, x]
            if old_value < min_value:
                clipped = min_value
            elif old_value > max_value:
                clipped = max_value
            else:
                clipped = old_value
            shifted = clipped - min_value
            scaled = shifted * (255 / shifted_max)
            result[y, x] = np.uint8(np.round(scaled))
    return result
```

## Replace repetitive calculations with a lookup table

Can we speed up this code?
There's a number of steps used to calculate the new value, perhaps we could optimize that calculation?

In fact, optimizing the calculation is a distraction.
The key to faster code in this case is remembering that the fastest code is code that doesn't run at all.

Notice that our function takes a value between 0 and 255 and turns it into another value between 0 and 255.
For a given `min_value` and `max_value`, _that relationship is fixed_.
If you encounter a pixel with value 17, the rescaled value will be exactly the same as the last time you encountered 17.
And yet, in our existing implementation, we will do the same exact calculation with the same exact results, over and over and over again.

Instead of repeatedly doing the same calculation, we can create a lookup table once.
The lookup table only needs to hold 256 `uint8`s, one for each potential pixel value, so it'll use only a small amount of memory[^cpucache].
Here's what this looks like:

[^cpucache]: In later chapters we'll see that keeping frequently accessed data small can speed up your code, thanks to CPU memory caches. If the lookup table is too big, the cost of memory accesses might undo the benefits of reduced calculations.

```{python}
@jit
def lut_rescale_intensity(img, min_value, max_value):
    shifted_max = max_value - min_value

    # ðŸ˜Ž Create a lookup table mapping from original value to rescaled value.
    # We only need to do 256 calculations. We could perhaps optimize this
    # further, but assuming the image is large enough, the runtime will be
    # dominated by the next section, not this one.
    rescaled = np.empty((256,), dtype=np.uint8)
    for old_value in range(256):
        if old_value < min_value:
            clipped = min_value
        elif old_value > max_value:
            clipped = max_value
        else:
            clipped = old_value
        shifted = clipped - min_value
        scaled = shifted * (255 / shifted_max)
        rescaled[old_value] = np.uint8(np.round(scaled))

    # ðŸ˜Ž For each pixel in the image, update it using the corresponding value
    # in the lookup table, a very cheap operation:
    result = np.empty(img.shape, dtype=np.uint8)
    height, width = img.shape
    for y in range(height):
        for x in range(width):
            result[y, x] = rescaled[img[y, x]]
    return result

assert np.array_equal(
    compiled_rescale_intensity(IMAGE, 30, 190),
    lut_rescale_intensity(IMAGE, 30, 190)
)
```

Here's the speed of our new version:

```{python}
#| echo: false
%%compare_timing --measure=peak_memory
compiled_rescale_intensity(IMAGE, 30, 190)
lut_rescale_intensity(IMAGE, 30, 190)
```
